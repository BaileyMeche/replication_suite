{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0725294a",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b9a45",
   "metadata": {},
   "source": [
    "## TIPS Treasury Arbitrage:\n",
    "\n",
    "```python\n",
    "def task_config():\n",
    "    \"\"\"Create empty directories for data and output if they don't exist\"\"\"\n",
    "    return {\n",
    "        \"actions\": [\"ipython ./src/settings.py\"],\n",
    "        \"targets\": [DATA_DIR, OUTPUT_DIR],\n",
    "        \"file_dep\": [\"./src/settings.py\"],\n",
    "        \"clean\": [],\n",
    "    }\n",
    "```\n",
    "\n",
    "## Equity Spot Futures:\n",
    "```python\n",
    "# Define the paths based on configuration\n",
    "BASE_DIR = config(\"BASE_DIR\")\n",
    "DATA_DIR = Path(config(\"DATA_DIR\"))\n",
    "OUTPUT_DIR = Path(config(\"OUTPUT_DIR\"))\n",
    "TEMP_DIR = Path(config(\"TEMP_DIR\"))\n",
    "INPUT_DIR = Path(config(\"INPUT_DIR\"))\n",
    "# PUBLISH_DIR = Path(config(\"PUBLISH_DIR\"))\n",
    "PROCESSED_DIR = Path(config(\"PROCESSED_DIR\"))\n",
    "\n",
    "# Define log file paths\n",
    "LOG_FILES = [\n",
    "    TEMP_DIR / \"futures_processing.log\",\n",
    "    TEMP_DIR / \"ois_processing.log\",\n",
    "    TEMP_DIR / \"bloomberg_data_extraction.log\"\n",
    "]\n",
    "\n",
    "def task_config():\n",
    "    \"\"\"Create empty directories for data and output if they don't exist, and ensure log files are created\"\"\"\n",
    "    return {\n",
    "        \"actions\": [\"ipython ./src/settings.py\"],  # This action should ensure directories and files are prepared\n",
    "        \"targets\": [\n",
    "            DATA_DIR, OUTPUT_DIR, TEMP_DIR, INPUT_DIR,  PROCESSED_DIR\n",
    "        ] + LOG_FILES,  # Include log files in the targets to manage their existence\n",
    "        \"file_dep\": [\"./src/settings.py\"],\n",
    "        \"clean\": True,  # This will clean up all directories and log files when 'doit clean' is executed\n",
    "    }\n",
    "```\n",
    "\n",
    "\n",
    "## CIP\n",
    "```python\n",
    "\n",
    "DATA_DIR = (config(\"DATA_DIR\"))\n",
    "OUTPUT_DIR = (config(\"OUTPUT_DIR\"))\n",
    "MANUAL_DATA_DIR = (config(\"MANUAL_DATA_DIR\"))\n",
    "PUBLISH_DIR = (config(\"PUBLISH_DIR\"))\n",
    "\n",
    "def update_bloomberg():\n",
    "    \"\"\"Prompt the user and update the BLOOMBERG variable in settings.py.\"\"\"\n",
    "    user_input = input(\"Do you want to run from Bloomberg terminal? (Y/N): \").strip().upper()\n",
    "    new_value = \"True\" if user_input == \"Y\" else \"False\"\n",
    "\n",
    "    settings_path = Path(\"src/settings.py\")\n",
    "\n",
    "    # Read settings.py\n",
    "    with open(settings_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Modify the BLOOMBERG variable if it exists, otherwise append it\n",
    "    found = False\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().startswith(\"BLOOMBERG =\"):\n",
    "            lines[i] = f\"BLOOMBERG = {new_value}\\n\"\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    if not found:\n",
    "        lines.append(f\"\\nBLOOMBERG = {new_value}\\n\")\n",
    "\n",
    "    # Write updated settings back to file\n",
    "    with open(settings_path, \"w\") as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "    import importlib\n",
    "    importlib.reload(src.settings)\n",
    "\n",
    "    if new_value == \"True\":\n",
    "        try:\n",
    "            from xbbg import blp  # Attempt to import xbbg\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"No Bloomberg terminal found on device.\")\n",
    "\n",
    "def task_BLOOMBERG():\n",
    "    \"\"\"PyDoit task to update Bloomberg mode in settings.py.\"\"\"\n",
    "    return {\n",
    "        \"actions\": [update_bloomberg],\n",
    "        \"verbosity\": 2,  # Ensure output is shown\n",
    "    }\n",
    "\n",
    "def task_config():\n",
    "    \"\"\"Create directories for data and output if they don't exist.\"\"\"\n",
    "    def create_dirs():\n",
    "        os.makedirs(DATA_DIR, exist_ok=True)\n",
    "        os.makedirs(MANUAL_DATA_DIR, exist_ok=True)\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        os.makedirs(PUBLISH_DIR, exist_ok=True)\n",
    "\n",
    "    return {\n",
    "        \"actions\": [create_dirs],\n",
    "        \"targets\": [str(DATA_DIR), str(OUTPUT_DIR), str(MANUAL_DATA_DIR), str(PUBLISH_DIR)],\n",
    "        \"clean\": True,\n",
    "    }\n",
    "```\n",
    "\n",
    "## Market Expectations\n",
    "```python\n",
    "BASE_DIR = config(\"BASE_DIR\")\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "MANUAL_DATA_DIR = config(\"MANUAL_DATA_DIR\")\n",
    "OUTPUT_DIR = config(\"OUTPUT_DIR\")\n",
    "#OS_TYPE = config(\"OS_TYPE\")\n",
    "PUBLISH_DIR = config(\"PUBLISH_DIR\")\n",
    "USER = config(\"USER\")\n",
    "PLOTS_DIR = config(\"PLOTS_DIR\")\n",
    "TABLES_DIR = config(\"TABLES_DIR\")\n",
    "\n",
    "\n",
    "def task_config():\n",
    "    \"\"\"Create empty directories for data, output, plots, and tables if they don't exist\"\"\"\n",
    "    return {\n",
    "        \"actions\": [\"ipython ./src/settings.py\"],\n",
    "        \"targets\": [DATA_DIR, OUTPUT_DIR, PLOTS_DIR, TABLES_DIR],\n",
    "        \"file_dep\": [\"./src/settings.py\"],\n",
    "        \"clean\": [],  # Don't clean these files by default.\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd54389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from platform import system\n",
    "from decouple import config as _config\n",
    "from pandas import to_datetime\n",
    "BLOOMBERG = False\n",
    "\n",
    "def get_os():\n",
    "    os_name = system()\n",
    "    if os_name == \"Windows\":\n",
    "        return \"windows\"\n",
    "    elif os_name in (\"Darwin\", \"Linux\"):\n",
    "        return \"nix\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "def if_relative_make_abs(path):\n",
    "    \"\"\"If a relative path is given, make it absolute, assuming\n",
    "    that it is relative to the project root directory (BASE_DIR)\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ```\n",
    "    >>> if_relative_make_abs(Path('_data'))\n",
    "    WindowsPath('C:/Users/jdoe/GitRepositories/blank_project/_data')\n",
    "\n",
    "    >>> if_relative_make_abs(Path(\"C:/Users/jdoe/GitRepositories/blank_project/_output\"))\n",
    "    WindowsPath('C:/Users/jdoe/GitRepositories/blank_project/_output')\n",
    "    ```\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    if path.is_absolute():\n",
    "        abs_path = path.resolve()\n",
    "    else:\n",
    "        abs_path = (d[\"BASE_DIR\"] / path).resolve()\n",
    "    return abs_path\n",
    "\n",
    "d = {}\n",
    "d[\"OS_TYPE\"] = get_os()\n",
    "\n",
    "# Absolute path to root directory of the project\n",
    "d[\"BASE_DIR\"] = Path(__file__).absolute().parent.parent\n",
    "\n",
    "# fmt: off\n",
    "## Other .env variables\n",
    "d[\"START_DATE\"] = _config(\"START_DATE\", default=\"1913-01-01\", cast=to_datetime)\n",
    "d[\"END_DATE\"] = _config(\"END_DATE\", default=\"2024-01-01\", cast=to_datetime)\n",
    "d[\"PIPELINE_DEV_MODE\"] = _config(\"PIPELINE_DEV_MODE\", default=True, cast=bool)\n",
    "d[\"PIPELINE_THEME\"] = _config(\"PIPELINE_THEME\", default=\"pipeline\")\n",
    "\n",
    "d[\"USING_XBBG\"]        = _config(\"USING_XBBG\", default=False, cast=bool)\n",
    "\n",
    "## Paths\n",
    "d[\"DATA_DIR\"] = if_relative_make_abs(_config('DATA_DIR', default=Path('_data'), cast=Path))\n",
    "d[\"MANUAL_DATA_DIR\"] = if_relative_make_abs(_config('MANUAL_DATA_DIR', default=Path('data_manual'), cast=Path))\n",
    "d[\"OUTPUT_DIR\"] = if_relative_make_abs(_config('OUTPUT_DIR', default=Path('_output'), cast=Path))\n",
    "d[\"PUBLISH_DIR\"] = if_relative_make_abs(_config('PUBLISH_DIR', default=Path('_output/publish'), cast=Path))\n",
    "# fmt: on\n",
    "\n",
    "#Equity spot\n",
    "d[\"INPUT_DIR\"]     = if_relative_make_abs(_config('INPUT_DIR', default=Path('_data/input'), cast=Path))\n",
    "d[\"PROCESSED_DIR\"] = if_relative_make_abs(_config('PROCESSED_DIR', default=Path('_data/processed'), cast=Path))\n",
    "d[\"TEMP_DIR\"]      = if_relative_make_abs(_config('TEMP_DIR', default=Path('_output/temp'), cast=Path))\n",
    "\n",
    "#CIP\n",
    "d[\"REPORTS_DIR\"] = if_relative_make_abs(_config(\"REPORTS_DIR\", default=Path(\"reports\"), cast=Path))\n",
    "\n",
    "# Market Expectations\n",
    "d[\"PLOTS_DIR\"] = if_relative_make_abs(_config('PLOTS_DIR', default=Path('reports/plots'), cast=Path))\n",
    "d[\"TABLES_DIR\"] = if_relative_make_abs(_config('TABLES_DIR', default=Path('reports/tables'), cast=Path))\n",
    "\n",
    "\n",
    "## Name of Stata Executable in path\n",
    "if d[\"OS_TYPE\"] == \"windows\":\n",
    "    d[\"STATA_EXE\"] = _config(\"STATA_EXE\", default=\"StataMP-64.exe\")\n",
    "elif d[\"OS_TYPE\"] == \"nix\":\n",
    "    d[\"STATA_EXE\"] = _config(\"STATA_EXE\", default=\"stata-mp\")\n",
    "else:\n",
    "    raise ValueError(\"Unknown OS type\")\n",
    "\n",
    "def create_dirs():\n",
    "    ## If they don't exist, create the _data and _output directories\n",
    "    d[\"DATA_DIR\"].mkdir(parents=True, exist_ok=True)\n",
    "    d[\"OUTPUT_DIR\"].mkdir(parents=True, exist_ok=True)\n",
    "    # (d[\"BASE_DIR\"] / \"_docs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #Equity spot\n",
    "    d[\"TEMP_DIR\"].mkdir(parents=True, exist_ok=True)\n",
    "    d[\"INPUT_DIR\"].mkdir(parents=True, exist_ok=True)\n",
    "    # d[\"PUBLISH_DIR\"].mkdir(parents=True, exist_ok=True)\n",
    "    d[\"PROCESSED_DIR\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # If you'd like to ensure these log files exist (touch them):\n",
    "    for log_filename in (\n",
    "        \"futures_processing.log\",\n",
    "        \"ois_processing.log\",\n",
    "        \"bloomberg_data_extraction.log\",\n",
    "    ):\n",
    "        log_file_path = d[\"TEMP_DIR\"] / log_filename\n",
    "        log_file_path.touch(exist_ok=True)\n",
    "\n",
    "    # CIP\n",
    "    d[\"REPORTS_DIR\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def config(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Retrieve configuration variables. \n",
    "    Checks `d` first. If not found, falls back to .env via decouple.config.\n",
    "    \"\"\"\n",
    "    key = args[0]\n",
    "    default = kwargs.get(\"default\", None)\n",
    "    cast = kwargs.get(\"cast\", None)\n",
    "    if key in d:\n",
    "        var = d[key]\n",
    "        # If a default was passed but we already have a value in d, raise an error\n",
    "        if default is not None:\n",
    "            raise ValueError(f\"Default for {key} already exists. Check your settings.py file.\")\n",
    "        if cast is not None:\n",
    "            # If cast is requested, check that it wouldn't change the type\n",
    "            if not isinstance(var, cast):\n",
    "                # or if we want to actually recast:\n",
    "                try:\n",
    "                    new_var = cast(var)\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"Could not cast {key} to {cast}: {e}\") from e\n",
    "                if type(new_var) is not type(var):\n",
    "                    raise ValueError(f\"Type for {key} differs. Check your settings.py file.\")\n",
    "        return var\n",
    "    else:\n",
    "        return _config(*args, **kwargs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_dirs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fb5f7",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6d902",
   "metadata": {},
   "source": [
    "## TIPS Treasury Arbitrage:\n",
    "Manual data: \n",
    "\n",
    "```python\n",
    "def task_pull_fed_yield_curve():\n",
    "    \"\"\" \"\"\"\n",
    "    file_dep = [\n",
    "        \"./src/pull_fed_yield_curve.py\",\n",
    "    ]\n",
    "    targets = [\n",
    "        DATA_DIR / \"fed_yield_curve_all.parquet\",\n",
    "        DATA_DIR / \"fed_yield_curve.parquet\",\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            \"ipython ./src/pull_fed_yield_curve.py\",\n",
    "        ],\n",
    "        \"targets\": targets,\n",
    "        \"file_dep\": file_dep,\n",
    "        \"clean\": [],\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "from settings import config\n",
    "DATA_DIR = config('DATA_DIR')\n",
    "\n",
    "\n",
    "def pull_fed_yield_curve():\n",
    "    \"\"\"\n",
    "    Download the latest yield curve from the Federal Reserve\n",
    "    \n",
    "    This is the published data using Gurkaynak, Sack, and Wright (2007) model\n",
    "    \n",
    "    load in as: \n",
    "    \"Treasury_SF_10Y\",\n",
    "    \"Treasury_SF_02Y\",\n",
    "    \"Treasury_SF_20Y\",\n",
    "    \"Treasury_SF_03Y\",\n",
    "    \"Treasury_SF_30Y\",\n",
    "    \"Treasury_SF_05Y\",\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://www.federalreserve.gov/data/yield-curve-tables/feds200628.csv\"\n",
    "    response = requests.get(url)\n",
    "    pdf_stream = BytesIO(response.content)\n",
    "    df_all = pd.read_csv(pdf_stream, skiprows=9, index_col=0, parse_dates=True)\n",
    "\n",
    "    cols = ['SVENY' + str(i).zfill(2) for i in range(1, 31)]\n",
    "    df = df_all[cols]\n",
    "    return df_all, df\n",
    "\n",
    "def load_fed_yield_curve_all(data_dir=DATA_DIR):\n",
    "    path = data_dir / \"fed_yield_curve_all.parquet\"\n",
    "    _df = pd.read_parquet(path)\n",
    "    \n",
    "    # Select the specific columns. Note: SVENY03 is included so that\n",
    "    # we can rename to \"Treasury_SF_03Y\" as requested.\n",
    "    selected_cols = ['SVENY02', 'SVENY03', 'SVENY05', 'SVENY10', 'SVENY20', 'SVENY30']\n",
    "    _df = _df[selected_cols]\n",
    "    \n",
    "    # Rename the columns to the desired names.\n",
    "    rename_mapping = {\n",
    "        'SVENY10': 'Treasury_SF_10Y',\n",
    "        'SVENY02': 'Treasury_SF_02Y',\n",
    "        'SVENY20': 'Treasury_SF_20Y',\n",
    "        'SVENY03': 'Treasury_SF_03Y',\n",
    "        'SVENY30': 'Treasury_SF_30Y',\n",
    "        'SVENY05': 'Treasury_SF_05Y'\n",
    "    }\n",
    "    _df = _df.rename(columns=rename_mapping)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "def load_fed_yield_curve(data_dir=DATA_DIR):\n",
    "    path = data_dir / \"fed_yield_curve.parquet\"\n",
    "    _df = pd.read_parquet(path)\n",
    "    return _df\n",
    "\n",
    "def _demo():\n",
    "    _df = pull_fed_yield_curve(data_dir=DATA_DIR)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_all, df = pull_fed_yield_curve()\n",
    "    path = Path(DATA_DIR) / \"fed_yield_curve_all.parquet\"\n",
    "    df_all.to_parquet(path)\n",
    "    path = Path(DATA_DIR) / \"fed_yield_curve.parquet\"\n",
    "    df.to_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d50e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            SVENY01  SVENY02  SVENY03  SVENY04  SVENY05  SVENY06  SVENY07  \\\n",
      "Date                                                                        \n",
      "1961-06-14   2.9825   3.3771   3.5530   3.6439   3.6987   3.7351   3.7612   \n",
      "1961-06-15   2.9941   3.4137   3.5981   3.6930   3.7501   3.7882   3.8154   \n",
      "1961-06-16   3.0012   3.4142   3.5994   3.6953   3.7531   3.7917   3.8192   \n",
      "1961-06-19   2.9949   3.4386   3.6252   3.7199   3.7768   3.8147   3.8418   \n",
      "1961-06-20   2.9833   3.4101   3.5986   3.6952   3.7533   3.7921   3.8198   \n",
      "1961-06-21   2.9993   3.4236   3.6132   3.7107   3.7694   3.8085   3.8364   \n",
      "1961-06-22   2.9837   3.4036   3.5976   3.6981   3.7587   3.7990   3.8279   \n",
      "1961-06-23   2.9749   3.3706   3.5725   3.6816   3.7478   3.7921   3.8237   \n",
      "1961-06-26   2.9563   3.3623   3.5678   3.6784   3.7455   3.7903   3.8224   \n",
      "1961-06-27   2.9666   3.3593   3.5641   3.6778   3.7477   3.7945   3.8280   \n",
      "\n",
      "            SVENY08  SVENY09  SVENY10  ...  SVENY21  SVENY22  SVENY23  \\\n",
      "Date                                   ...                              \n",
      "1961-06-14      NaN      NaN      NaN  ...      NaN      NaN      NaN   \n",
      "1961-06-15      NaN      NaN      NaN  ...      NaN      NaN      NaN   \n",
      "1961-06-16      NaN      NaN      NaN  ...      NaN      NaN      NaN   \n",
      "1961-06-19      NaN      NaN      NaN  ...      NaN      NaN      NaN   \n",
      "1961-06-20      NaN      NaN      NaN  ...      NaN      NaN      NaN   \n",
      "1961-06-21      NaN      NaN      NaN  ...      NaN      NaN      NaN   \n",
      "1961-06-22      NaN      NaN      NaN  ...      NaN      NaN      NaN   \n",
      "1961-06-23      NaN      NaN      NaN  ...      NaN      NaN      NaN   \n",
      "1961-06-26      NaN      NaN      NaN  ...      NaN      NaN      NaN   \n",
      "1961-06-27      NaN      NaN      NaN  ...      NaN      NaN      NaN   \n",
      "\n",
      "            SVENY24  SVENY25  SVENY26  SVENY27  SVENY28  SVENY29  SVENY30  \n",
      "Date                                                                       \n",
      "1961-06-14      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "1961-06-15      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "1961-06-16      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "1961-06-19      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "1961-06-20      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "1961-06-21      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "1961-06-22      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "1961-06-23      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "1961-06-26      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "1961-06-27      NaN      NaN      NaN      NaN      NaN      NaN      NaN  \n",
      "\n",
      "[10 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from settings import config\n",
    "\n",
    "LOCAL_SERIAL_DATA_DIR    = config(\"LOCAL_SERIAL_DATA_DIR\")\n",
    "\n",
    "def local_parquet_head(filepath, rows=10):\n",
    "    \"\"\"\n",
    "    Reads a local Parquet file and returns the first `rows` rows as a DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_parquet(filepath, engine='pyarrow')  # or 'fastparquet'\n",
    "        return df.head(rows)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading parquet file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "file_path = 'fed_yield_curve.parquet'  # Replace with your local path\n",
    "df_head = local_parquet_head(LOCAL_SERIAL_DATA_DIR+'/'+file_path)\n",
    "\n",
    "if df_head is not None:\n",
    "    print(df_head)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef353d7",
   "metadata": {},
   "source": [
    "```python\n",
    "def task_pull_fed_tips_yield_curve():\n",
    "    \"\"\" \"\"\"\n",
    "    file_dep = [\n",
    "        \"./src/pull_fed_tips_yield_curve.py\",\n",
    "    ]\n",
    "    targets = [\n",
    "        DATA_DIR / \"fed_tips_yield_curve.parquet\",\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            \"ipython ./src/pull_fed_tips_yield_curve.py\",\n",
    "        ],\n",
    "        \"targets\": targets,\n",
    "        \"file_dep\": file_dep,\n",
    "        \"clean\": [],\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611f3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import config\n",
    "DATA_DIR = config('DATA_DIR')\n",
    "\n",
    "# Define the URL for the TIPS yield data\n",
    "TIPS_URL = \"https://www.federalreserve.gov/data/yield-curve-tables/feds200805.csv\"\n",
    "\n",
    "def pull_fed_tips_yield_curve():\n",
    "    \"\"\"\n",
    "    Download and process the latest zero-coupon TIPS yield curve from the Federal Reserve.\n",
    "\n",
    "    Expected CSV structure:\n",
    "    - Metadata in the first 19 rows (to be skipped).\n",
    "    - 'Date' column in YMD format.\n",
    "    - TIPS yield columns named 'TIPSY02', 'TIPSY05', 'TIPSY10', 'TIPSY20'.\n",
    "    - Yield values are in percentage terms and must be converted to decimals.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed TIPS yield data.\n",
    "    \"\"\"\n",
    "    # Fetch the data from the Federal Reserve\n",
    "    response = requests.get(TIPS_URL)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch TIPS data: HTTP {response.status_code}\")\n",
    "    \n",
    "    # Read CSV while skipping the first 19 rows (metadata)\n",
    "    df = pd.read_csv(BytesIO(response.content), skiprows=18)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_tips_yield_curve(df, data_dir):\n",
    "    \"\"\"\n",
    "    Save the TIPS yield curve DataFrame to a parquet file.\n",
    "    \"\"\"\n",
    "    path = Path(data_dir) / \"fed_tips_yield_curve.parquet\"\n",
    "    df.to_parquet(path)\n",
    "\n",
    "def load_tips_yield_curve(data_dir):\n",
    "    \"\"\"\n",
    "    Load the TIPS yield curve DataFrame from a parquet file.\n",
    "    Selects and renames the following columns:\n",
    "    \n",
    "    Source columns: TIPSY02, TIPSY05, TIPSY10, TIPSY20, TIPSY30\n",
    "    Target columns: ['TIPS_Treasury_02Y', 'TIPS_Treasury_05Y', 'TIPS_Treasury_10Y', 'TIPS_Treasury_20Y']\n",
    "    \n",
    "    Note: TIPSY30 is ignored since only four target columns are provided.\n",
    "    \"\"\"\n",
    "    path = Path(data_dir) / \"fed_tips_yield_curve.parquet\"\n",
    "    df = pd.read_parquet(path)\n",
    "    \n",
    "    # Select only the required columns (ignoring TIPSY30)\n",
    "    selected_cols = ['TIPSY02', 'TIPSY05', 'TIPSY10', 'TIPSY20']\n",
    "    df = df[selected_cols]\n",
    "    \n",
    "    # Rename the selected columns as specified.\n",
    "    rename_mapping = {\n",
    "        'TIPSY02': 'TIPS_Treasury_02Y',\n",
    "        'TIPSY05': 'TIPS_Treasury_05Y',\n",
    "        'TIPSY10': 'TIPS_Treasury_10Y',\n",
    "        'TIPSY20': 'TIPS_Treasury_20Y'\n",
    "    }\n",
    "    df = df.rename(columns=rename_mapping)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    tips_df = pull_fed_tips_yield_curve()\n",
    "    save_tips_yield_curve(tips_df, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce319d2",
   "metadata": {},
   "source": [
    "```python\n",
    "def task_pull_bloomberg_treasury_inflation_swaps():\n",
    "    \"\"\"Run pull_bloomberg_treasury_inflation_swaps only if treasury_inflation_swaps.csv is not present in OUTPUT_DIR.\"\"\"\n",
    "    from pathlib import Path  # ensure Path is available\n",
    "    output_dir = Path(OUTPUT_DIR)\n",
    "\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    if not any(output_dir.iterdir()):\n",
    "        # Only yield the nested task if the CSV file is not present.\n",
    "        yield {\n",
    "            \"name\": \"run\",\n",
    "            \"actions\": [\"ipython ./src/pull_bloomberg_treasury_inflation_swaps.py\"],\n",
    "            \"file_dep\": [\"./src/pull_bloomberg_treasury_inflation_swaps.py\"],\n",
    "            \"targets\": [DATA_DIR / \"treasury_inflation_swaps.parquet\"],\n",
    "            \"clean\": [],\n",
    "        }\n",
    "    else:\n",
    "        print(\"treasury_inflation_swaps.csv exists in OUTPUT_DIR; skipping task_pull_bloomberg_treasury_inflation_swaps\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xbbg import blp\n",
    "from decouple import config\n",
    "\n",
    "OUTPUT_DIR = config(\"OUTPUT_DIR\")\n",
    "START_DATE = config(\"START_DATE\", \"2020-01-01\")\n",
    "END_DATE = config(\"END_DATE\", \"2025-01-01\")\n",
    "\n",
    "def pull_treasury_inflation_swaps(\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "    output_path=\"treasury_inflation_swaps.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Connects to Bloomberg via xbbg, pulls historical daily prices for USD\n",
    "    Treasury Inflation Swaps, and saves them to a CSV with columns matching\n",
    "    the provided treasury_inflation_swaps.csv file.\n",
    "\n",
    "    :param start_date: Start date in 'YYYY-MM-DD' format (str).\n",
    "    :param end_date: End date in 'YYYY-MM-DD' format (str).\n",
    "    :param output_path: Path to save the resulting CSV file.\n",
    "    :return: A pandas DataFrame containing the replicated data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tickers to replicate. Adjust as needed for 1M, 3M, 6M, etc.\n",
    "    tickers = [\n",
    "        \"USSWIT1 BGN Curncy\",   # 1Y\n",
    "        \"USSWIT2 BGN Curncy\",   # 2Y\n",
    "        \"USSWIT3 BGN Curncy\",   # 3Y\n",
    "        \"USSWIT4 BGN Curncy\",   # 4Y\n",
    "        \"USSWIT5 BGN Curncy\",   # 5Y\n",
    "        \"USSWIT10 BGN Curncy\",  # 10Y\n",
    "        \"USSWIT20 BGN Curncy\",  # 20Y\n",
    "        \"USSWIT30 BGN Curncy\",  # 30Y\n",
    "    ]\n",
    "\n",
    "    fields = [\"PX_LAST\"]\n",
    "\n",
    "    # Pull data using xbbg's bdh function\n",
    "    df = blp.bdh(\n",
    "        tickers=tickers,\n",
    "        flds=fields,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date\n",
    "    )\n",
    "    # 'df' is a multi-index DataFrame with (date) as the index and (ticker, field) as columns.\n",
    "    # Drop the second level of columns (\"PX_LAST\"), so columns are just the tickers\n",
    "    df.columns = df.columns.droplevel(level=1)\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    df = df.rename(columns={\"index\": \"Dates\", \"date\": \"Dates\"})\n",
    "\n",
    "    # Reorder columns so \"Dates\" is first, followed by each ticker\n",
    "    col_order = [\"Dates\"] + tickers\n",
    "    df = df[col_order]\n",
    "\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pull_treasury_inflation_swaps()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed63708",
   "metadata": {},
   "source": [
    "## Equity Spot Futures:\n",
    "Manual data: \"bloomberg_historical_data.parquet\"\n",
    "\n",
    "```python\n",
    "\n",
    "def task_pull_bloomberg():\n",
    "    \"\"\" \"\"\"\n",
    "    file_dep = [\n",
    "        \"./src/settings.py\"\n",
    "    ]\n",
    "    targets = [\n",
    "        INPUT_DIR / \"bloomberg_historical_data.parquet\"\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            \"ipython ./src/pull_bloomberg_data.py\",\n",
    "        ],\n",
    "        \"targets\": targets,\n",
    "        \"file_dep\": file_dep,\n",
    "        \"clean\": [],  # Don't clean these files by default. The ideas\n",
    "        # is that a data pull might be expensive, so we don't want to\n",
    "        # redo it unless we really mean it. So, when you run\n",
    "        # doit clean, all other tasks will have their targets\n",
    "        # cleaned and will thus be rerun the next time you call doit.\n",
    "        # But this one wont.\n",
    "        # Use doit forget --all to redo all tasks. Use doit clean\n",
    "        # to clean and forget the cheaper tasks.\n",
    "    }\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c647960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# Add the src directory to path to load configuration settings\n",
    "sys.path.insert(1, \"./src\")\n",
    "from settings import config\n",
    "\n",
    "# Load configuration values\n",
    "USING_XBBG = config(\"USING_XBBG\")\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "OUTPUT_DIR = config(\"OUTPUT_DIR\")\n",
    "START_DATE = config(\"START_DATE\")\n",
    "END_DATE = config(\"END_DATE\")\n",
    "TEMP_DIR = config(\"TEMP_DIR\")\n",
    "INPUT_DIR = config(\"INPUT_DIR\")\n",
    "\n",
    "# Setup Bloomberg access (requires xbbg and Bloomberg Terminal)\n",
    "if USING_XBBG:\n",
    "    from xbbg import blp\n",
    "else:\n",
    "    print(\"Warning: xbbg not available. This script needs to be run on a machine with Bloomberg access.\")\n",
    "\n",
    "\n",
    "log_file_path = TEMP_DIR/f'bloomberg_data_extraction.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file_path),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "INDEX_CONFIG = {\n",
    "    \"SP\": {\n",
    "        \"spot_ticker\": \"SPX Index\",\n",
    "        \"futures_tickers\": [\"ES1 Index\", \"ES2 Index\", \"ES3 Index\", \"ES4 Index\"]\n",
    "    },\n",
    "    \"Nasdaq\": {\n",
    "        \"spot_ticker\": \"NDX Index\",\n",
    "        \"futures_tickers\": [\"NQ1 Index\", \"NQ2 Index\", \"NQ3 Index\", \"NQ4 Index\"]\n",
    "    },\n",
    "    \"DowJones\": {\n",
    "        \"spot_ticker\": \"INDU Index\",\n",
    "        \"futures_tickers\": [\"DM1 Index\", \"DM2 Index\", \"DM3 Index\", \"DM4 Index\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "OIS_TICKERS = {\n",
    "    \"OIS_1W\": \"USSO1Z CMPN Curncy\",\n",
    "    \"OIS_1M\": \"USSO1 CMPN Curncy\",\n",
    "    \"OIS_3M\": \"USSOC CMPN Curncy\",\n",
    "    \"OIS_6M\": \"USSOF CMPN Curncy\",\n",
    "    \"OIS_1Y\": \"USSO10 CMPN Curncy\"\n",
    "}\n",
    "\n",
    "def pull_spot_div_data(tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Extracts spot price and dividend yield data for specified tickers from Bloomberg.\n",
    "\n",
    "    Args:\n",
    "        tickers (list): List of Bloomberg tickers (e.g., [\"SPX Index\"])\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing historical spot price and dividend estimates\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Extracting spot/dividend data for {tickers}\")\n",
    "        fields = [\"PX_LAST\", \"IDX_EST_DVD_YLD\", \"INDX_GROSS_DAILY_DIV\"]\n",
    "        df = blp.bdh(tickers, fields, start_date=start_date, end_date=end_date)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error pulling spot data for {tickers}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def pull_futures_data(tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Retrieves historical futures contract data from Bloomberg.\n",
    "\n",
    "    Args:\n",
    "        tickers (list): List of Bloomberg futures tickers (e.g., [\"ES1 Index\", \"ES2 Index\"])\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with closing prices, volumes, open interest, and contract months\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Extracting futures data for {tickers}\")\n",
    "        fields = [\"PX_LAST\", \"PX_VOLUME\", \"OPEN_INT\", \"CURRENT_CONTRACT_MONTH_YR\"]\n",
    "        df = blp.bdh(tickers, fields, start_date=start_date, end_date=end_date)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error pulling futures data for {tickers}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def pull_ois_rates(tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Extracts Overnight Indexed Swap (OIS) rate data from Bloomberg.\n",
    "\n",
    "    Args:\n",
    "        tickers (list): List of OIS tickers (e.g., [\"USSOC CMPN Curncy\"])\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing OIS rates over time\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Extracting OIS rates for {tickers}\")\n",
    "        fields = [\"PX_LAST\"]\n",
    "        df = blp.bdh(tickers, fields, start_date=start_date, end_date=end_date)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error pulling OIS rates: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to extract Bloomberg data and save it to a Parquet file.\"\"\"\n",
    "    if USING_XBBG:\n",
    "        try:\n",
    "            logger.info(f\"Pulling data from {START_DATE} to {END_DATE}\")\n",
    "\n",
    "            spot_dfs, futures_dfs = [], []\n",
    "            for index_name, cfg in INDEX_CONFIG.items():\n",
    "                spot_df = pull_spot_div_data([cfg[\"spot_ticker\"]], START_DATE, END_DATE)\n",
    "                futures_df = pull_futures_data(cfg[\"futures_tickers\"], START_DATE, END_DATE)\n",
    "                spot_dfs.append(spot_df)\n",
    "                futures_dfs.append(futures_df)\n",
    "\n",
    "            all_spot = pd.concat(spot_dfs, axis=1) if spot_dfs else pd.DataFrame()\n",
    "            all_futures = pd.concat(futures_dfs, axis=1) if futures_dfs else pd.DataFrame()\n",
    "\n",
    "            ois_df = pull_ois_rates(list(OIS_TICKERS.values()), START_DATE, END_DATE)\n",
    "\n",
    "            final_df = all_spot.join(all_futures, how='outer') if not all_spot.empty else all_futures\n",
    "            final_df = final_df.join(ois_df, how='outer') if not final_df.empty else ois_df\n",
    "            final_df.sort_index(inplace=True)\n",
    "            \n",
    "            INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "            output_path = INPUT_DIR / \"bloomberg_historical_data.parquet\"\n",
    "            final_df.to_parquet(output_path)\n",
    "            logger.info(f\"Final merged data saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting Bloomberg data: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        logger.warning(\"Defaulting to cached data. Set USING_XBBG=True in settings.py to pull fresh data.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ae1ab1",
   "metadata": {},
   "source": [
    "## CIP:\n",
    "Manual data: \"CIP_2025.xlsx\"\n",
    "\n",
    "```python\n",
    "def task_download_cip_data():\n",
    "    \"\"\"\n",
    "    Download CIP_2025.xlsx from GitHub and save it to the manual data folder.\n",
    "    \"\"\"\n",
    "    target_file = MANUAL_DATA_DIR / \"CIP_2025.xlsx\"\n",
    "\n",
    "    def download():\n",
    "        import requests\n",
    "        url = \"https://raw.githubusercontent.com/Kunj121/CIP_DATA/main/CIP_2025%20(1).xlsx\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        os.makedirs(MANUAL_DATA_DIR, exist_ok=True)\n",
    "        with open(target_file, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"File saved to {target_file.resolve()}\")\n",
    "\n",
    "    return {\n",
    "        \"actions\": [download],\n",
    "        \"targets\": [str(target_file)],\n",
    "        \"uptodate\": [False],\n",
    "        \"clean\": True,\n",
    "    }\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db93def3",
   "metadata": {},
   "source": [
    "## Market Expectations\n",
    "```python\n",
    "def task_pull_ken_french_data():\n",
    "    \"\"\"Pull Data from Ken French's Website \"\"\"\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            \"ipython src/settings.py\",\n",
    "            \"ipython src/pull_ken_french_data.py\",\n",
    "        ],\n",
    "        \"targets\": [\n",
    "            Path(DATA_DIR) / \"6_Portfolios_2x3.xlsx\",\n",
    "            Path(DATA_DIR) / \"25_Portfolios_5x5.xlsx\",\n",
    "            Path(DATA_DIR) / \"100_Portfolios_10x10.xlsx\",\n",
    "        ],\n",
    "        \"file_dep\": [\"./src/settings.py\", \"./src/pull_ken_french_data.py\"],\n",
    "        \"clean\": [],  # Don't clean these files by default.\n",
    "        \"verbosity\": 2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "from settings import config\n",
    "\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "START_DATE = config(\"START_DATE\")\n",
    "END_DATE = config(\"END_DATE\")\n",
    "\n",
    "\n",
    "def pull_ken_french_excel(\n",
    "    dataset_name=\"Portfolios_Formed_on_INV\",\n",
    "    data_dir=DATA_DIR,\n",
    "    log=True,\n",
    "    start_date=START_DATE,\n",
    "    end_date=END_DATE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Pulls the Ken French portfolio data..\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset_name (str): Name of the dataset to pull.\n",
    "    - data_dir (str): Directory to save the Excel file.\n",
    "    - log (bool): Whether to log the path of the saved Excel file.\n",
    "    - start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "    - end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "    \n",
    "    Returns:\n",
    "    - Excel File: Contains date, return, and other key fields.\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir = Path(data_dir)\n",
    "    # Suppress the specific FutureWarning about date_parser\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            category=FutureWarning,\n",
    "            message=\"The argument 'date_parser' is deprecated\",\n",
    "        )\n",
    "        data = web.DataReader(\n",
    "            dataset_name,\n",
    "            \"famafrench\",\n",
    "            start=start_date,\n",
    "            end=end_date,\n",
    "        )\n",
    "        excel_path = (\n",
    "            data_dir / f\"{dataset_name.replace('/', '_')}.xlsx\"\n",
    "        )  # Ensure the name is file-path friendly\n",
    "\n",
    "        with pd.ExcelWriter(excel_path, engine=\"xlsxwriter\") as writer:\n",
    "            # Write the description to the first sheet\n",
    "            if \"DESCR\" in data:\n",
    "                description_df = pd.DataFrame([data[\"DESCR\"]], columns=[\"Description\"])\n",
    "                description_df.to_excel(writer, sheet_name=\"Description\", index=False)\n",
    "\n",
    "            # Write each table in the data to subsequent sheets\n",
    "            for table_key, df in data.items():\n",
    "                if table_key == \"DESCR\":\n",
    "                    continue  # Skip the description since it's already handled\n",
    "                sheet_name = str(table_key)  # Naming sheets by their table_key\n",
    "                df.to_excel(\n",
    "                    writer, sheet_name=sheet_name[:31]\n",
    "                )  # Sheet name limited to 31 characters\n",
    "    if log:\n",
    "        print(f\"Excel file saved to {excel_path}\")\n",
    "    return excel_path\n",
    "\n",
    "\n",
    "def load_returns(dataset_name, weighting=\"value-weighted\", data_dir=DATA_DIR):\n",
    "    data_dir = Path(data_dir)\n",
    "    excel_path = data_dir / f\"{dataset_name.replace('/', '_')}.xlsx\"\n",
    "    if weighting == \"value-weighted\":\n",
    "        sheet_name = \"0\"\n",
    "    elif weighting == \"equal-weighted\":\n",
    "        sheet_name = \"1\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid weighting: {weighting}\")\n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_sheet(dataset_name, sheet_name: str = \"0\", data_dir=DATA_DIR):\n",
    "    \"\"\"For example, for dataset_name = '6_Portfolios_2x3V', the full data\n",
    "    is the description and the 10 tables of returns and properties.\n",
    "\n",
    "    6 Portfolios 2x3\n",
    "    ----------------\n",
    "\n",
    "    \n",
    "    This file was created by CMPT_ME_BEME_OP_INV_RETS using the 202412 CRSP database. It contains value- and equal-weighted returns for portfolios formed on ME and BEME. The portfolios are constructed at the end of June. BEME is book value at the last fiscal year end of the prior calendar year divided by ME at the end of December of the prior year. Annual returns are from January to December. Missing data are indicated by -99.99 or -999. The break points include utilities and include financials. The portfolios include utilities and include financials. Copyright 2024 Eugene F. Fama and Kenneth R. French\n",
    "\n",
    "    0 : Average Value Weighted Returns -- Monthly (1129 rows x 6 cols)\n",
    "    1 : Average Equal Weighted Returns -- Monthly (1129 rows x 6 cols)\n",
    "    2 : Average Value Weighted Returns -- Annual (95 rows x 6 cols)\n",
    "    3 : Average Equal Weighted Returns -- Annual (95 rows x 6 cols)\n",
    "    4 : Number of Firms in Portfolios (1129 rows x 6 cols)\n",
    "    5 : Average Market Cap (1129 rows x 6 cols)\n",
    "    6 : For portfolios formed in June of year t   Value Weight Average of BE/ME Calculated for June of t to June of t+1 as:    Sum[ME(Mth) * BE(Fiscal Year t-1) / ME(Dec t-1)] / Sum[ME(Mth)]   Where Mth is a month from June of t to June of t+1   and BE(Fiscal Year t-1) is adjusted for net stock issuance to Dec t-1 (1129 rows x 6 cols)\n",
    "    7 : For portfolios formed in June of year t   Value Weight Average of BE_FYt-1/ME_June t Calculated for June of t to June of t+1 as:    Sum[ME(Mth) * BE(Fiscal Year t-1) / ME(Jun t)] / Sum[ME(Mth)]   Where Mth is a month from June of t to June of t+1   and BE(Fiscal Year t-1) is adjusted for net stock issuance to Jun t (1129 rows x 6 cols)\n",
    "    8 : For portfolios formed in June of year t   Value Weight Average of OP Calculated as:    Sum[ME(Mth) * OP(fiscal year t-1) / BE(fiscal year t-1)] / Sum[ME(Mth)]    Where Mth is a month from June of t to June of t+1 (727 rows x 6 cols)\n",
    "    9 : For portfolios formed in June of year t   Value Weight Average of investment (rate of growth of assets) Calculated as:    Sum[ME(Mth) * Log(ASSET(t-1) / ASSET(t-2) / Sum[ME(Mth)]    Where Mth is a month from June of t to June of t+1 (727 rows x 6 cols)\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(sheet_name, int):\n",
    "        sheet_name = str(sheet_name)\n",
    "\n",
    "    data_dir = Path(data_dir)\n",
    "    excel_path = data_dir / f\"{dataset_name.replace('/', '_')}.xlsx\"\n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "    if sheet_name == \"Description\":\n",
    "        return df.iloc[0, 0]\n",
    "    return df\n",
    "\n",
    "\n",
    "def _demo():\n",
    "    df = load_sheet(\"Portfolios_Formed_on_INV\", sheet_name=\"0\")\n",
    "    df\n",
    "    df_desc = load_sheet(\"25_Portfolios_OP_INV_5x5\", sheet_name=\"Description\")\n",
    "    print(df_desc)\n",
    "\n",
    "    ff_factors = load_sheet(\"F-F_Research_Data_Factors\", sheet_name=\"0\")\n",
    "    ff_factors\n",
    "    ff_factors_desc = load_sheet(\"F-F_Research_Data_Factors\", sheet_name=\"Description\")\n",
    "    print(ff_factors_desc)\n",
    "\n",
    "    ff_portfolios = load_sheet(\"6_Portfolios_2x3\", sheet_name=\"0\")\n",
    "    ff_portfolios\n",
    "    ff_portfolios_desc = load_sheet(\"6_Portfolios_2x3\", sheet_name=\"Description\")\n",
    "    print(ff_portfolios_desc)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _ = pull_ken_french_excel(\n",
    "        dataset_name=\"6_Portfolios_2x3\",\n",
    "    )  # Save 6_Portfolios_2x3.xlsx\n",
    "    _ = pull_ken_french_excel(\n",
    "        dataset_name=\"25_Portfolios_5x5\",\n",
    "    )  # Save 25_Portfolios_5x5.xlsx\n",
    "    _ = pull_ken_french_excel(\n",
    "        dataset_name=\"100_Portfolios_10x10\",\n",
    "    )  # Save 100_Portfolios_10x10.xlsx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e28862",
   "metadata": {},
   "source": [
    "```python\n",
    "# This is needed for CRSP value weighted index\n",
    "def task_pull_CRSP_index():\n",
    "    \"\"\" Pull CRSP Value-Weighted Index \"\"\"\n",
    "    file_dep = [\n",
    "        \"./src/pull_CRSP_index.py\",\n",
    "        ]\n",
    "    file_output = [\n",
    "        \"crsp_value_weighted_index.csv\",\n",
    "        ]\n",
    "    targets = [DATA_DIR / file for file in file_output]\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            \"ipython ./src/pull_CRSP_index.py\",\n",
    "        ],\n",
    "        \"targets\": targets,\n",
    "        \"file_dep\": file_dep,\n",
    "        \"clean\": [],  # Don't clean these files by default.\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrds\n",
    "import pandas as pd\n",
    "from decouple import config\n",
    "import os\n",
    "from settings import config\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "START_DATE = config(\"START_DATE\")\n",
    "END_DATE = config(\"END_DATE\")\n",
    "\n",
    "def pull_crsp_value_weighted_index(\n",
    "        data_dir=DATA_DIR,\n",
    "        log=True,\n",
    "        start_date=START_DATE, \n",
    "        end_date=END_DATE\n",
    "):\n",
    "    \"\"\"\n",
    "    Pulls the CRSP value-weighted index monthly returns from WRDS.\n",
    "    \n",
    "    Parameters:\n",
    "    - start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "    - end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Contains date, return, and other key fields.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get WRDS username from environment variables or .env file\n",
    "    WRDS_USERNAME = config(\"WRDS_USERNAME\", default=os.getenv(\"WRDS_USERNAME\"))\n",
    "\n",
    "    if not WRDS_USERNAME:\n",
    "        raise ValueError(\"WRDS_USERNAME is not set. Add it to your environment variables or .env file.\")\n",
    "\n",
    "    # Connect to WRDS\n",
    "    db = wrds.Connection(wrds_username=WRDS_USERNAME)\n",
    "\n",
    "    # Check available columns in crsp.msi\n",
    "    table_desc = db.describe_table('crsp', 'msi')\n",
    "    print(\"CRSP.msi Table Columns:\", table_desc.columns.tolist())\n",
    "\n",
    "    # Verify column names (adjust based on the actual table structure)\n",
    "    query = f\"\"\"\n",
    "        SELECT date AS date, vwretd AS value_weighted_return\n",
    "        FROM crsp.msi\n",
    "        WHERE date BETWEEN '{start_date}' AND '{end_date}'\n",
    "        ORDER BY date;\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute query\n",
    "    df = db.raw_sql(query, date_cols=['date'])\n",
    "\n",
    "    # Close connection\n",
    "    db.close()\n",
    "\n",
    "    # Convert date to datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_path = (\n",
    "        data_dir / \"crsp_value_weighted_index.csv\"\n",
    "    )\n",
    "\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    # crsp_data.to_csv(\"crsp_value_weighted_index.csv\", index=False)\n",
    "\n",
    "    print(\"CRSP value-weighted index data saved to crsp_value_weighted_index.csv\")\n",
    "    \n",
    "    if log:\n",
    "        print(f\"CSV file saved to {csv_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Pull CRSP value-weighted index data\n",
    "    crsp_data = pull_crsp_value_weighted_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e073bcfe",
   "metadata": {},
   "source": [
    "## Treasury Spot\n",
    "Manual Data: `OIS.xlsx`, `treasury_spot_futures.xlsx`\n",
    "\n",
    "```python\n",
    "DATA_DIR = Path(config(\"DATA_DIR\"))\n",
    "OUTPUT_DIR = Path(config(\"OUTPUT_DIR\"))\n",
    "OS_TYPE = config(\"OS_TYPE\")\n",
    "MANUAL_DATA_DIR = config(\"MANUAL_DATA_DIR\")\n",
    "\n",
    "\n",
    "# Prevent Jupyter from complaining about file path validations\n",
    "environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "from settings import config\n",
    "\n",
    "\n",
    "ois_file = f\"{MANUAL_DATA_DIR}/OIS.xlsx\"\n",
    "data_file = f\"{MANUAL_DATA_DIR}/treasury_spot_futures.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c62807",
   "metadata": {},
   "source": [
    "# Comupte Series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212bf02",
   "metadata": {},
   "source": [
    "## TIPS Treasury Arbitrage:\n",
    "```python\n",
    "def task_compute_tips_treasury():\n",
    "    \"\"\" \"\"\"\n",
    "    file_dep = [\n",
    "        \"./src/compute_tips_treasury.py\",\n",
    "    ]\n",
    "    targets = [\n",
    "        OUTPUT_DIR / \"tips_treasury_implied_rf.parquet\",\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            \"ipython ./src/compute_tips_treasury.py\",\n",
    "        ],\n",
    "        \"targets\": targets,\n",
    "        \"file_dep\": file_dep,\n",
    "        \"clean\": [],\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad752fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from decouple import config\n",
    "import os\n",
    "\n",
    "DATA_DIR = config('DATA_DIR')\n",
    "OUTPUT_DIR = config(\"OUTPUT_DIR\")\n",
    "\n",
    "\n",
    "# Import inflation swap data\n",
    "def import_inflation_swap_data():\n",
    "\t# Point to the CSV file instead of an Excel file\n",
    "\tswaps_path = os.path.join(OUTPUT_DIR, \"treasury_inflation_swaps.csv\")\n",
    "\n",
    "\t# Read CSV; explicitly parse the \"Dates\" column as datetime\n",
    "\tswaps = pd.read_csv(swaps_path, parse_dates=[\"Dates\"])\n",
    "\n",
    "\t# Create a column mapping based on the schema you provided\n",
    "\tcolumn_map = {\n",
    "\t\t\"Dates\": \"date\",\n",
    "\t\t\"USSWITA BGN Curncy\": \"inf_swap_1m\",\n",
    "\t\t\"USSWITC BGN Curncy\": \"inf_swap_3m\",\n",
    "\t\t\"USSWITF BGN Curncy\": \"inf_swap_6m\",\n",
    "\t\t\"USSWIT1 BGN Curncy\": \"inf_swap_1y\",\n",
    "\t\t\"USSWIT2 BGN Curncy\": \"inf_swap_2y\",\n",
    "\t\t\"USSWIT3 BGN Curncy\": \"inf_swap_3y\",\n",
    "\t\t\"USSWIT4 BGN Curncy\": \"inf_swap_4y\",\n",
    "\t\t\"USSWIT5 BGN Curncy\": \"inf_swap_5y\",\n",
    "\t\t\"USSWIT10 BGN Curncy\": \"inf_swap_10y\",\n",
    "\t\t\"USSWIT20 BGN Curncy\": \"inf_swap_20y\",\n",
    "\t\t\"USSWIT30 BGN Curncy\": \"inf_swap_30y\"\n",
    "\t}\n",
    "\n",
    "\t# Rename columns using the mapping\n",
    "\tswaps = swaps.rename(columns=column_map)\n",
    "\n",
    "\t# Convert relevant columns to numeric and divide by 100\n",
    "\tinf_cols = [\n",
    "\t\t\"inf_swap_1y\", \"inf_swap_2y\", \"inf_swap_3y\",\n",
    "\t\t\"inf_swap_4y\", \"inf_swap_5y\", \"inf_swap_10y\",\n",
    "\t\t\"inf_swap_20y\", \"inf_swap_30y\"\n",
    "\t]\n",
    "\tfor col in inf_cols:\n",
    "\t\tswaps[col] = pd.to_numeric(swaps[col], errors=\"coerce\") / 100.0\n",
    "\n",
    "\t# Select only the date and inflation swap columns, in a clean order\n",
    "\tswaps = swaps[[\"date\"] + inf_cols]\n",
    "\n",
    "\treturn swaps\n",
    "\n",
    "# Read in zero-coupon TIPS and Treasury yields\n",
    "def import_treasury_yields():\n",
    "    # Define the path to the parquet file\n",
    "    nom_path = os.path.join(DATA_DIR, \"fed_yield_curve.parquet\")\n",
    "\n",
    "    # Read the parquet file; date is assumed to be in the index\n",
    "    nom = pd.read_parquet(nom_path)\n",
    "\n",
    "    if not pd.api.types.is_datetime64_any_dtype(nom.index):\n",
    "        nom.index = pd.to_datetime(nom.index, format=\"%m/%d/%Y\")\n",
    "\n",
    "    # If the index has no name or is named \"Date\", set it to \"date\"\n",
    "    if nom.index.name is None or nom.index.name == \"Date\":\n",
    "        nom.index.name = \"date\"\n",
    "\n",
    "    # For each tenor (2, 5, 10, 20), compute the nominal zero-coupon yield (in basis points)\n",
    "    for t in [2, 5, 10, 20]:\n",
    "        col = f\"SVENY{'0' + str(t) if t < 10 else str(t)}\"\n",
    "        nom[f\"nom_zc{t}\"] = 1e4 * (np.exp(nom[col] / 100) - 1)\n",
    "\n",
    "    # Convert the date index to a column and rename it to \"date\" if necessary\n",
    "    nom = nom.reset_index()\n",
    "    nom = nom.rename(columns={'Date': 'date'})\n",
    "\n",
    "    # Subset the DataFrame to include the date column plus the computed 'nom' columns\n",
    "    nom = nom[[\"date\"] + [col for col in nom.columns if col.startswith(\"nom\")]]\n",
    "\n",
    "    return nom\n",
    "\n",
    "\n",
    "def import_tips_yields():\n",
    "\treal_path = os.path.join(DATA_DIR, \"fed_tips_yield_curve.parquet\")\n",
    "\treal = pd.read_parquet(real_path)\n",
    "\n",
    "\tif not pd.api.types.is_datetime64_any_dtype(real['Date']):\n",
    "\t\treal.rename(columns={'Date': 'date'}, inplace=True)\n",
    "\t\treal['date'] = pd.to_datetime(real['date'], format=\"%Y-%m-%d\")\n",
    "\n",
    "\tfor t in [2, 5, 10, 20]:\n",
    "\t\tcol = f\"TIPSY{'0' + str(t) if t < 10 else str(t)}\"\n",
    "\t\treal[f\"real_cc{t}\"] = real[col] / 100\n",
    "\n",
    "\treal = real[[\"date\"] + [col for col in real.columns if col.startswith(\"real\")]]\n",
    "\n",
    "\treturn real\n",
    "\n",
    "# Merge all data, compute implied riskless rate from TIPS\n",
    "def compute_tips_treasury():\n",
    "\t\"\"\"\n",
    "\tCreate Constant-Maturity TIPS-Treasury Arbitrage Series and Compute Implied Risk-Free Rates\n",
    "\n",
    "\tThis function merges data from three sources:\n",
    "\t\t1. TIPS yields (real rates) imported via import_tips_yields()\n",
    "\t\t2. Zero-coupon Treasury yields (nominal rates) imported via import_treasury_yields()\n",
    "\t\t3. Inflation swap data (inflation expectations) imported via import_inflation_swap_data()\n",
    "\n",
    "\tIt computes for each tenor (2, 5, 10, and 20 years):\n",
    "\t\t- The TIPS-implied risk-free rate:\n",
    "\t\t\ttips_treas_{t}_rf = 1e4 * (exp(real_cc{t} + log(1 + inf_swap_{t}y)) - 1)\n",
    "\t\twhere:\n",
    "\t\t\t* real_cc{t} is the continuously compounded TIPS real yield (in decimal form),\n",
    "\t\t\t* inf_swap_{t}y is the inflation swap rate as a decimal.\n",
    "\t\t- Arbitrage opportunities (arb_{t}) as the difference between the TIPS-implied risk-free rate\n",
    "\t\tand the nominal zero-coupon Treasury yield (nom_zc{t}). A positive arb_{t} suggests that the\n",
    "\t\tTIPS-derived rate exceeds the nominal rate, indicating a potential arbitrage opportunity.\n",
    "\n",
    "\tThe final merged DataFrame, saved as a parquet file, includes:\n",
    "\t\t- date: Observation date.\n",
    "\t\t- Columns starting with \"real_\": TIPS real yields for each tenor (e.g., real_cc2, real_cc5, real_cc10, real_cc20)\n",
    "\t\texpressed in decimal form (e.g., 0.02 for 2%).\n",
    "\t\t- Columns starting with \"nom_\": Computed nominal zero-coupon Treasury yields for each tenor\n",
    "\t\t(e.g., nom_zc2, nom_zc5, nom_zc10, nom_zc20) expressed in basis points.\n",
    "\t\t- Columns starting with \"tips_\": TIPS-implied risk-free rates (e.g., tips_treas_2_rf, tips_treas_5_rf,\n",
    "\t\ttips_treas_10_rf, tips_treas_20_rf) expressed in basis points.\n",
    "\t\t- Columns starting with \"arb_\": Arbitrage measures (e.g., arb_2, arb_5, arb_10, arb_20) representing the\n",
    "\t\tdifference between the TIPS-implied risk-free rate and the corresponding nominal yield (tips_treas_{t}_rf - nom_zc{t}).\n",
    "\n",
    "\tData quality is maintained by generating missing value indicators and filtering out observations with too many missing values.\n",
    "\tThe resulting dataset is saved as a parquet file with Snappy compression, making it ready for further analysis.\n",
    "\t\"\"\"\n",
    "\treal = import_tips_yields()\n",
    "\tnom = import_treasury_yields()\n",
    "\tswaps = import_inflation_swap_data()\n",
    "\n",
    "\tmerged = pd.merge(real, nom, on=\"date\", how=\"inner\")\n",
    "\tmerged = pd.merge(merged, swaps, on=\"date\", how=\"inner\")\n",
    "\n",
    "\t# Compute implied riskless rates from TIPS and arbitrage measures for each tenor\n",
    "\tmissing_indicators = []\n",
    "\tfor t in [2, 5, 10, 20]:\n",
    "\t\tmerged[f\"tips_treas_{t}_rf\"] = 1e4 * (np.exp(merged[f\"real_cc{t}\"] +\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnp.log(1 + merged[f\"inf_swap_{t}y\"])) - 1)\n",
    "\t\tmerged[f\"mi_{t}\"] = merged[f\"tips_treas_{t}_rf\"].isna().astype(int)\n",
    "\t\tmissing_indicators.append(f\"mi_{t}\")\n",
    "\t\t\n",
    "\t\t# Create new arbitrage columns with prefix \"arb_\"\n",
    "\t\tmerged[f\"arb_{t}\"] = merged[f\"tips_treas_{t}_rf\"] - merged[f\"nom_zc{t}\"]\n",
    "\n",
    "\tmerged[\"miss_count\"] = merged[missing_indicators].sum(axis=1)\n",
    "\tmerged = merged[merged[\"miss_count\"] < 4]\n",
    "\n",
    "\tmerged = merged.drop(missing_indicators + [\"miss_count\"], axis=1)\n",
    "\n",
    "\t\n",
    "\n",
    "\tcols_to_keep = ([\"date\"] +\n",
    "\t\t\t\t\t[col for col in merged.columns if col.startswith(\"real_\")] +\n",
    "\t\t\t\t\t[col for col in merged.columns if col.startswith(\"nom_\")] +\n",
    "\t\t\t\t\t[col for col in merged.columns if col.startswith(\"tips_\")] +\n",
    "\t\t\t\t\t[col for col in merged.columns if col.startswith(\"arb_\")])\n",
    "\tmerged = merged[cols_to_keep]\n",
    "\n",
    "\toutput_path = os.path.join(DATA_DIR, \"tips_treasury_implied_rf.parquet\")\n",
    "\tmerged.to_parquet(output_path, compression=\"snappy\")\n",
    "\n",
    "\tprint(f\"Data saved to {output_path}\")\n",
    "\treturn merged \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tcompute_tips_treasury()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47030930",
   "metadata": {},
   "source": [
    "## Equity Spot Futures:\n",
    "```python\n",
    "\n",
    "def task_process_futures_data():\n",
    "    \"\"\"\n",
    "    Process futures data for indices after pulling the latest data.\n",
    "    \"\"\"\n",
    "    file_dep = [\n",
    "        \"./src/settings.py\",\n",
    "        \"./src/pull_bloomberg_data.py\",\n",
    "        \"./src/futures_data_processing.py\"\n",
    "    ]\n",
    "    targets = [\n",
    "        PROCESSED_DIR / \"all_indices_calendar_spreads.csv\",\n",
    "        PROCESSED_DIR / \"INDU_calendar_spread.csv\",\n",
    "        PROCESSED_DIR / \"SPX_calendar_spread.csv\",\n",
    "        PROCESSED_DIR / \"NDX_calendar_spread.csv\",\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            \"python ./src/futures_data_processing.py\",\n",
    "        ],\n",
    "        \"file_dep\": file_dep,\n",
    "        \"targets\": targets,\n",
    "        \"clean\": True,  \n",
    "    }\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, \"./src\")\n",
    "from settings import config\n",
    "from datetime import datetime\n",
    "# Configuration from settings\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "TEMP_DIR = config(\"TEMP_DIR\")\n",
    "INPUT_DIR = config(\"INPUT_DIR\")\n",
    "PROCESSED_DIR = config(\"PROCESSED_DIR\")\n",
    "DATA_MANUAL = config(\"MANUAL_DATA_DIR\")\n",
    "\n",
    "\n",
    "log_file_path = TEMP_DIR/f'futures_processing.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file_path),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_third_friday(year, month):\n",
    "    \"\"\"\n",
    "    Calculate the third Friday of a given month and year.\n",
    "    \n",
    "    Args:\n",
    "        year (int): Year\n",
    "        month (int): Month (1-12)\n",
    "        \n",
    "    Returns:\n",
    "        datetime: Date object for the third Friday\n",
    "    \"\"\"\n",
    "    # Use calendar.monthcalendar: each week is a list of ints (0 if day not in month)\n",
    "    month_cal = calendar.monthcalendar(year, month)\n",
    "    # The first week that has a Friday (weekday index 4)\n",
    "    fridays = [week[calendar.FRIDAY] for week in month_cal if week[calendar.FRIDAY] != 0]\n",
    "    if len(fridays) < 3:\n",
    "        raise ValueError(f\"Not enough Fridays in {year}-{month}\")\n",
    "    return datetime(year, month, fridays[2])  # third Friday\n",
    "\n",
    "def parse_contract_month_year(contract_str):\n",
    "    \"\"\"\n",
    "    Parse Bloomberg's contract month/year string (e.g., 'DEC 10') into\n",
    "    a month number and a full year.\n",
    "    \n",
    "    Args:\n",
    "        contract_str (str): Contract month/year string\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (month_num, year_full) or (None, None) if invalid.\n",
    "    \"\"\"\n",
    "    if pd.isna(contract_str) or contract_str.strip() == '':\n",
    "        return None, None\n",
    "    parts = contract_str.split()\n",
    "    if len(parts) != 2:\n",
    "        logger.warning(f\"Unexpected contract format: {contract_str}\")\n",
    "        return None, None\n",
    "    month_abbr, year_abbr = parts\n",
    "    allowed = {\"MAR\": 3, \"JUN\": 6, \"SEP\": 9, \"DEC\": 12}\n",
    "    if month_abbr.upper() not in allowed:\n",
    "        raise ValueError(f\"Contract month {month_abbr} not in allowed set {list(allowed.keys())}\")\n",
    "    month_num = allowed[month_abbr.upper()]\n",
    "    try:\n",
    "        yr = int(year_abbr)\n",
    "        year_full = 2000 + yr if yr < 50 else 1900 + yr\n",
    "    except ValueError:\n",
    "        logger.warning(f\"Could not parse year: {year_abbr}\")\n",
    "        return None, None\n",
    "    return month_num, year_full\n",
    "\n",
    "def process_index_futures(data, futures_codes):\n",
    "    \"\"\"\n",
    "    Process futures data for one index.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Multi-index DataFrame with Bloomberg data (indexed by Date)\n",
    "        futures_codes (list): List of futures codes (e.g., ['ES1', 'ES2', ...])\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of processed DataFrames, one for each futures code.\n",
    "              Each DataFrame contains:\n",
    "                  - Date\n",
    "                  - Futures_Price (from PX_LAST)\n",
    "                  - Volume, OpenInterest (if available)\n",
    "                  - ContractSpec (raw CURRENT_CONTRACT_MONTH_YR)\n",
    "                  - SettlementDate (actual settlement, 3rd Friday)\n",
    "                  - TTM (time-to-maturity in days)\n",
    "    \"\"\"\n",
    "    result_dfs = {}\n",
    "    for code in futures_codes:\n",
    "        logger.info(f\"Processing futures data for {code} Index\")\n",
    "        try:\n",
    "            # Extract columns for this contract:\n",
    "            # Expecting columns like: (f'{code} Index', 'PX_LAST'), (f'{code} Index', 'CURRENT_CONTRACT_MONTH_YR'), etc.\n",
    "            price_series = data.loc[:, (f'{code} Index', 'PX_LAST')]\n",
    "            volume_series = data.loc[:, (f'{code} Index', 'PX_VOLUME')]\n",
    "            oi_series = data.loc[:, (f'{code} Index', 'OPEN_INT')]\n",
    "            contract_series = data.loc[:, (f'{code} Index', 'CURRENT_CONTRACT_MONTH_YR')]\n",
    "            \n",
    "            # Create a DataFrame for this contract; index is Date (from raw data)\n",
    "            df_contract = pd.DataFrame({\n",
    "                'Date': data.index,\n",
    "                'Futures_Price': price_series,\n",
    "                'Volume': volume_series,\n",
    "                'OpenInterest': oi_series,\n",
    "                'ContractSpec': contract_series\n",
    "            })\n",
    "            df_contract = df_contract.reset_index(drop=True)\n",
    "            \n",
    "            # Parse contract specification and compute settlement date\n",
    "            settlement_dates = []\n",
    "            for cs in df_contract['ContractSpec']:\n",
    "                month_num, year_full = parse_contract_month_year(cs)\n",
    "                if month_num is None or year_full is None:\n",
    "                    settlement_dates.append(None)\n",
    "                else:\n",
    "                    settlement_dates.append(get_third_friday(year_full, month_num))\n",
    "            df_contract['SettlementDate'] = pd.to_datetime(settlement_dates)\n",
    "            \n",
    "            # Compute TTM in days: SettlementDate - Date\n",
    "            df_contract['Date'] = pd.to_datetime(df_contract['Date'])\n",
    "            df_contract['TTM'] = (df_contract['SettlementDate'] - df_contract['Date']).dt.days\n",
    "            \n",
    "            # Drop rows with missing TTM (if settlement date couldn't be computed)\n",
    "            df_contract = df_contract.dropna(subset=['TTM'])\n",
    "            result_dfs[code] = df_contract\n",
    "            logger.info(f\"Processed {code}: {len(df_contract)} rows\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {code}: {e}\")\n",
    "            continue\n",
    "    return result_dfs\n",
    "\n",
    "def merge_calendar_spreads(all_futures):\n",
    "    \"\"\"\n",
    "    For each index, merge the processed data for the two nearest futures contracts (Term 1 and Term 2)\n",
    "    on the Date field, and then combine the calendar spreads for all indices.\n",
    "    \n",
    "    Args:\n",
    "        all_futures (dict): Dictionary keyed by index code (e.g., 'SPX', 'NDX', 'INDU') where the value is\n",
    "                            another dictionary mapping futures code to its processed DataFrame.\n",
    "                            \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined calendar spread data for all indices.\n",
    "    \"\"\"\n",
    "    combined = []\n",
    "    # For each index, assume the first two codes in the list are the two nearest contracts.\n",
    "    # For SPX, these would be ['ES1', 'ES2'].\n",
    "    for index_code, fut_dict in all_futures.items():\n",
    "        # Identify term1 and term2 codes:\n",
    "        codes = list(fut_dict.keys())\n",
    "        if len(codes) < 2:\n",
    "            logger.warning(f\"Not enough futures data for {index_code}\")\n",
    "            continue\n",
    "        term1 = fut_dict[codes[0]].copy()\n",
    "        term2 = fut_dict[codes[1]].copy()\n",
    "        # Add a prefix so that we can merge and distinguish columns:\n",
    "        term1 = term1.add_prefix('Term1_')\n",
    "        term2 = term2.add_prefix('Term2_')\n",
    "        # Rename the Date columns back to 'Date' for merging\n",
    "        term1.rename(columns={'Term1_Date': 'Date'}, inplace=True)\n",
    "        term2.rename(columns={'Term2_Date': 'Date'}, inplace=True)\n",
    "        merged = pd.merge(term1, term2, on='Date', how='inner')\n",
    "        merged['Index'] = index_code\n",
    "        combined.append(merged)\n",
    "        # Save each indexs calendar spread separately\n",
    "        output_file = PROCESSED_DIR / f\"{index_code}_calendar_spread.csv\"\n",
    "        merged.to_csv(output_file , index=False)\n",
    "        logger.info(f\"Saved calendar spread for {index_code}: {len(merged)} rows\")\n",
    "        logger.info(f\"DataFrame merged:\\n{merged.head()}\")\n",
    "    if combined:\n",
    "        combined_df = pd.concat(combined, ignore_index=True)\n",
    "        output_file = PROCESSED_DIR / \"all_indices_calendar_spreads.csv\"\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        logger.info(f\"Saved combined calendar spread data: {len(combined_df)} rows\")\n",
    "        logger.info(f\"DataFrame combined:\\n{combined_df.head()}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        logger.warning(\"No valid calendar spread data to combine\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process raw futures data from a parquet file.\n",
    "    \n",
    "    It:\n",
    "      - Loads the multi-index raw data from DATA_DIR/input/bloomberg_historical_data.parquet\n",
    "      - Processes each index (SPX, NDX, INDU) futures for the two nearest contracts\n",
    "      - Extracts settlement dates and computes TTM\n",
    "      - Merges the Term 1 and Term 2 data to form calendar spread datasets\n",
    "      - Saves both individual and combined outputs for downstream spread calculations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        try:\n",
    "            INPUT_FILE = INPUT_DIR / \"bloomberg_historical_data.parquet\"\n",
    "            raw_data = pd.read_parquet(INPUT_FILE)\n",
    "        except Exception as e:\n",
    "            INPUT_FILE = DATA_MANUAL / \"bloomberg_historical_data.parquet\"\n",
    "            raw_data = pd.read_parquet(INPUT_FILE)\n",
    "        logger.info(f\"Loading raw data from {INPUT_FILE}\")\n",
    "        if not isinstance(raw_data.index, pd.DatetimeIndex):\n",
    "            raw_data.index = pd.to_datetime(raw_data.index)\n",
    "        indices = {\n",
    "            'SPX': ['ES1', 'ES2', 'ES3', 'ES4'],\n",
    "            'NDX': ['NQ1', 'NQ2', 'NQ3', 'NQ4'],\n",
    "            'INDU': ['DM1', 'DM2', 'DM3', 'DM4']\n",
    "        }\n",
    "        \n",
    "        all_futures = {}\n",
    "        for index_code, futures_codes in indices.items():\n",
    "            logger.info(f\"Processing futures for index {index_code}\")\n",
    "            processed = process_index_futures(raw_data, futures_codes)\n",
    "            all_futures[index_code] = processed\n",
    "        \n",
    "        # Merge calendar spreads (using only Term 1 and Term 2)\n",
    "        combined_spreads = merge_calendar_spreads(all_futures)\n",
    "        logger.info(\"Futures data processing completed successfully\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf91ff87",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def task_process_ois_data():\n",
    "    \"\"\"\n",
    "    Process OIS data for 3-month rates after pulling the latest Bloomberg data.\n",
    "    \"\"\"\n",
    "    file_dep = [\n",
    "        \"./src/settings.py\",\n",
    "        \"./src/pull_bloomberg_data.py\",\n",
    "        \"./src/OIS_data_processing.py\"\n",
    "    ]\n",
    "    targets = [\n",
    "        PROCESSED_DIR / \"cleaned_ois_rates.csv\"\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            \"python ./src/OIS_data_processing.py\",\n",
    "        ],\n",
    "        \"file_dep\": file_dep,\n",
    "        \"targets\": targets,\n",
    "        \"clean\": True,  # Add appropriate clean actions if necessary\n",
    "    }\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, \"./src\")\n",
    "from settings import config\n",
    "\n",
    "# Load configuration paths\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "TEMP_DIR = config(\"TEMP_DIR\")\n",
    "INPUT_DIR = config(\"INPUT_DIR\")\n",
    "PROCESSED_DIR = config(\"PROCESSED_DIR\")\n",
    "DATA_MANUAL = config(\"MANUAL_DATA_DIR\")\n",
    "\n",
    "log_file = TEMP_DIR / f'ois_processing.log'\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Required column mapping\n",
    "OIS_TENORS = {\n",
    "    \"OIS_3M\": \"USSOC CMPN Curncy\",   # 3 Month OIS Rate\n",
    "}\n",
    "\n",
    "def process_ois_data(filepath: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts, cleans, and formats only the 3-month OIS rate from Bloomberg historical dataset.\n",
    "\n",
    "    Args:\n",
    "        filepath (Path): Path to the parquet file containing multi-index Bloomberg data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned OIS dataset containing only the 3-month OIS rate.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the required OIS column is missing.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading OIS data from {filepath}\")\n",
    "\n",
    "    try:\n",
    "        ois_df = pd.read_parquet(filepath)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading parquet file: {e}\")\n",
    "        raise\n",
    "\n",
    "    logger.info(f\"Column levels: {ois_df.columns.names}\")\n",
    "\n",
    "    # Ensure required OIS column is present\n",
    "    required_col = OIS_TENORS[\"OIS_3M\"]\n",
    "    if (required_col, \"PX_LAST\") not in ois_df.columns:\n",
    "        raise ValueError(f\"Missing required OIS column: {required_col}\")\n",
    "\n",
    "    # Select only the required 3-month OIS rate column\n",
    "    ois_df = ois_df.loc[:, [(required_col, \"PX_LAST\")]]\n",
    "    ois_df.columns = [\"OIS_3M\"]  # Rename to a clean column name\n",
    "\n",
    "    # Convert OIS rates from percentage to decimal format (if applicable)\n",
    "    logger.info(\"Converting OIS_3M from percentage to decimal format\")\n",
    "    ois_df[\"OIS_3M\"] = ois_df[\"OIS_3M\"] / 100\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    ois_df = ois_df.dropna(subset=[\"OIS_3M\"])\n",
    "\n",
    "    # Save the cleaned dataset\n",
    "    output_path = Path(PROCESSED_DIR) / \"cleaned_ois_rates.csv\"\n",
    "    ois_df.to_csv(output_path, index=True)\n",
    "    logger.info(f\"Saved cleaned OIS rates to {output_path}\")\n",
    "\n",
    "    # Log dataset summary\n",
    "    logger.info(\"\\n========== OIS Data Summary ==========\")\n",
    "    logger.info(f\"Shape of dataset: {ois_df.shape} (rows, columns)\")\n",
    "    logger.info(f\"Missing values per column:\\n{ois_df.isna().sum().to_string()}\")\n",
    "    logger.info(\"Descriptive statistics:\\n%s\", ois_df.describe().to_string())\n",
    "    logger.info(\"First 5 rows of cleaned OIS data:\\n%s\", ois_df.head().to_string())\n",
    "\n",
    "    return ois_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process OIS rates.\n",
    "    Loads Bloomberg historical data, extracts only the 3-month OIS rate,\n",
    "    cleans and formats it, and saves it for further use.\n",
    "    \"\"\"\n",
    "    INPUT_FILE = Path(INPUT_DIR) / \"bloomberg_historical_data.parquet\"\n",
    "\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        logger.warning(\"Primary input file not found, switching to cached data\")\n",
    "        INPUT_FILE = Path(DATA_MANUAL) / \"bloomberg_historical_data.parquet\"\n",
    "\n",
    "    try:\n",
    "        process_ois_data(INPUT_FILE)\n",
    "        logger.info(\"OIS data processing completed successfully!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing OIS data: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae91d98",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def task_spread_calculations():\n",
    "    \"\"\"\n",
    "    Spread calculations from processed data\n",
    "    \"\"\"\n",
    "    file_dep = [\n",
    "        \"./src/settings.py\",\n",
    "        \"./src/pull_bloomberg_data.py\",\n",
    "        \"./src/OIS_data_processing.py\",  \n",
    "        \"./src/futures_data_processing.py\"\n",
    "    ]\n",
    "    targets = [\n",
    "        PROCESSED_DIR / \"SPX_Forward_Rates.csv\",\n",
    "        PROCESSED_DIR / \"NDX_Forward_Rates.csv\",\n",
    "        PROCESSED_DIR / \"INDU_Forward_Rates.csv\",\n",
    "        OUTPUT_DIR / \"all_indices_spread_to_2020.png\",\n",
    "        OUTPUT_DIR / \"all_indices_spread_to_present.png\"\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            \"python ./src/Spread_calculations.py\",\n",
    "        ],\n",
    "        \"file_dep\": file_dep,\n",
    "        \"targets\": targets,\n",
    "        \"clean\": True,  \n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.insert(1, \"./src\")\n",
    "from settings import config\n",
    "\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "TEMP_DIR = config(\"TEMP_DIR\")\n",
    "INPUT_DIR = config(\"INPUT_DIR\")\n",
    "PROCESSED_DIR = config(\"PROCESSED_DIR\")\n",
    "DATA_MANUAL = config(\"MANUAL_DATA_DIR\")\n",
    "OUTPUT_DIR = config(\"OUTPUT_DIR\")\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "log_file = Path(TEMP_DIR) / f\"forward_rate_calculation.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "INDEX_CODES = [\"SPX\", \"NDX\", \"INDU\"]\n",
    "\n",
    "\n",
    "def build_daily_dividends(index_code: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load daily dividends for the given index code from bloomberg_historical_data.parquet.\n",
    "    Return columns: [Date, Daily_Div].\n",
    "    \"\"\"\n",
    "    logger.info(f\"[{index_code}] Building daily dividend table\")\n",
    "\n",
    "    input_file = Path(INPUT_DIR) / \"bloomberg_historical_data.parquet\"\n",
    "    if not os.path.exists(input_file):\n",
    "        logger.warning(\"Primary input file not found, switching to cached data\")\n",
    "        input_file = Path(DATA_MANUAL) / \"bloomberg_historical_data.parquet\"\n",
    "\n",
    "    raw_df = pd.read_parquet(input_file)\n",
    "    div_col = (f\"{index_code} Index\", \"INDX_GROSS_DAILY_DIV\")\n",
    "    if div_col not in raw_df.columns:\n",
    "        raise ValueError(f\"Missing daily dividend column {div_col} for index={index_code}\")\n",
    "\n",
    "    div_df = raw_df.loc[:, div_col].to_frame(\"Daily_Div\").reset_index()\n",
    "    div_df.rename(columns={\"index\": \"Date\"}, inplace=True)\n",
    "    div_df[\"Date\"] = pd.to_datetime(div_df[\"Date\"], errors=\"coerce\")\n",
    "    div_df[\"Daily_Div\"] = div_df[\"Daily_Div\"].fillna(0)\n",
    "\n",
    "    # Optionally drop any row that has no valid date\n",
    "    before_drop = len(div_df)\n",
    "    div_df.dropna(subset=[\"Date\"], inplace=True)\n",
    "    after_drop = len(div_df)\n",
    "    if after_drop < before_drop:\n",
    "        logger.info(\n",
    "            f\"[{index_code}] Dropped {before_drop - after_drop} rows with invalid or missing date in daily_div.\"\n",
    "        )\n",
    "\n",
    "    div_df.sort_values(\"Date\", inplace=True)\n",
    "    div_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    logger.info(f\"[{index_code}] daily dividends final shape: {div_df.shape}\")\n",
    "    logger.info(f\"[{index_code}] Sample daily dividends:\\n{div_df.head(10)}\")\n",
    "    return div_df\n",
    "\n",
    "\n",
    "def barndorff_nielsen_filter(df: pd.DataFrame,\n",
    "                             colname: str,\n",
    "                             date_col: str = \"Date\",\n",
    "                             window: int = 45,\n",
    "                             threshold: float = 10.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Barndorff-Nielsen outlier filter on 'colname' over window days.\n",
    "    1) rolling median => ...\n",
    "    2) abs_dev from that median\n",
    "    3) rolling mean(abs_dev) => mad\n",
    "    4) outlier if abs_dev/mad >= threshold => set colname_filtered=NaN\n",
    "    \"\"\"\n",
    "    df = df.sort_values(date_col).copy()\n",
    "\n",
    "    rolling_median = df[colname].rolling(window=window*2+1, center=True, min_periods=1).median()\n",
    "    rolling_median_shifted = rolling_median.shift(1)\n",
    "\n",
    "    df[\"abs_dev\"] = (df[colname] - rolling_median_shifted).abs()\n",
    "    rolling_mad = df[\"abs_dev\"].rolling(window=window*2+1, center=True, min_periods=1).mean()\n",
    "    rolling_mad_shifted = rolling_mad.shift(1)\n",
    "\n",
    "    df[\"bad_price\"] = (df[\"abs_dev\"] / rolling_mad_shifted) >= threshold\n",
    "    df.loc[df[colname].isna(), \"bad_price\"] = False\n",
    "\n",
    "    # Count how many outliers\n",
    "    outlier_count = df[\"bad_price\"].sum()\n",
    "    if outlier_count > 0:\n",
    "        logger.info(f\"Barndorff-Nielsen filter: flagged {int(outlier_count)} outliers in {colname}\")\n",
    "\n",
    "    df[f\"{colname}_filtered\"] = df[colname].where(~df[\"bad_price\"], np.nan)\n",
    "\n",
    "    df.drop([\"abs_dev\", \"bad_price\"], axis=1, inplace=True, errors=\"ignore\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_index_forward_rates(index_code: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Load near/next futures for index_code from _Calendar_spread.csv\n",
    "    2) Merge with single OIS_3M (as-of)\n",
    "    3) Merge daily dividends, compute Div_Sum1_Comp & Div_Sum2_Comp\n",
    "    4) Implied forward => cal_{index_code}_rf, OIS forward => ois_fwd_{index_code}, spread\n",
    "    5) Barndorff outlier filter, then multiply spread by 100 => bps\n",
    "    6) Save & return\n",
    "    \"\"\"\n",
    "    logger.info(f\"[{index_code}] Starting forward rate computation\")\n",
    "\n",
    "    fut_file = Path(PROCESSED_DIR) / f\"{index_code}_Calendar_spread.csv\"\n",
    "    if not fut_file.exists():\n",
    "        logger.error(f\"[{index_code}] Missing futures file: {fut_file}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    fut_df = pd.read_csv(fut_file)\n",
    "    logger.info(f\"[{index_code}] Loaded futures shape: {fut_df.shape}\")\n",
    "\n",
    "\n",
    "    if \"Date\" not in fut_df.columns:\n",
    "        logger.error(f\"[{index_code}] No 'Date' column in {fut_file}, aborting.\")\n",
    "        return pd.DataFrame()\n",
    "    fut_df[\"Date\"] = pd.to_datetime(fut_df[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "    before_drop = len(fut_df)\n",
    "    fut_df.dropna(subset=[\"Date\"], inplace=True)\n",
    "    logger.info(f\"[{index_code}] Dropped {before_drop - len(fut_df)} rows lacking a valid Date in futures.\")\n",
    "    fut_df[\"Term1_SettlementDate\"] = pd.to_datetime(fut_df[\"Term1_SettlementDate\"], errors=\"coerce\")\n",
    "    fut_df[\"Term2_SettlementDate\"] = pd.to_datetime(fut_df[\"Term2_SettlementDate\"], errors=\"coerce\")\n",
    "\n",
    "    fut_df.sort_values(\"Date\", inplace=True)\n",
    "    fut_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # === Merge single OIS_3M\n",
    "    ois_file = Path(PROCESSED_DIR) / \"cleaned_ois_rates.csv\"\n",
    "    if not ois_file.exists():\n",
    "        logger.error(f\"[{index_code}] Missing OIS file: {ois_file}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    ois_df = pd.read_csv(ois_file)\n",
    "    if \"Date\" not in ois_df.columns:\n",
    "        ois_df.rename(columns={\"Unnamed: 0\": \"Date\"}, inplace=True)\n",
    "    ois_df[\"Date\"] = pd.to_datetime(ois_df[\"Date\"], errors=\"coerce\")\n",
    "    ois_df.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "    # as-of merge\n",
    "    prev_len = len(fut_df)\n",
    "    merged_df = pd.merge_asof(\n",
    "        fut_df, ois_df, on=\"Date\", direction=\"backward\"\n",
    "    )\n",
    "    after_len = len(merged_df)\n",
    "    logger.info(f\"[{index_code}] as-of merged OIS: from {prev_len} -> {after_len} rows (should be same).\")\n",
    "\n",
    "    # rename OIS_3M => 'OIS'\n",
    "    if \"OIS_3M\" in merged_df.columns:\n",
    "        merged_df.rename(columns={\"OIS_3M\": \"OIS\"}, inplace=True)\n",
    "    else:\n",
    "        logger.warning(f\"[{index_code}] 'OIS_3M' column not found in OIS data, using default 'OIS_3M'?\")\n",
    "\n",
    "    # === Load daily dividends\n",
    "    div_df = build_daily_dividends(index_code)\n",
    "    # add cumsum in div_df\n",
    "    div_df[\"CumDiv\"] = div_df[\"Daily_Div\"].cumsum()\n",
    "\n",
    "    # merge cumsum at current date\n",
    "    prev_len = len(merged_df)\n",
    "    merged_df = pd.merge_asof(\n",
    "        merged_df.sort_values(\"Date\"),\n",
    "        div_df[[\"Date\", \"CumDiv\"]].sort_values(\"Date\"),\n",
    "        on=\"Date\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    after_len = len(merged_df)\n",
    "    logger.info(\n",
    "        f\"[{index_code}] as-of merged CumDiv at current date: from {prev_len} -> {after_len} rows.\"\n",
    "    )\n",
    "    merged_df.rename(columns={\"CumDiv\": \"CumDiv_current\"}, inplace=True)\n",
    "    logger.info(f\"[{index_code}] Sample merged rows with cumulative div:\\n{merged_df.head(10)}\")\n",
    "    # same approach for Term1 & Term2\n",
    "    t1_df = div_df.rename(columns={\"Date\": \"Term1_SettlementDate\", \"CumDiv\": \"CumDiv_Term1\"})\n",
    "    prev_len = len(merged_df)\n",
    "    merged_df = pd.merge_asof(\n",
    "        merged_df.sort_values(\"Term1_SettlementDate\"),\n",
    "        t1_df.sort_values(\"Term1_SettlementDate\"),\n",
    "        on=\"Term1_SettlementDate\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    after_len = len(merged_df)\n",
    "    logger.info(\n",
    "        f\"[{index_code}] as-of merged CumDiv for Term1: from {prev_len} -> {after_len} rows.\"\n",
    "    )\n",
    "\n",
    "    t2_df = div_df.rename(columns={\"Date\": \"Term2_SettlementDate\", \"CumDiv\": \"CumDiv_Term2\"})\n",
    "    prev_len = len(merged_df)\n",
    "    merged_df = pd.merge_asof(\n",
    "        merged_df.sort_values(\"Term2_SettlementDate\"),\n",
    "        t2_df.sort_values(\"Term2_SettlementDate\"),\n",
    "        on=\"Term2_SettlementDate\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    after_len = len(merged_df)\n",
    "    logger.info(\n",
    "        f\"[{index_code}] as-of merged CumDiv for Term2: from {prev_len} -> {after_len} rows.\"\n",
    "    )\n",
    "\n",
    "    # compute Div_Sum1 & Div_Sum2\n",
    "    merged_df[\"Div_Sum1\"] = merged_df[\"CumDiv_Term1\"] - merged_df[\"CumDiv_current\"]\n",
    "    merged_df[\"Div_Sum2\"] = merged_df[\"CumDiv_Term2\"] - merged_df[\"CumDiv_current\"]\n",
    "\n",
    "    # Handle missing TTM or price\n",
    "    # If TTM is missing, can't compute rates => drop\n",
    "    before_drop = len(merged_df)\n",
    "    merged_df.dropna(subset=[\"Term1_TTM\", \"Term2_TTM\", \"Term1_Futures_Price\", \"Term2_Futures_Price\"], inplace=True)\n",
    "    logger.info(\n",
    "        f\"[{index_code}] Dropped {before_drop - len(merged_df)} rows missing TTM or Futures_Price.\"\n",
    "    )\n",
    "\n",
    "    # 4) Compounding\n",
    "    ttm1 = \"Term1_TTM\"\n",
    "    ttm2 = \"Term2_TTM\"\n",
    "    merged_df[\"Div_Sum1_Comp\"] = merged_df[\"Div_Sum1\"] * (\n",
    "        ((merged_df[ttm1] / 2.0) / 360.0) * merged_df[\"OIS\"] + 1.0\n",
    "    )\n",
    "    merged_df[\"Div_Sum2_Comp\"] = merged_df[\"Div_Sum2\"] * (\n",
    "        ((merged_df[ttm2] / 2.0) / 360.0) * merged_df[\"OIS\"] + 1.0\n",
    "    )\n",
    "\n",
    "    # Implied Forward\n",
    "    fp1 = \"Term1_Futures_Price\"\n",
    "    fp2 = \"Term2_Futures_Price\"\n",
    "    merged_df[\"implied_forward_raw\"] = (\n",
    "        (merged_df[fp2] + merged_df[\"Div_Sum2_Comp\"]) /\n",
    "        (merged_df[fp1] + merged_df[\"Div_Sum1_Comp\"])\n",
    "        - 1.0\n",
    "    )\n",
    "\n",
    "    dt = merged_df[ttm2] - merged_df[ttm1]\n",
    "    merged_df[f\"cal_{index_code}_rf\"] = np.where(\n",
    "        dt > 0,\n",
    "        100.0 * merged_df[\"implied_forward_raw\"] * (360.0 / dt),\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # OIS-implied forward\n",
    "    merged_df[\"ois_fwd_raw\"] = (\n",
    "        (1.0 + merged_df[\"OIS\"] * merged_df[ttm2] / 360.0) /\n",
    "        (1.0 + merged_df[\"OIS\"] * merged_df[ttm1] / 360.0)\n",
    "        - 1.0\n",
    "    )\n",
    "    merged_df[f\"ois_fwd_{index_code}\"] = np.where(\n",
    "        dt > 0,\n",
    "        merged_df[\"ois_fwd_raw\"] * (360.0 / dt) * 100.0,\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    # Spread\n",
    "    spread_col = f\"spread_{index_code}\"\n",
    "    merged_df[spread_col] = merged_df[f\"cal_{index_code}_rf\"] - merged_df[f\"ois_fwd_{index_code}\"]\n",
    "    # 8) BN outlier filter\n",
    "    merged_df = barndorff_nielsen_filter(merged_df, spread_col, date_col=\"Date\", window=45, threshold=10)\n",
    "    # If outlier => set cal_rf & spread to NaN\n",
    "    out_mask = merged_df[f\"{spread_col}_filtered\"].isna()\n",
    "    outliers_count = out_mask.sum()\n",
    "    if outliers_count > 0:\n",
    "        logger.info(f\"[{index_code}] Setting {outliers_count} outliers to NaN for cal_{index_code}_rf & {spread_col}\")\n",
    "    merged_df.loc[out_mask, f\"cal_{index_code}_rf\"] = np.nan\n",
    "    merged_df.loc[out_mask, spread_col] = np.nan\n",
    "\n",
    "    # Multiply spread by 100 => bps\n",
    "\n",
    "    merged_df[spread_col] = merged_df[spread_col] * 100.0\n",
    "    merged_df.set_index(\"Date\", inplace=True)\n",
    "    out_file = Path(PROCESSED_DIR) / f\"{index_code}_Forward_Rates.csv\"\n",
    "    merged_df.to_csv(out_file)\n",
    "    logger.info(f\"[{index_code}] Final forward rates shape: {merged_df.shape}, saved to {out_file}\")\n",
    "    logger.info(\n",
    "        f\"[{index_code}] Sample final rows:\\n\"\n",
    "        + merged_df[[f\"cal_{index_code}_rf\", f\"ois_fwd_{index_code}\", spread_col]].tail(5).to_string()\n",
    "    )\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def plot_all_indices(results: dict, keep_dates: bool = True):\n",
    "    \"\"\"\n",
    "    Generate two plots:\n",
    "    1) From START_DATE to 01/01/2020\n",
    "    2) From START_DATE to END_DATE\n",
    "\n",
    "    If keep_dates=True, reindex all series to the union of dates in results.\n",
    "    \"\"\"\n",
    "    START_DATE = pd.to_datetime(config(\"START_DATE\"))\n",
    "    END_DATE = pd.to_datetime(config(\"END_DATE\"))\n",
    "    MID_DATE = pd.to_datetime(\"2020-01-01\")\n",
    "\n",
    "    def _plot(date_range, filename_suffix):\n",
    "        plt.figure(figsize=(12, 7))\n",
    "\n",
    "        date_index = None\n",
    "        if keep_dates:\n",
    "            all_dates = set()\n",
    "            for df in results.values():\n",
    "                if df is not None and not df.empty:\n",
    "                    all_dates.update(df.index)\n",
    "            date_index = pd.to_datetime(sorted(all_dates))\n",
    "\n",
    "        colors = {\"SPX\": \"blue\", \"NDX\": \"green\", \"INDU\": \"red\"}\n",
    "\n",
    "        for idx, df in results.items():\n",
    "            if df is not None and not df.empty:\n",
    "                spread_col = f\"spread_{idx}\"\n",
    "                df_plot = df.reindex(date_index).ffill() if keep_dates and date_index is not None else df\n",
    "                df_plot = df_plot.loc[(df_plot.index >= START_DATE) & (df_plot.index <= date_range)]\n",
    "                \n",
    "                plt.plot(df_plot.index, df_plot[spread_col], color=colors.get(idx, \"black\"), alpha=0.8, label=f\"{idx} Spread (bps)\")\n",
    "\n",
    "        plt.axhline(0, color=\"k\", linestyle=\"--\", alpha=0.7)\n",
    "        plt.title(f\"Implied Forward Spread Across Indices (bps)\\n[{START_DATE.date()} to {date_range.date()}]\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Spread (bps)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        ax = plt.gca()\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "\n",
    "        out_png = Path(OUTPUT_DIR) / f\"all_indices_spread_{filename_suffix}.png\"\n",
    "        plt.savefig(out_png, dpi=300)\n",
    "        logger.info(f\"Saved plot to {out_png}\")\n",
    "        plt.close()\n",
    "\n",
    "    # Generate two plots with different date ranges\n",
    "    _plot(MID_DATE, \"to_2020\")\n",
    "    _plot(END_DATE, \"to_present\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    logger.info(\"== Starting forward rate calculations with compounding dividends, single OIS, BN outlier filter ==\")\n",
    "    results = {}\n",
    "    for idx in INDEX_CODES:\n",
    "        df_res = process_index_forward_rates(idx)\n",
    "        results[idx] = df_res\n",
    "    plot_all_indices(results, keep_dates=True)\n",
    "\n",
    "    logger.info(\"All computations completed successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b108d",
   "metadata": {},
   "source": [
    "## CIP\n",
    "\n",
    "```python\n",
    "def task_clean_data():\n",
    "    \"\"\"Run the CIP data cleaning script.\"\"\"\n",
    "    tidy_data_file = DATA_DIR / \"tidy_data.csv\"\n",
    "    return {\n",
    "        \"actions\": [\"ipython ./src/clean_data.py\"],\n",
    "        \"file_dep\": [str(MANUAL_DATA_DIR / \"CIP_2025.xlsx\")],\n",
    "        \"targets\": [str(tidy_data_file)],\n",
    "        \"clean\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check this works\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from pull_bloomberg_cip_data import *\n",
    "    import pull_bloomberg_cip_data as pull_bloomberg_cip_data\n",
    "except ModuleNotFoundError:\n",
    "    from src.pull_bloomberg_cip_data import *\n",
    "    import src.pull_bloomberg_cip_data as pull_bloomberg_cip_data\n",
    "\n",
    "try:\n",
    "    from settings import config\n",
    "except ModuleNotFoundError:\n",
    "    from src.settings import config\n",
    "\n",
    "DATA_DIR = Path(config(\"DATA_DIR\"))  # Should point to '_data'\n",
    "\n",
    "df = pull_bloomberg_cip_data.load_raw('2025-03-01')\n",
    "output_file = DATA_DIR / \"tidy_data.csv\"\n",
    "\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure the root directory (CIP/) is in sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n",
    "\n",
    "try:\n",
    "    import src.settings as settings  # Try to import normally\n",
    "except ModuleNotFoundError:\n",
    "    import settings as settings # Fallback if src.settings isn't found\n",
    "\n",
    "\n",
    "BLOOMBERG = settings.BLOOMBERG\n",
    "\n",
    "def download():\n",
    "    target_file = \"./data_manual/CIP_2025.xlsx\"\n",
    "    import requests, os\n",
    "    url = \"https://raw.githubusercontent.com/Kunj121/CIP_DATA/main/CIP_2025%20(1).xlsx\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an error on bad responses\n",
    "    # Ensure the data_manual folder exists.\n",
    "    os.makedirs(os.path.dirname(target_file), exist_ok=True)\n",
    "    # Write the file to the target path.\n",
    "    with open(target_file, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    df =  pd.read_excel(target_file)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_bloomberg_historical_data(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch historical data from Bloomberg using xbbg for predefined sets of tickers,\n",
    "    clean up the data, and merge into a single DataFrame similar to the existing process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_date : str, optional\n",
    "        Start date in 'YYYY-MM-DD' format, defaults to \"2010-01-01\"\n",
    "    end_date : str, optional\n",
    "        End date in 'YYYY-MM-DD' format, defaults to \"2025-12-31\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A merged DataFrame containing all the processed historical data\n",
    "        (spot rates, swap rates, interest rates) for AUD, CAD, CHF, EUR,\n",
    "        GBP, JPY, NZD, and SEK (with USD as reference).\n",
    "    \"\"\"\n",
    "    from xbbg import blp\n",
    "    start_date = \"2010-01-01\"\n",
    "    end_date = \"2025-12-31\"\n",
    "\n",
    "    # Tickers for Spot Rates\n",
    "    interest_rates = [\n",
    "        \"ADSOC CMPN Curncy\",  # AUD\n",
    "        \"CDSOC CMPN Curncy\",  # CAD\n",
    "        \"SFSNTC CMPN Curncy\", # CHF\n",
    "        \"EUSWEC CMPN Curncy\", # EUR\n",
    "        \"BPSWSC CMPN Curncy\", # GBP\n",
    "        \"JYSOC CMPN Curncy\",  # JPY\n",
    "        \"NDSOC CMPN Curncy\",  # NZD\n",
    "        \"SKSWTNC BGN Curncy\", # SEK\n",
    "        \"USSOC CMPN Curncy\",  # USD\n",
    "    ]\n",
    "\n",
    "    # Tickers for 3M interest rates (overnight or 3M LIBOR, depending on the data vendor)\n",
    "    forward_rates = [\n",
    "        \"AUD3M CMPN Curncy\",\n",
    "        \"CAD3M CMPN Curncy\",\n",
    "        \"CHF3M CMPN Curncy\",\n",
    "        \"EUR3M CMPN Curncy\",\n",
    "        \"GBP3M CMPN Curncy\",\n",
    "        \"JPY3M CMPN Curncy\",\n",
    "        \"NZD3M CMPN Curncy\",\n",
    "        \"SEK3M CMPN Curncy\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Tickers for 3M swap rates (or 3M forward quotes)\n",
    "    spot_rates = [\n",
    "        \"AUD CMPN Curncy\",\n",
    "        \"CAD CMPN Curncy\",\n",
    "        \"CHF CMPN Curncy\",\n",
    "        \"EUR CMPN Curncy\",\n",
    "        \"GBP CMPN Curncy\",\n",
    "        \"JPY CMPN Curncy\",\n",
    "        \"NZD CMPN Curncy\",\n",
    "        \"SEK CMPN Curncy\"\n",
    "    ]\n",
    "\n",
    "    # Mapping from Bloomberg columns to simpler names\n",
    "   \n",
    "    IR_mapping = {\n",
    "        \"ADSOC CMPN Curncy_PX_LAST\": \"AUD_IR\",\n",
    "        \"CDSOC CMPN Curncy_PX_LAST\": \"CAD_IR\",\n",
    "        \"SFSNTC CMPN Curncy_PX_LAST\": \"CHF_IR\",\n",
    "        \"EUSWEC CMPN Curncy_PX_LAST\": \"EUR_IR\",\n",
    "        \"BPSWSC CMPN Curncy_PX_LAST\": \"GBP_IR\",\n",
    "        \"JYSOC CMPN Curncy_PX_LAST\": \"JPY_IR\",\n",
    "        \"NDSOC CMPN Curncy_PX_LAST\": \"NZD_IR\",\n",
    "        \"SKSWTNC BGN Curncy_PX_LAST\": \"SEK_IR\",\n",
    "        \"USSOC CMPN Curncy_PX_LAST\": \"USD_IR\"\n",
    "    }\n",
    "\n",
    "    forward_mapping = {\n",
    "        \"AUD3M CMPN Curncy_PX_LAST\": \"AUD_CURNCY3M\",\n",
    "        \"CAD3M CMPN Curncy_PX_LAST\": \"CAD_CURNCY3M\",\n",
    "        \"CHF3M CMPN Curncy_PX_LAST\": \"CHF_CURNCY3M\",\n",
    "        \"EUR3M CMPN Curncy_PX_LAST\": \"EUR_CURNCY3M\",\n",
    "        \"GBP3M CMPN Curncy_PX_LAST\": \"GBP_CURNCY3M\",\n",
    "        \"JPY3M CMPN Curncy_PX_LAST\": \"JPY_CURNCY3M\",\n",
    "        \"NZD3M CMPN Curncy_PX_LAST\": \"NZD_CURNCY3M\",\n",
    "        \"SEK3M CMPN Curncy_PX_LAST\": \"SEK_CURNCY3M\"\n",
    "    }\n",
    "\n",
    "    spot_mapping = {\n",
    "        \"AUD CMPN CURNCY_PX_LAST\": \"AUD_CURNCY\",\n",
    "        \"CAD CMPN CURNCY_PX_LAST\": \"CAD_CURNCY\",\n",
    "        \"CHF CMPN CURNCY_PX_LAST\": \"CHF_CURNCY\",\n",
    "        \"EUR CMPN CURNCY_PX_LAST\": \"EUR_CURNCY\",\n",
    "        \"GBP CMPN CURNCY_PX_LAST\": \"GBP_CURNCY\",\n",
    "        \"JPY CMPN CURNCY_PX_LAST\": \"JPY_CURNCY\",\n",
    "        \"NZD CMPN CURNCY_PX_LAST\": \"NZD_CURNCY\",\n",
    "        \"SEK CMPN CURNCY_PX_LAST\": \"SEK_CURNCY\"\n",
    "    }\n",
    "\n",
    "    fields = [\"PX_LAST\"]\n",
    "\n",
    "    # Helper to flatten the multi-index\n",
    "    def process_df(df, column_mapping):\n",
    "        if not df.empty:\n",
    "            df.columns = [f\"{t[0]}_{t[1]}\" for t in df.columns]\n",
    "            df.rename(columns=column_mapping, inplace=True)\n",
    "            df.set_index('date', inplace=True)\n",
    "        return df\n",
    "\n",
    "    # Pull each set of tickers\n",
    "    interest_rates_df = process_df(\n",
    "        blp.bdh(\n",
    "            tickers=interest_rates,\n",
    "            flds=fields,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "        ),\n",
    "        IR_mapping\n",
    "    )\n",
    "\n",
    "    forward_rates_df = process_df(\n",
    "        blp.bdh(\n",
    "            tickers=forward_rates,\n",
    "            flds=fields,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "        ),\n",
    "        forward_mapping\n",
    "    )\n",
    "\n",
    "    exchange_rates_df = process_df(\n",
    "        blp.bdh(\n",
    "            tickers=spot_rates,\n",
    "            flds=fields,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "        ),\n",
    "        spot_mapping\n",
    "    )\n",
    "\n",
    "    # For demonstration, we replicate the \"3M forward\" concept by appending\n",
    "    # the spot data (currency_df) to the swap rates (swap_df).\n",
    "    # In a real scenario, you might need the forward points separately\n",
    "    # to add to / subtract from spot. This code treats 'swap_df' as 3M rates.\n",
    "\n",
    "    # We keep the original spot rates in swap_df as well for reference:\n",
    "    cols = [\"AUD\", \"CAD\", \"CHF\", \"EUR\", \"GBP\", \"JPY\", \"NZD\", \"SEK\"]\n",
    "    cols_IR = [\"AUD\", \"CAD\", \"CHF\", \"EUR\", \"GBP\", \"JPY\", \"NZD\", \"SEK\", \"USD\"]\n",
    "    exchange_rates_df.columns = cols\n",
    "    forward_rates_df.columns = cols\n",
    "    interest_rates_df.columns = cols_IR\n",
    "\n",
    "    # Convert certain currencies to reciprocals\n",
    "    # The forward df is actually forward points, so we need to make this into forward rates.\n",
    "    forward_rates_df[[c for c in cols if c != 'JPY']] /= 10000\n",
    "    forward_rates_df['JPY'] /= 100\n",
    "    forward_rates_df = exchange_rates_df + forward_rates_df\n",
    "\n",
    "    exchange_rates_df.columns = [name+\"_CURNCY\" for name in exchange_rates_df.columns]\n",
    "    forward_rates.columns = [name+\"_CURNCY3M\" for name in forward_rates.columns]\n",
    "    interest_rates.columns = [name+\"_IR\" for name in interest_rates.columns]\n",
    "\n",
    "    # Merge all\n",
    "    df_merged = (\n",
    "        exchange_rates_df\n",
    "        .merge(forward_rates_df, left_index=True, right_index=True, how='inner')\n",
    "        .merge(interest_rates_df, left_index=True, right_index=True, how='inner')\n",
    "    )\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "def plot_cip(end ='2025-03-01'):\n",
    "    \"\"\"\n",
    "    Reads data from Excel if excel=True, otherwise fetch from Bloomberg using xbbg.\n",
    "\n",
    "    After retrieving data, calculates a CIP measure and cleans outliers.\n",
    "    Finally, plots the CIP spreads for a 2010-2019 subset and the full range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start : str, optional\n",
    "        Start date in 'YYYY-MM-DD' format, used if excel=False\n",
    "    end : str, optional\n",
    "        End date in 'YYYY-MM-DD' format, used if excel=False\n",
    "    excel : bool, optional\n",
    "        If True, read from a local Excel file. If False, use Bloomberg xbbg.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_merged : pandas.DataFrame\n",
    "        Final cleaned DataFrame with CIP spreads and underlying data.\n",
    "    \"\"\"\n",
    "    start = '2010-01-01'\n",
    "    if BLOOMBERG == False:\n",
    "        possible_paths = [\n",
    "            \"./data_manual/CIP_2025.xlsx\",\n",
    "            \"../data_manual/CIP_2025.xlsx\",\n",
    "        ]\n",
    "\n",
    "        data = None\n",
    "        for filepath in possible_paths:\n",
    "            if os.path.exists(filepath):\n",
    "                try:\n",
    "                    data = pd.read_excel(filepath, sheet_name=None)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {filepath}: {e}\")\n",
    "            else:\n",
    "                pass\n",
    "        if data is None:\n",
    "            raise FileNotFoundError(\"Could not find or load the CIP_2025.xlsx file in any of the expected locations\")\n",
    "\n",
    "        df_spot = data[\"Spot\"]\n",
    "        exchange_rates = df_spot.set_index(\"Date\")\n",
    "\n",
    "        df_forward = data[\"Forward\"]\n",
    "        forward_rates = df_forward.set_index(\"Date\")\n",
    "\n",
    "        df_ir = data[\"OIS\"]\n",
    "        interest_rates = df_ir.set_index(\"Date\")\n",
    "        # Standard columns\n",
    "        cols = [\"AUD\", \"CAD\", \"CHF\", \"EUR\", \"GBP\", \"JPY\", \"NZD\", \"SEK\"]\n",
    "        exchange_rates.columns = cols\n",
    "        forward_rates.columns = cols\n",
    "\n",
    "        # Convert forward points to forward rates\n",
    "        # Non-JPY: forward points are per 10,000; JPY: per 100\n",
    "        forward_rates[[c for c in cols if c != 'JPY']] /= 10000\n",
    "        forward_rates['JPY'] /= 100\n",
    "        forward_rates = exchange_rates + forward_rates\n",
    "\n",
    "        # Rename to keep track\n",
    "        exchange_rates.columns = [f\"{name}_CURNCY\" for name in exchange_rates.columns]\n",
    "        forward_rates.columns = [f\"{name}_CURNCY3M\" for name in forward_rates.columns]\n",
    "        interest_rates.columns = [f\"{name}_IR\" for name in interest_rates.columns]\n",
    "\n",
    "        # Merge\n",
    "        df_merged = (\n",
    "            exchange_rates\n",
    "            .merge(forward_rates, left_index=True, right_index=True, how='inner')\n",
    "            .merge(interest_rates, left_index=True, right_index=True, how='inner')\n",
    "        )\n",
    "\n",
    "        # Convert to reciprocal for these currencies\n",
    "        reciprocal_currencies = ['EUR', 'GBP', 'AUD', 'NZD']\n",
    "        for ccy in reciprocal_currencies:\n",
    "            df_merged[f\"{ccy}_CURNCY\"] = 1.0 / df_merged[f\"{ccy}_CURNCY\"]\n",
    "            df_merged[f\"{ccy}_CURNCY3M\"] = 1.0 / df_merged[f\"{ccy}_CURNCY3M\"]\n",
    "\n",
    "    else:\n",
    "        # 2) Pull from Bloomberg\n",
    "        df_merged = fetch_bloomberg_historical_data(start, end)\n",
    "\n",
    "    # List of all the core currencies\n",
    "    currencies = ['AUD', 'CAD', 'CHF', 'EUR', 'GBP', 'JPY', 'NZD', 'SEK']\n",
    "\n",
    "    ######################################\n",
    "    # Compute the log CIP basis in basis points\n",
    "    ######################################\n",
    "    for ccy in currencies:\n",
    "        fwd_col    = f'{ccy}_CURNCY3M'\n",
    "        spot_col   = f'{ccy}_CURNCY'\n",
    "        ir_col     = f'{ccy}_IR'\n",
    "        usd_ir_col = 'USD_IR'  # The US interest rate column\n",
    "\n",
    "        # CIP in log terms (bps) = 100*100 x [ domestic_i - (logF - logS)*(360/90) - foreign_i ]\n",
    "        cip_col = f'CIP_{ccy}_ln'\n",
    "        df_merged[cip_col] = 100*100 * (\n",
    "            (df_merged[ir_col] / 100.0)               # domestic interest rate\n",
    "            - (360.0 / 90.0) * (\n",
    "                np.log(df_merged[fwd_col]) - np.log(df_merged[spot_col])\n",
    "            )\n",
    "            - (df_merged[usd_ir_col] / 100.0)         # foreign interest rate (USD)\n",
    "        )\n",
    "\n",
    "    ######################################\n",
    "    # Rolling outlier cleanup (45-day window)\n",
    "    ######################################\n",
    "    window_size = 45\n",
    "    for ccy in currencies:\n",
    "        cip_col = f'CIP_{ccy}_ln'\n",
    "        if cip_col not in df_merged.columns:\n",
    "            continue\n",
    "\n",
    "        # Rolling median over 45 days\n",
    "        rolling_median = df_merged[cip_col].rolling(window_size).median()\n",
    "\n",
    "        # Absolute deviation from median\n",
    "        abs_dev = (df_merged[cip_col] - rolling_median).abs()\n",
    "\n",
    "        # Rolling mean of abs_dev (proxy for MAD)\n",
    "        rolling_mad = abs_dev.rolling(window_size).mean()\n",
    "\n",
    "        # Mark outliers (abs_dev / mad >= 10) and replace with NaN\n",
    "        outlier_mask = (abs_dev / rolling_mad) >= 10\n",
    "        df_merged.loc[outlier_mask, cip_col] = np.nan\n",
    "\n",
    "    # Create a separate DataFrame for the CIP columns\n",
    "    cip_cols = [f'CIP_{c}_ln' for c in currencies if f'CIP_{c}_ln' in df_merged.columns]\n",
    "    spreads = df_merged[cip_cols].copy()\n",
    "\n",
    "    # Shorten column names for plotting\n",
    "    spreads.columns = [c[4:7] for c in spreads.columns]  # e.g., CIP_AUD_ln -> AUD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plot_spreads(spreads_df, yr):\n",
    "        \"\"\"\n",
    "        Plots the CIP spreads in basis points.\n",
    "        Saves both a PDF and PNG with the specified yr suffix in the filename.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(13, 8), dpi=300)\n",
    "\n",
    "        # Plot each column in the DataFrame\n",
    "        for column in spreads_df.columns:\n",
    "            ax.plot(spreads_df.index, spreads_df[column], label=column, linewidth=1, antialiased=True)\n",
    "\n",
    "        ax.set_xlabel(\"Dates\", fontsize=14)\n",
    "        ax.set_ylabel(\"Arbitrage Spread (bps)\", fontsize=14)\n",
    "\n",
    "        # Format the x-axis for years\n",
    "        ax.xaxis.set_major_locator(mdates.YearLocator(2))\n",
    "        ax.xaxis.set_minor_locator(mdates.YearLocator(1))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        plt.xticks(rotation=0)\n",
    "\n",
    "        # Horizontal grid lines only\n",
    "        ax.yaxis.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "        ax.xaxis.grid(False)\n",
    "\n",
    "        # Hard limit the y-axis\n",
    "        ax.set_ylim([-50, 210])\n",
    "\n",
    "        # Legend below the plot\n",
    "        ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=4, fontsize=12, frameon=True)\n",
    "\n",
    "        # Remove top and right spines\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.15, 1, 1])  # ensure legend fits\n",
    "\n",
    "        plt.savefig(f\"spread_plot_{yr}.pdf\", format=\"pdf\", bbox_inches='tight')\n",
    "        plt.savefig(f\"spread_plot_{yr}.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # Plot from start to 2019, and the full range\n",
    "    if isinstance(df_merged.index, pd.DatetimeIndex):\n",
    "        plot_spreads(spreads.loc[:end], 'rep')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_raw(end ='2025-03-01', plot = False):\n",
    "    \"\"\"\n",
    "    Reads data from Excel if excel=True, otherwise fetch from Bloomberg using xbbg.\n",
    "\n",
    "    After retrieving data, calculates a CIP measure and cleans outliers.\n",
    "    Finally, plots the CIP spreads for a 2010-2019 subset and the full range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start : str, optional\n",
    "        Start date in 'YYYY-MM-DD' format, used if excel=False\n",
    "    end : str, optional\n",
    "        End date in 'YYYY-MM-DD' format, used if excel=False\n",
    "    excel : bool, optional\n",
    "        If True, read from a local Excel file. If False, use Bloomberg xbbg.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_merged : pandas.DataFrame\n",
    "        Final cleaned DataFrame with CIP spreads and underlying data.\n",
    "    \"\"\"\n",
    "\n",
    "    start = '2010-01-01'\n",
    "    if BLOOMBERG == False:\n",
    "        possible_paths = [\n",
    "            \"./data_manual/CIP_2025.xlsx\",\n",
    "            \"../data_manual/CIP_2025.xlsx\",\n",
    "        ]\n",
    "\n",
    "        data = None\n",
    "        for filepath in possible_paths:\n",
    "            if os.path.exists(filepath):\n",
    "                try:\n",
    "                    data = pd.read_excel(filepath, sheet_name=None)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {filepath}: {e}\")\n",
    "            else:\n",
    "                pass\n",
    "        if data is None:\n",
    "            download()\n",
    "        data = pd.read_excel(filepath, sheet_name=None, parse_dates=['Date'])\n",
    "\n",
    "        df_spot = data[\"Spot\"]\n",
    "        exchange_rates = df_spot.set_index(\"Date\")\n",
    "\n",
    "        df_forward = data[\"Forward\"]\n",
    "        forward_rates = df_forward.set_index(\"Date\")\n",
    "\n",
    "        df_ir = data[\"OIS\"]\n",
    "        interest_rates = df_ir.set_index(\"Date\")\n",
    "\n",
    "        # Standard columns\n",
    "        cols = [\"AUD\", \"CAD\", \"CHF\", \"EUR\", \"GBP\", \"JPY\", \"NZD\", \"SEK\"]\n",
    "        exchange_rates.columns = cols\n",
    "        forward_rates.columns = cols\n",
    "\n",
    "        # Convert forward points to forward rates\n",
    "        # Non-JPY: forward points are per 10,000; JPY: per 100\n",
    "        forward_rates[[c for c in cols if c != 'JPY']] /= 10000\n",
    "        forward_rates['JPY'] /= 100\n",
    "        forward_rates = exchange_rates + forward_rates\n",
    "\n",
    "        # Rename to keep track\n",
    "        exchange_rates.columns = [f\"{name}_CURNCY\" for name in exchange_rates.columns]\n",
    "        forward_rates.columns = [f\"{name}_CURNCY3M\" for name in forward_rates.columns]\n",
    "        interest_rates.columns = [f\"{name}_IR\" for name in interest_rates.columns]\n",
    "\n",
    "        # Merge\n",
    "        df_merged = (\n",
    "            exchange_rates\n",
    "            .merge(forward_rates, left_index=True, right_index=True, how='inner')\n",
    "            .merge(interest_rates, left_index=True, right_index=True, how='inner')\n",
    "        )\n",
    "\n",
    "        # Convert to reciprocal for these currencies\n",
    "        reciprocal_currencies = ['EUR', 'GBP', 'AUD', 'NZD']\n",
    "        for ccy in reciprocal_currencies:\n",
    "            df_merged[f\"{ccy}_CURNCY\"] = 1.0 / df_merged[f\"{ccy}_CURNCY\"]\n",
    "            df_merged[f\"{ccy}_CURNCY3M\"] = 1.0 / df_merged[f\"{ccy}_CURNCY3M\"]\n",
    "\n",
    "\n",
    "    else:\n",
    "        # 2) Pull from Bloomberg\n",
    "        df_merged = fetch_bloomberg_historical_data(start, end)\n",
    "\n",
    "\n",
    "    return df_merged.loc[:end]\n",
    "\n",
    "def compute_cip(end = '2020-01-01'):\n",
    "    df_merged = load_raw(end = end)\n",
    "\n",
    "    # List of all the core currencies\n",
    "    currencies = ['AUD', 'CAD', 'CHF', 'EUR', 'GBP', 'JPY', 'NZD', 'SEK']\n",
    "\n",
    "    ######################################\n",
    "    # Compute the log CIP basis in basis points\n",
    "    ######################################\n",
    "    for ccy in currencies:\n",
    "        fwd_col = f'{ccy}_CURNCY3M'\n",
    "        spot_col = f'{ccy}_CURNCY'\n",
    "        ir_col = f'{ccy}_IR'\n",
    "        usd_ir_col = 'USD_IR'  # The US interest rate column\n",
    "\n",
    "        # CIP in log terms (bps) = 100*100 x [ domestic_i - (logF - logS)*(360/90) - foreign_i ]\n",
    "        cip_col = f'CIP_{ccy}_ln'\n",
    "        df_merged[cip_col] = 100 * 100 * (\n",
    "                (df_merged[ir_col] / 100.0)  # domestic interest rate\n",
    "                - (360.0 / 90.0) * (\n",
    "                        np.log(df_merged[fwd_col]) - np.log(df_merged[spot_col])\n",
    "                )\n",
    "                - (df_merged[usd_ir_col] / 100.0)  # foreign interest rate (USD)\n",
    "        )\n",
    "\n",
    "    ######################################\n",
    "    # Rolling outlier cleanup (45-day window)\n",
    "    ######################################\n",
    "    window_size = 45\n",
    "    for ccy in currencies:\n",
    "        cip_col = f'CIP_{ccy}_ln'\n",
    "        if cip_col not in df_merged.columns:\n",
    "            continue\n",
    "\n",
    "        # Rolling median over 45 days\n",
    "        rolling_median = df_merged[cip_col].rolling(window_size).median()\n",
    "\n",
    "        # Absolute deviation from median\n",
    "        abs_dev = (df_merged[cip_col] - rolling_median).abs()\n",
    "\n",
    "        # Rolling mean of abs_dev (proxy for MAD)\n",
    "        rolling_mad = abs_dev.rolling(window_size).mean()\n",
    "\n",
    "        # Mark outliers (abs_dev / mad >= 10) and replace with NaN\n",
    "        outlier_mask = (abs_dev / rolling_mad) >= 10\n",
    "        df_merged.loc[outlier_mask, cip_col] = np.nan\n",
    "\n",
    "    # Create a separate DataFrame for the CIP columns\n",
    "    cip_cols = [f'CIP_{c}_ln' for c in currencies if f'CIP_{c}_ln' in df_merged.columns]\n",
    "    spreads = df_merged[cip_cols].copy()\n",
    "\n",
    "    # Shorten column names for plotting\n",
    "    spreads.columns = [c[4:7] for c in spreads.columns]  # e.g., CIP_AUD_ln -> AUD\n",
    "\n",
    "    return df_merged.iloc[:, -8:]\n",
    "\n",
    "\n",
    "def load_raw_pieces(end ='2025-03-01',excel=False, plot = False):\n",
    "    \"\"\"\n",
    "    Reads data from Excel if excel=True, otherwise fetch from Bloomberg using xbbg.\n",
    "\n",
    "    After retrieving data, calculates a CIP measure and cleans outliers.\n",
    "    Finally, plots the CIP spreads for a 2010-2019 subset and the full range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    start : str, optional\n",
    "        Start date in 'YYYY-MM-DD' format, used if excel=False\n",
    "    end : str, optional\n",
    "        End date in 'YYYY-MM-DD' format, used if excel=False\n",
    "    excel : bool, optional\n",
    "        If True, read from a local Excel file. If False, use Bloomberg xbbg.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_merged : pandas.DataFrame\n",
    "        Final cleaned DataFrame with CIP spreads and underlying data.\n",
    "    \"\"\"\n",
    "    start = '2010-01-01'\n",
    "    if BLOOMBERG == False:\n",
    "        possible_paths = [\n",
    "            \"./data_manual/CIP_2025.xlsx\",\n",
    "            \"../data_manual/CIP_2025.xlsx\",\n",
    "        ]\n",
    "\n",
    "        data = None\n",
    "        for filepath in possible_paths:\n",
    "            if os.path.exists(filepath):\n",
    "                try:\n",
    "                    data = pd.read_excel(filepath, sheet_name=None)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {filepath}: {e}\")\n",
    "            else:\n",
    "                pass\n",
    "        df_spot = data[\"Spot\"]\n",
    "        exchange_rates = df_spot.set_index(\"Date\")\n",
    "\n",
    "        df_forward = data[\"Forward\"]\n",
    "        forward_rates = df_forward.set_index(\"Date\")\n",
    "\n",
    "        df_ir = data[\"OIS\"]\n",
    "        interest_rates = df_ir.set_index(\"Date\")\n",
    "\n",
    "        # Standard columns\n",
    "        cols = [\"AUD\", \"CAD\", \"CHF\", \"EUR\", \"GBP\", \"JPY\", \"NZD\", \"SEK\"]\n",
    "        exchange_rates.columns = cols\n",
    "        forward_rates.columns = cols\n",
    "\n",
    "        # Convert forward points to forward rates\n",
    "        # Non-JPY: forward points are per 10,000; JPY: per 100\n",
    "        forward_rates[[c for c in cols if c != 'JPY']] /= 10000\n",
    "        forward_rates['JPY'] /= 100\n",
    "        forward_rates = exchange_rates + forward_rates\n",
    "\n",
    "        # Rename to keep track\n",
    "        exchange_rates.columns = [f\"{name}_CURNCY\" for name in exchange_rates.columns]\n",
    "        forward_rates.columns = [f\"{name}_CURNCY3M\" for name in forward_rates.columns]\n",
    "        interest_rates.columns = [f\"{name}_IR\" for name in interest_rates.columns]\n",
    "\n",
    "        # Merge\n",
    "        df_merged = (\n",
    "            exchange_rates\n",
    "            .merge(forward_rates, left_index=True, right_index=True, how='inner')\n",
    "            .merge(interest_rates, left_index=True, right_index=True, how='inner')\n",
    "        )\n",
    "\n",
    "        # Convert to reciprocal for these currencies\n",
    "        reciprocal_currencies = ['EUR', 'GBP', 'AUD', 'NZD']\n",
    "        for ccy in reciprocal_currencies:\n",
    "            df_merged[f\"{ccy}_CURNCY\"] = 1.0 / df_merged[f\"{ccy}_CURNCY\"]\n",
    "            df_merged[f\"{ccy}_CURNCY3M\"] = 1.0 / df_merged[f\"{ccy}_CURNCY3M\"]\n",
    "\n",
    "    else:\n",
    "        # 2) Pull from Bloomberg\n",
    "        df_merged = fetch_bloomberg_historical_data(start, end)\n",
    "\n",
    "\n",
    "    exchange_rates_df = df_merged.iloc[:,:8]\n",
    "    forward_rates_df = df_merged.iloc[:, 8:16]\n",
    "    interest_rates_df = df_merged.iloc[:, -9:]\n",
    "\n",
    "    return exchange_rates_df, forward_rates_df, interest_rates_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05f6989",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def task_summary_stats():\n",
    "    \"\"\"Generate summary statistics and save them as HTML files.\"\"\"\n",
    "    def generate_summary():\n",
    "        # Ensure we correctly import the required functions\n",
    "        from src.directory_functions import save_cip_statistics_as_html\n",
    "        from src.pull_bloomberg_cip_data import compute_cip\n",
    "        from src.cip_analysis import compute_cip_statistics\n",
    "\n",
    "        # Step 1: Compute CIP data\n",
    "        cip_data = compute_cip()\n",
    "\n",
    "        # Step 2: Compute statistics\n",
    "        stats_dict = compute_cip_statistics(cip_data)\n",
    "\n",
    "        # Debugging: Check if 'overall_statistics' exists\n",
    "        if \"overall_statistics\" not in stats_dict:\n",
    "            raise KeyError(\"Error: 'overall_statistics' key is missing from stats_dict!\")\n",
    "\n",
    "        # Step 3: Save statistics as HTML\n",
    "        save_cip_statistics_as_html(stats_dict)\n",
    "\n",
    "    return {\n",
    "        \"actions\": [generate_summary],\n",
    "        \"file_dep\": [\"./src/pull_bloomberg_cip_data.py\", \"./src/cip_analysis.py\"],\n",
    "        \"targets\": [\n",
    "            str(OUTPUT_DIR / \"cip_summary_overall.html\"),\n",
    "            str(OUTPUT_DIR / \"cip_correlation_matrix.html\"),\n",
    "            str(OUTPUT_DIR / \"cip_annual_statistics.html\"),\n",
    "        ],\n",
    "        \"clean\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec453eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_cip_statistics_as_html(stats_dict):\n",
    "    output_dir = os.path.join(OUTPUT_DIR, \"html_files\")\n",
    "    \"\"\"Save CIP statistics as HTML files.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    overall_html = os.path.join(output_dir, \"cip_summary_overall.html\")\n",
    "    corr_html = os.path.join(output_dir, \"cip_correlation_matrix.html\")\n",
    "    annual_html = os.path.join(output_dir, \"cip_annual_statistics.html\")\n",
    "\n",
    "    stats_dict[\"overall_statistics\"].to_html(overall_html)\n",
    "    stats_dict[\"correlation_matrix\"].to_html(corr_html)\n",
    "    stats_dict[\"annual_statistics\"].to_html(annual_html)\n",
    "\n",
    "    return overall_html, corr_html, annual_html\n",
    "\n",
    "def compute_cip(end = '2020-01-01'):\n",
    "    df_merged = load_raw(end = end)\n",
    "\n",
    "    # List of all the core currencies\n",
    "    currencies = ['AUD', 'CAD', 'CHF', 'EUR', 'GBP', 'JPY', 'NZD', 'SEK']\n",
    "\n",
    "    ######################################\n",
    "    # Compute the log CIP basis in basis points\n",
    "    ######################################\n",
    "    for ccy in currencies:\n",
    "        fwd_col = f'{ccy}_CURNCY3M'\n",
    "        spot_col = f'{ccy}_CURNCY'\n",
    "        ir_col = f'{ccy}_IR'\n",
    "        usd_ir_col = 'USD_IR'  # The US interest rate column\n",
    "\n",
    "        # CIP in log terms (bps) = 100*100 x [ domestic_i - (logF - logS)*(360/90) - foreign_i ]\n",
    "        cip_col = f'CIP_{ccy}_ln'\n",
    "        df_merged[cip_col] = 100 * 100 * (\n",
    "                (df_merged[ir_col] / 100.0)  # domestic interest rate\n",
    "                - (360.0 / 90.0) * (\n",
    "                        np.log(df_merged[fwd_col]) - np.log(df_merged[spot_col])\n",
    "                )\n",
    "                - (df_merged[usd_ir_col] / 100.0)  # foreign interest rate (USD)\n",
    "        )\n",
    "\n",
    "    ######################################\n",
    "    # Rolling outlier cleanup (45-day window)\n",
    "    ######################################\n",
    "    window_size = 45\n",
    "    for ccy in currencies:\n",
    "        cip_col = f'CIP_{ccy}_ln'\n",
    "        if cip_col not in df_merged.columns:\n",
    "            continue\n",
    "\n",
    "        # Rolling median over 45 days\n",
    "        rolling_median = df_merged[cip_col].rolling(window_size).median()\n",
    "\n",
    "        # Absolute deviation from median\n",
    "        abs_dev = (df_merged[cip_col] - rolling_median).abs()\n",
    "\n",
    "        # Rolling mean of abs_dev (proxy for MAD)\n",
    "        rolling_mad = abs_dev.rolling(window_size).mean()\n",
    "\n",
    "        # Mark outliers (abs_dev / mad >= 10) and replace with NaN\n",
    "        outlier_mask = (abs_dev / rolling_mad) >= 10\n",
    "        df_merged.loc[outlier_mask, cip_col] = np.nan\n",
    "\n",
    "    # Create a separate DataFrame for the CIP columns\n",
    "    cip_cols = [f'CIP_{c}_ln' for c in currencies if f'CIP_{c}_ln' in df_merged.columns]\n",
    "    spreads = df_merged[cip_cols].copy()\n",
    "\n",
    "    # Shorten column names for plotting\n",
    "    spreads.columns = [c[4:7] for c in spreads.columns]  # e.g., CIP_AUD_ln -> AUD\n",
    "\n",
    "    return df_merged.iloc[:, -8:]\n",
    "\n",
    "def compute_cip_statistics(cip_data):\n",
    "    \"\"\"Compute CIP statistics from CIP data.\"\"\"\n",
    "    if cip_data is None or cip_data.empty:\n",
    "        raise ValueError(\"Error: cip_data is empty or None. Check if compute_cip() is working correctly.\")\n",
    "\n",
    "    stats_dict = {}\n",
    "    cip_columns = [col for col in cip_data.columns if col.startswith('CIP_') and col.endswith('_ln')]\n",
    "    cip_df = cip_data[cip_columns]\n",
    "\n",
    "    if cip_df.empty:\n",
    "        raise ValueError(\"Error: cip_df is empty after filtering CIP columns.\")\n",
    "\n",
    "    stats_dict[\"overall_statistics\"] = cip_df.describe()\n",
    "    stats_dict[\"correlation_matrix\"] = cip_df.corr()\n",
    "    cip_data.index = pd.to_datetime(cip_data.index)\n",
    "    stats_dict[\"annual_statistics\"] = cip_df.resample('YE').agg(['mean', 'std', 'min', 'max'])\n",
    "\n",
    "    return stats_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f1be86",
   "metadata": {},
   "source": [
    "## Treasury Spot\n",
    "```python\n",
    "def task_clean_raw():\n",
    "    \"\"\"Run `clean_raw.ipynb` to generate necessary datasets in `_data`.\"\"\"\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            jupyter_execute_notebook(\"clean_raw\")\n",
    "        ],\n",
    "        \"targets\": [\n",
    "            DATA_DIR / \"treasury_df.csv\",\n",
    "            DATA_DIR / \"ois_df.csv\",\n",
    "            DATA_DIR / \"last_day_df.csv\"\n",
    "        ],\n",
    "        \"clean\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51de7940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dates = pd.read_excel(data_file, sheet_name=\"T_SF\", usecols=\"A\", skiprows=6, header=None)\n",
    "df_dates.columns = [\"Date\"]\n",
    "df_dates[\"Date\"] = pd.to_datetime(df_dates[\"Date\"])\n",
    "\n",
    "# Compute month, year, and day\n",
    "df_dates[\"Mat_Month\"] = df_dates[\"Date\"].dt.month\n",
    "df_dates[\"Mat_Year\"] = df_dates[\"Date\"].dt.year\n",
    "df_dates = df_dates.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "# For each month, keep the last date (mimicking: keep if mofd(Date) != mofd(Date[_n+1]))\n",
    "df_last = df_dates.groupby([df_dates[\"Mat_Year\"], df_dates[\"Mat_Month\"]], as_index=False).agg({\"Date\": \"last\"})\n",
    "\n",
    "# Compute the last day of the month\n",
    "df_last[\"Mat_Day\"] = df_last[\"Date\"].dt.day\n",
    "\n",
    "# Drop duplicates and keep required columns\n",
    "df_matday = df_last[[\"Date\", \"Mat_Month\", \"Mat_Year\", \"Mat_Day\"]].copy()\n",
    "output_file = os.path.join(DATA_DIR, \"last_day_df.csv\") \n",
    "# Save as CSV\n",
    "df_matday.to_csv(output_file, index=False)\n",
    "\n",
    "# Display the final DataFrame\n",
    "df_matday.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92761e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load data from the Excel file\n",
    "df = pd.read_excel(data_file, sheet_name=\"T_SF\", skiprows=6, header=None)\n",
    "\n",
    "# Define base column names\n",
    "base_columns = [\"Date\"]\n",
    "tenors = [10, 5, 2, 20, 30]  # Available tenors\n",
    "versions = [1, 2]  # Nearby (1) and Deferred (2) contract versions\n",
    "\n",
    "# Generate column names dynamically\n",
    "col_names = [\"Date\"] + [\n",
    "    f\"{metric}_{v}_{tenor}\" \n",
    "    for v in versions \n",
    "    for tenor in tenors \n",
    "    for metric in [\"Implied_Repo\", \"Vol\", \"Contract\", \"Price\"]\n",
    "]\n",
    "\n",
    "# Assign column names\n",
    "df.columns = col_names\n",
    "\n",
    "# Drop rows with missing dates\n",
    "df = df.dropna(subset=[\"Date\"])\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# Convert numeric columns\n",
    "numeric_cols = [col for col in df.columns if col.startswith((\"Implied_Repo\", \"Vol_\", \"Price_\"))]\n",
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# **Sort columns alphabetically while keeping \"Date\" first**\n",
    "sorted_columns = [\"Date\"] + sorted([col for col in df.columns if col != \"Date\"])\n",
    "df = df[sorted_columns]\n",
    "\n",
    "# Display sorted DataFrame\n",
    "df.head()\n",
    "\n",
    "treasury_df = df.copy()\n",
    "output_file = os.path.join(DATA_DIR, \"treasury_df.csv\") \n",
    "# Save as CSV\n",
    "# Optionally, save it to a file for later use\n",
    "treasury_df.to_csv(output_file, index=False)  # Save as CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725f2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the Excel file\n",
    "df_ois = pd.read_excel(ois_file, header=None)\n",
    "\n",
    "# Drop the first 4 columns\n",
    "df_ois = df_ois.iloc[:, 4:]\n",
    "\n",
    "# Drop the top 3 rows and reset index\n",
    "df_ois = df_ois.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "# Drop rows at index 1 and 2, then reset index again\n",
    "df_ois = df_ois.drop(index=[1, 2]).reset_index(drop=True)\n",
    "\n",
    "# Set the first row as the column headers, then drop it from the DataFrame\n",
    "df_ois.columns = df_ois.iloc[0]\n",
    "df_ois = df_ois[1:].reset_index(drop=True)\n",
    "\n",
    "# Rename the first column to \"Date\"\n",
    "df_ois.rename(columns={df_ois.columns[0]: \"Date\"}, inplace=True)\n",
    "\n",
    "# Define the renaming mapping for OIS columns\n",
    "rename_map = {\n",
    "    \"USSO1Z CMPN Curncy\": \"OIS_1W\",\n",
    "    \"USSOA CMPN Curncy\":  \"OIS_1M\",\n",
    "    \"USSOB CMPN Curncy\":  \"OIS_2M\",\n",
    "    \"USSOC CMPN Curncy\":  \"OIS_3M\",\n",
    "    \"USSOF CMPN Curncy\":  \"OIS_6M\",\n",
    "    \"USSO1 CMPN Curncy\":  \"OIS_1Y\",\n",
    "    \"USSO2 CMPN Curncy\":  \"OIS_2Y\",\n",
    "    \"USSO3 CMPN Curncy\":  \"OIS_3Y\",\n",
    "    \"USSO4 CMPN Curncy\":  \"OIS_4Y\",\n",
    "    \"USSO5 CMPN Curncy\":  \"OIS_5Y\",\n",
    "    \"USSO7 CMPN Curncy\":  \"OIS_7Y\",\n",
    "    \"USSO10 CMPN Curncy\": \"OIS_10Y\",\n",
    "    \"USSO15 CMPN Curncy\": \"OIS_15Y\",\n",
    "    \"USSO20 CMPN Curncy\": \"OIS_20Y\",\n",
    "    \"USSO30 CMPN Curncy\": \"OIS_30Y\"\n",
    "}\n",
    "\n",
    "# Rename the OIS columns\n",
    "df_ois.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "df_ois[\"Date\"] = pd.to_datetime(df_ois[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "output_file = os.path.join(DATA_DIR, \"ois_df.csv\") \n",
    "\n",
    "# Save the cleaned data to CSV\n",
    "df_ois.to_csv(output_file, index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "df_ois.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3bea93",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def task_generate_reference():\n",
    "    \"\"\"Run generate_reference.py to create reference.csv\"\"\"\n",
    "    return {\n",
    "        \"actions\": [\"python ./src/generate_reference.py\"],\n",
    "        \"targets\": [DATA_DIR / \"reference.csv\"],\n",
    "        \"clean\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce4c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from settings import config\n",
    "import load_bases_data  # Ensure this module is correctly located in src/\n",
    "\n",
    "# Get DATA_DIR from config\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "\n",
    "# Ensure DATA_DIR exists\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "df = load_bases_data.load_combined_spreads_wide(data_dir=DATA_DIR)\n",
    "\n",
    "# Forward-fill missing values (limit 5), then drop remaining NaNs\n",
    "df = df.ffill(limit=5).dropna()\n",
    "\n",
    "# Reindex columns in sorted order and filter only columns matching \"Treasury_SF_*\"\n",
    "filtered_df = df.reindex(sorted(df.columns), axis=1).filter(regex=\"^Treasury_SF_\")\n",
    "\n",
    "# Define output file path\n",
    "output_file = os.path.join(DATA_DIR, \"reference.csv\")\n",
    "\n",
    "# Save the filtered DataFrame to CSV in DATA_DIR\n",
    "filtered_df.to_csv(output_file, index=True)\n",
    "\n",
    "# Display dataset information\n",
    "print(filtered_df.info())\n",
    "\n",
    "print(f\"Reference dataset saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e6dff",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def task_calc_treasury_data():\n",
    "    \"\"\"Run `calc_treasury_data.py` which processes Treasury SF data.\"\"\"\n",
    "    return {\n",
    "        \"actions\": [\"python ./src/calc_treasury_data.py\"],\n",
    "        \"file_dep\": [\n",
    "            DATA_DIR / \"treasury_df.csv\",\n",
    "            DATA_DIR / \"ois_df.csv\",\n",
    "            DATA_DIR / \"last_day_df.csv\"\n",
    "        ],\n",
    "        \"targets\": [DATA_DIR / \"treasury_sf_output.csv\"],\n",
    "        \"clean\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a6a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure 'src' is in sys.path\n",
    "sys.path.append(os.path.abspath(\"./src\"))  # Add 'src' to the path\n",
    "\n",
    "# Now import config from settings.py\n",
    "from settings import config  # <- FIXED\n",
    "\n",
    "# Set data directory\n",
    "DATA_DIR = config(\"DATA_DIR\")\n",
    "MANUAL_DATA_DIR = config(\"MANUAL_DATA_DIR\")\n",
    "OUTPUT_DIR = config(\"OUTPUT_DIR\")\n",
    "\n",
    "def parse_contract_date(contract_str):\n",
    "    \"\"\"\n",
    "    Parse contract string to extract month and year.\n",
    "    E.g., \"DEC 21\" -> (12, 2021)\n",
    "    \"\"\"\n",
    "    if pd.isna(contract_str) or not isinstance(contract_str, str):\n",
    "        return None, None\n",
    "    month_abbr = contract_str[:3].upper()\n",
    "    year_str = contract_str[4:6]\n",
    "    month_map = {'DEC': 12, 'MAR': 3, 'JUN': 6, 'SEP': 9}\n",
    "    month = month_map.get(month_abbr, np.nan)\n",
    "    try:\n",
    "        year = int(year_str) + 2000\n",
    "    except:\n",
    "        year = np.nan\n",
    "    return month, year\n",
    "\n",
    "def interpolate_ois(ttm, ois_1w, ois_1m, ois_3m, ois_6m, ois_1y):\n",
    "    \"\"\"Interpolate the OIS rate based on TTM (in days).\"\"\"\n",
    "    if ttm <= 7:\n",
    "        return ois_1w\n",
    "    elif 7 < ttm <= 30:\n",
    "        return ((30 - ttm) / 23) * ois_1w + ((ttm - 7) / 23) * ois_1m\n",
    "    elif 30 < ttm <= 90:\n",
    "        return ((90 - ttm) / 60) * ois_1m + ((ttm - 30) / 60) * ois_3m\n",
    "    elif 90 < ttm <= 180:\n",
    "        return ((180 - ttm) / 90) * ois_3m + ((ttm - 90) / 90) * ois_6m\n",
    "    else:\n",
    "        return ((360 - ttm) / 180) * ois_6m + ((ttm - 180) / 180) * ois_1y\n",
    "\n",
    "def rolling_outlier_flag(df, group_col, date_col, value_col, window_days=45, threshold=10):\n",
    "    \"\"\"\n",
    "    Flag outliers using a rolling window (45 days) per group.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['bad_price'] = False\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df.sort_values(date_col, inplace=True)\n",
    "    \n",
    "    for name, group in df.groupby(group_col):\n",
    "        for idx, row in group.iterrows():\n",
    "            curr_date = row[date_col]\n",
    "            window_mask = (group[date_col] >= curr_date - timedelta(days=window_days)) & \\\n",
    "                          (group[date_col] <= curr_date + timedelta(days=window_days)) & \\\n",
    "                          (group.index != idx)\n",
    "            window_vals = group.loc[window_mask, value_col]\n",
    "            if len(window_vals) > 0:\n",
    "                median_val = window_vals.median()\n",
    "                abs_dev = abs(row[value_col] - median_val)\n",
    "                mad = window_vals.subtract(median_val).abs().mean()\n",
    "                if mad > 0 and (abs_dev / mad) >= threshold:\n",
    "                    df.at[idx, 'bad_price'] = True\n",
    "    return df\n",
    "\n",
    "def calc_treasury():\n",
    "    # Load intermediate data\n",
    "    data_dir = DATA_DIR\n",
    "\n",
    "    # Load the required CSV files from _data\n",
    "    treasury_file = os.path.join(data_dir, \"treasury_df.csv\")\n",
    "    ois_file = os.path.join(data_dir, \"ois_df.csv\")\n",
    "    last_day_file = os.path.join(data_dir, \"last_day_df.csv\")\n",
    "\n",
    "    # Read the datasets\n",
    "    df = pd.read_csv(treasury_file)\n",
    "    df_ois = pd.read_csv(ois_file)\n",
    "    last_day_df = pd.read_csv(last_day_file)\n",
    "\n",
    "    # Convert date columns to datetime format\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df_ois[\"Date\"] = pd.to_datetime(df_ois[\"Date\"])\n",
    "    last_day_df[\"Date\"] = pd.to_datetime(last_day_df[\"Date\"])\n",
    "\n",
    "    # -------------------------\n",
    "    # Reshape from wide to long format\n",
    "    stubnames = [\"Contract_1\", \"Contract_2\", \"Implied_Repo_1\", \"Implied_Repo_2\",\n",
    "                 \"Vol_1\", \"Vol_2\", \"Price_1\", \"Price_2\"]\n",
    "    df_long = pd.wide_to_long(df, stubnames=stubnames, i=\"Date\", j=\"Tenor\", sep=\"_\", suffix=r'\\d+').reset_index()\n",
    "\n",
    "    # Filter dates > June 22, 2004\n",
    "    cutoff_date = datetime(2004, 6, 22)\n",
    "    df_long = df_long[df_long[\"Date\"] > cutoff_date].copy()\n",
    "    \n",
    "    # -------------------------\n",
    "    # Compute time-to-maturity for contracts v=1 and v=2\n",
    "    for v in [1, 2]:\n",
    "        contract_col = f\"Contract_{v}\"\n",
    "        ttm_col = f\"TTM_{v}\"\n",
    "        mat_date_col = f\"Mat_Date_{v}\"\n",
    "        \n",
    "        # Parse contract string to get month and year\n",
    "        df_long[[f\"Mat_Month_{v}\", f\"Mat_Year_{v}\"]] = df_long[contract_col].apply(\n",
    "            lambda s: pd.Series(parse_contract_date(s))\n",
    "        )\n",
    "        \n",
    "        # Merge with last_day_df to get the day-of-month\n",
    "        df_long = df_long.merge(last_day_df, left_on=[f\"Mat_Month_{v}\", f\"Mat_Year_{v}\"], \n",
    "                                right_on=[\"Mat_Month\", \"Mat_Year\"], how=\"left\", suffixes=(\"\", f\"_{v}\"))\n",
    "        # For specific contracts without a business day, set Mat_Day = 31\n",
    "        cond_special = df_long[contract_col].isin([\"DEC 21\", \"MAR 22\"])\n",
    "        df_long.loc[cond_special, \"Mat_Day\"] = 31\n",
    "        \n",
    "        def make_mat_date(row):\n",
    "            try:\n",
    "                return datetime(int(row[f\"Mat_Year_{v}\"]), int(row[f\"Mat_Month_{v}\"]), int(row[\"Mat_Day\"]))\n",
    "            except Exception:\n",
    "                return pd.NaT\n",
    "        df_long[mat_date_col] = df_long.apply(make_mat_date, axis=1)\n",
    "        df_long[ttm_col] = (df_long[mat_date_col] - df_long[\"Date\"]).dt.days\n",
    "        \n",
    "        # Clean up temporary columns\n",
    "        df_long.drop(columns=[f\"Mat_Month_{v}\", f\"Mat_Year_{v}\", \"Mat_Month\", \"Mat_Year\", \"Mat_Day\"], \n",
    "                      inplace=True, errors='ignore')\n",
    "    \n",
    "    # -------------------------\n",
    "    \n",
    "    # Merge with USD OIS Rates on Date\n",
    "    df_ois['Date'] = pd.to_datetime(df_ois['Date'])\n",
    "    df_long = df_long.merge(df_ois, left_on=\"Date\", right_on=\"Date\", how=\"left\")\n",
    "    # df_long.drop(columns=[\"Date\"], inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "    # -------------------------\n",
    "    # Interpolate OIS rates for contracts v=1 and v=2\n",
    "    for v in [1, 2]:\n",
    "        ttm_col = f\"TTM_{v}\"\n",
    "        ois_col = f\"OIS_{v}\"\n",
    "        df_long[ois_col] = df_long.apply(lambda row: \n",
    "                                         interpolate_ois(row[ttm_col],\n",
    "                                                         row.get(\"OIS_1W\", np.nan),\n",
    "                                                         row.get(\"OIS_1M\", np.nan),\n",
    "                                                         row.get(\"OIS_3M\", np.nan),\n",
    "                                                         row.get(\"OIS_6M\", np.nan),\n",
    "                                                         row.get(\"OIS_1Y\", np.nan)\n",
    "                                                        ) if pd.notnull(row[ttm_col]) else np.nan, axis=1)\n",
    "    \n",
    "    # -------------------------\n",
    "    # Compute Treasury arbitrage spreads\n",
    "    df_long[\"Arb_N\"] = (df_long[\"Implied_Repo_1\"] - df_long[\"OIS_1\"]) * 100\n",
    "    df_long[\"Arb_D\"] = (df_long[\"Implied_Repo_2\"] - df_long[\"OIS_2\"]) * 100\n",
    "    df_long[\"arb\"] = df_long[\"Arb_D\"]   # Use deferred contract\n",
    "    \n",
    "    # Outlier cleanup: flag observations based on a 45-day rolling window.\n",
    "    df_long = rolling_outlier_flag(df_long, group_col=\"Tenor\", date_col=\"Date\", value_col=\"arb\",\n",
    "                                   window_days=45, threshold=10)\n",
    "    df_long.loc[df_long[\"bad_price\"] & df_long[\"arb\"].notnull(), \"arb\"] = np.nan\n",
    "    \n",
    "    # Drop rows without trading volume in deferred contract (Vol_2)\n",
    "    df_long = df_long[df_long[\"Vol_2\"].notnull()].copy()\n",
    "    \n",
    "    # -------------------------\n",
    "    # Plot arbitrage spread for selected tenors.\n",
    "    output_dir = OUTPUT_DIR \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for tenor in [2, 5, 10, 20, 30]:\n",
    "        df_plot = df_long[df_long[\"Tenor\"] == str(tenor)]\n",
    "        if df_plot.empty:\n",
    "            continue\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(df_plot[\"Date\"], df_plot[\"arb\"], label=f\"Tenor = {tenor} years\")\n",
    "        plt.ylabel(\"Arbitrage Spread (bps)\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.title(f\"Tenor = {tenor} years\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(output_dir, f\"arbitrage_spread_{tenor}.pdf\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved plot: {plot_path}\")\n",
    "    \n",
    "    # -------------------------\n",
    "    # Prepare final output\n",
    "    df_long[\"T_SF_Rf\"] = df_long[\"Implied_Repo_2\"] * 100\n",
    "    df_long.loc[df_long[\"bad_price\"] & df_long[\"T_SF_Rf\"].notnull(), \"T_SF_Rf\"] = np.nan\n",
    "    df_long[\"rf_ois_t_sf_mat\"] = df_long[\"OIS_2\"] * 100\n",
    "    df_long[\"T_SF_TTM\"] = df_long[\"TTM_2\"]\n",
    "    df_out = df_long[[\"Date\", \"Tenor\", \"T_SF_Rf\", \"rf_ois_t_sf_mat\", \"T_SF_TTM\"]].copy()\n",
    "    \n",
    "    # Reshape output to wide format (one row per date)\n",
    "    df_wide = df_out.pivot(index=\"Date\", columns=\"Tenor\")\n",
    "    df_wide.columns = ['_'.join([str(c) for c in col]).strip() for col in df_wide.columns.values]\n",
    "    df_wide.reset_index(inplace=True)\n",
    "    \n",
    "    # Rename columns to match output convention\n",
    "    rename_dict = {\n",
    "        \"T_SF_Rf_2\": \"tfut_2_rf\",\n",
    "        \"T_SF_Rf_5\": \"tfut_5_rf\",\n",
    "        \"T_SF_Rf_10\": \"tfut_10_rf\",\n",
    "        \"T_SF_Rf_20\": \"tfut_20_rf\",\n",
    "        \"T_SF_Rf_30\": \"tfut_30_rf\",\n",
    "        \"T_SF_TTM_2\": \"tfut_2_ttm\",\n",
    "        \"T_SF_TTM_5\": \"tfut_5_ttm\",\n",
    "        \"T_SF_TTM_10\": \"tfut_10_ttm\",\n",
    "        \"T_SF_TTM_20\": \"tfut_20_ttm\",\n",
    "        \"T_SF_TTM_30\": \"tfut_30_ttm\",\n",
    "        \"rf_ois_t_sf_mat_2\": \"tfut_2_ois\",\n",
    "        \"rf_ois_t_sf_mat_5\": \"tfut_5_ois\",\n",
    "        \"rf_ois_t_sf_mat_10\": \"tfut_10_ois\",\n",
    "        \"rf_ois_t_sf_mat_20\": \"tfut_20_ois\",\n",
    "        \"rf_ois_t_sf_mat_30\": \"tfut_30_ois\"\n",
    "    }\n",
    "    df_wide.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "    df_wide[\"Treasury_SF_2Y\"] = df_wide[\"tfut_2_rf\"] - df_wide[\"tfut_2_ois\"]\n",
    "    df_wide[\"Treasury_SF_5Y\"] = df_wide[\"tfut_5_rf\"] - df_wide[\"tfut_5_ois\"]\n",
    "    df_wide[\"Treasury_SF_10Y\"] = df_wide[\"tfut_10_rf\"] - df_wide[\"tfut_10_ois\"]\n",
    "    df_wide[\"Treasury_SF_20Y\"] = df_wide[\"tfut_20_rf\"] - df_wide[\"tfut_20_ois\"]\n",
    "    df_wide[\"Treasury_SF_30Y\"] = df_wide[\"tfut_30_rf\"] - df_wide[\"tfut_30_ois\"]\n",
    "    # Select relevant columns\n",
    "    df_out = df_wide[[\"Date\", \"Treasury_SF_2Y\", \"Treasury_SF_5Y\", \"Treasury_SF_10Y\", \"Treasury_SF_20Y\", \"Treasury_SF_30Y\"]].copy()\n",
    "    df_out.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    \n",
    "    # Save final output as Stata .dta file (or CSV if preferred)\n",
    "    output_file = os.path.join(DATA_DIR, \"treasury_sf_output.csv\")\n",
    "    df_out.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Final output saved to {output_file}\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    calc_treasury()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b11c83",
   "metadata": {},
   "source": [
    "# Generate Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f78029",
   "metadata": {},
   "source": [
    "## TIPS Treasury Arbitrage:\n",
    "```python\n",
    "def task_generate_figures():\n",
    "    \"\"\" \"\"\"\n",
    "    file_dep = [\n",
    "        \"./src/generate_figures.py\",\n",
    "        \"./src/generate_latex_table.py\",\n",
    "    ]\n",
    "    file_output = [\n",
    "        \"tips_treasury_spreads.png\",\n",
    "        \"tips_treasury_summary_stats.csv\",\n",
    "        'tips_treasury_summary_table.tex'\n",
    "    ]\n",
    "    targets = [OUTPUT_DIR / file for file in file_output]\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            \"ipython ./src/generate_figures.py\",\n",
    "            \"ipython ./src/generate_latex_table.py\",\n",
    "        ],\n",
    "        \"targets\": targets,\n",
    "        \"file_dep\": file_dep,\n",
    "        \"clean\": [],\n",
    "    }\n",
    "```\n",
    "\n",
    "\n",
    "## CIP \n",
    "``` python\n",
    "\n",
    "def task_rename_plots():\n",
    "    \"\"\"Run CIP analysis and rename output plots correctly.\"\"\"\n",
    "\n",
    "    import re\n",
    "\n",
    "    def rename_output_files():\n",
    "        \"\"\"Rename automatically generated plots to expected filenames based on a pattern match.\"\"\"\n",
    "        output_dir = OUTPUT_DIR / \"main_cip_files\"\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        file_patterns = {\n",
    "            r\"main_cip_\\d+_0.png\": \"cip_spread_plot_replication.png\",\n",
    "            r\"main_cip_\\d+_1.png\": \"cip_spread_plot_2025.png\",\n",
    "        }\n",
    "\n",
    "        matched_files = set()  # Track matched files\n",
    "\n",
    "        for old_file in output_dir.glob(\"main_cip_*.png\"):\n",
    "            for pattern, new_name in file_patterns.items():\n",
    "                if re.match(pattern, old_file.name):\n",
    "                    new_path = output_dir / new_name\n",
    "\n",
    "                    # Check if the file already exists and avoid overwriting\n",
    "                    if new_path.exists():\n",
    "                        new_path = output_dir / \"cip_spread_2025.png\"\n",
    "\n",
    "                    old_file.rename(new_path)\n",
    "                    print(f\"Renamed {old_file}  {new_path}\")\n",
    "                    matched_files.add(old_file.name)\n",
    "                    # No break here so all files get renamed\n",
    "\n",
    "        # Check for missing files\n",
    "        for old_file in output_dir.glob(\"main_cip_*.png\"):\n",
    "            if old_file.name not in matched_files:\n",
    "                print(f\"Warning: No match found for {old_file}\")\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            \"cd src && ipython cip_analysis.py\",\n",
    "            rename_output_files  # Rename after execution\n",
    "        ],\n",
    "        \"file_dep\": [\n",
    "            \"./src/cip_analysis.py\",\n",
    "            \"./src/pull_bloomberg_cip_data.py\",\n",
    "            str(MANUAL_DATA_DIR / \"CIP_2025.xlsx\"),\n",
    "        ],\n",
    "        \"targets\": [\n",
    "            str(OUTPUT_DIR / \"main_cip_files\" / \"cip_spread_plot_replication.png\"),\n",
    "            str(OUTPUT_DIR / \"main_cip_files\" / \"cip_spread_plot_2025.png\"),\n",
    "        ],\n",
    "        \"task_dep\": [\"download_cip_data\"],\n",
    "        \"clean\": True,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8589681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1295aaa3",
   "metadata": {},
   "source": [
    "# Convert Notebooks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d16d5b",
   "metadata": {},
   "source": [
    "## TIPS Treasury Arbitrage:\n",
    "\n",
    "```python\n",
    "notebook_tasks = {\n",
    "    \"arb_replication.ipynb\": {\n",
    "        \"file_dep\": [\n",
    "            \"./src/generate_figures.py\"\n",
    "        ],\n",
    "        \"targets\": [],\n",
    "    }\n",
    "}\n",
    "\n",
    "def task_convert_notebooks_to_scripts():\n",
    "    \"\"\"Convert notebooks to script form to detect changes to source code rather\n",
    "    than to the notebook's metadata.\n",
    "    \"\"\"\n",
    "    build_dir = Path(OUTPUT_DIR)\n",
    "\n",
    "    for notebook in notebook_tasks.keys():\n",
    "        notebook_name = notebook.split(\".\")[0]\n",
    "        yield {\n",
    "            \"name\": notebook,\n",
    "            \"actions\": [\n",
    "                jupyter_clear_output(notebook_name),\n",
    "                jupyter_to_python(notebook_name, build_dir),\n",
    "            ],\n",
    "            \"file_dep\": [Path(\"./src\") / notebook],\n",
    "            \"targets\": [OUTPUT_DIR / f\"_{notebook_name}.py\"],\n",
    "            \"clean\": True,\n",
    "            \"verbosity\": 0,\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b5d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "305e4afe",
   "metadata": {},
   "source": [
    "## Equity Spot Futures:\n",
    "```python\n",
    "notebook_tasks = {\n",
    "    \"01_OIS_Data_Processing.ipynb\": {\n",
    "        \"file_dep\": [\"./src/settings.py\",\"./src/pull_bloomberg_data.py\", \"./src/OIS_data_processing.py\"],\n",
    "        \"targets\": [OUTPUT_DIR / 'ois_3m_rolling_statistics.png',\n",
    "                    OUTPUT_DIR / 'ois_3m_rate_time_series.png',\n",
    "                    OUTPUT_DIR / \"ois_summary_statistics.tex\"],\n",
    "    },\n",
    "    \"02_Futures_Data_Processing.ipynb\": {\n",
    "        \"file_dep\": [\"./src/settings.py\",\"./src/pull_bloomberg_data.py\", \"./src/futures_data_processing.py\"],\n",
    "        \"targets\": [OUTPUT_DIR / \"es1_contract_roll_pattern.png\",\n",
    "                    OUTPUT_DIR / \"es1_ttm_distribution.png\",\n",
    "                    OUTPUT_DIR / \"futures_prices_by_index.png\",],\n",
    "    },\n",
    "    \"03_Spread_Calculations.ipynb\": {\n",
    "        \"file_dep\": [\n",
    "            \"./src/settings.py\",\n",
    "            \"./src/pull_bloomberg_data.py\", \n",
    "            \"./src/futures_data_processing.py\",\n",
    "            \"./src/OIS_data_processing.py\",\n",
    "            \"./src/Spread_calculations.py\"\n",
    "        ],\n",
    "        \"targets\": [],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def task_convert_notebooks_to_scripts():\n",
    "    \"\"\"Convert notebooks to script form to detect changes to source code rather\n",
    "    than to the notebook's metadata.\n",
    "    \"\"\"\n",
    "    build_dir = Path(OUTPUT_DIR)\n",
    "\n",
    "    for notebook in notebook_tasks.keys():\n",
    "        notebook_name = notebook.split(\".\")[0]\n",
    "        yield {\n",
    "            \"name\": notebook,\n",
    "            \"actions\": [\n",
    "                jupyter_clear_output(notebook_name),\n",
    "                jupyter_to_python(notebook_name, build_dir),\n",
    "            ],\n",
    "            \"file_dep\": [Path(\"./src\") / notebook],\n",
    "            \"targets\": [OUTPUT_DIR / f\"_{notebook_name}.py\"],\n",
    "            \"clean\": True,\n",
    "            \"verbosity\": 0,\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f205f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a9eb378",
   "metadata": {},
   "source": [
    "## Market Expectations:\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "notebook_tasks = {\n",
    "    \"01_Market_Expectations_In_The_Cross-Section_Of_Present_Values_Final.ipynb\": {\n",
    "        \"file_dep\": [],\n",
    "        \"targets\": [],\n",
    "    },\n",
    "    \"run_regressions.ipynb\": {\n",
    "        \"file_dep\": [],\n",
    "        \"targets\": [],\n",
    "    },\n",
    "}\n",
    "\n",
    "def task_convert_notebooks_to_scripts():\n",
    "    \"\"\"Convert notebooks to script form to detect changes to source code rather\n",
    "    than to the notebook's metadata.\n",
    "    \"\"\"\n",
    "    build_dir = Path(OUTPUT_DIR)\n",
    "\n",
    "    for notebook in notebook_tasks.keys():\n",
    "        notebook_name = notebook.split(\".\")[0]\n",
    "        yield {\n",
    "            \"name\": notebook,\n",
    "            \"actions\": [\n",
    "                jupyter_clear_output(notebook_name),\n",
    "                jupyter_to_python(notebook_name, build_dir),\n",
    "            ],\n",
    "            \"file_dep\": [Path(\"./src\") / notebook],\n",
    "            \"targets\": [OUTPUT_DIR / f\"_{notebook_name}.py\"],\n",
    "            \"clean\": True,\n",
    "            \"verbosity\": 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6829042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70de5c5c",
   "metadata": {},
   "source": [
    "# Run Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d80ff96",
   "metadata": {},
   "source": [
    "\n",
    "## TIPS Treasury Arbitrage:\n",
    "\n",
    "```python\n",
    "# fmt: off\n",
    "def task_run_notebooks():\n",
    "    \"\"\"Preps the notebooks for presentation format.\n",
    "    Execute notebooks if the script version of it has been changed.\n",
    "    \"\"\"\n",
    "    for notebook in notebook_tasks.keys():\n",
    "        notebook_name = notebook.split(\".\")[0]\n",
    "        yield {\n",
    "            \"name\": notebook,\n",
    "            \"actions\": [\n",
    "                \"\"\"python -c \"import sys; from datetime import datetime; print(f'Start \"\"\" + notebook + \"\"\": {datetime.now()}', file=sys.stderr)\" \"\"\",\n",
    "                jupyter_execute_notebook(notebook_name),\n",
    "                jupyter_to_html(notebook_name),\n",
    "                copy_file(\n",
    "                    Path(\"./src\") / f\"{notebook_name}.ipynb\",\n",
    "                    OUTPUT_DIR / f\"{notebook_name}.ipynb\",\n",
    "                    mkdir=True,\n",
    "                ),\n",
    "                jupyter_clear_output(notebook_name),\n",
    "                # jupyter_to_python(notebook_name, build_dir),\n",
    "                \"\"\"python -c \"import sys; from datetime import datetime; print(f'End \"\"\" + notebook + \"\"\": {datetime.now()}', file=sys.stderr)\" \"\"\",\n",
    "            ],\n",
    "            \"file_dep\": [\n",
    "                OUTPUT_DIR / f\"_{notebook_name}.py\",\n",
    "                *notebook_tasks[notebook][\"file_dep\"],\n",
    "            ],\n",
    "            \"targets\": [\n",
    "                OUTPUT_DIR / f\"{notebook_name}.html\",\n",
    "                OUTPUT_DIR / f\"{notebook_name}.ipynb\",\n",
    "                *notebook_tasks[notebook][\"targets\"],\n",
    "            ],\n",
    "            \"clean\": True,\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395a161",
   "metadata": {},
   "source": [
    "## Equity Spot Futures:\n",
    "```python\n",
    "# fmt: off\n",
    "def task_run_notebooks():\n",
    "    \"\"\"Preps the notebooks for presentation format.\n",
    "    Execute notebooks if the script version of it has been changed.\n",
    "    \"\"\"\n",
    "    for notebook in notebook_tasks.keys():\n",
    "        notebook_name = notebook.split(\".\")[0]\n",
    "        yield {\n",
    "            \"name\": notebook,\n",
    "            \"actions\": [\n",
    "                \"\"\"python -c \"import sys; from datetime import datetime; print(f'Start \"\"\" + notebook + \"\"\": {datetime.now()}', file=sys.stderr)\" \"\"\",\n",
    "                jupyter_execute_notebook(notebook_name),\n",
    "                jupyter_to_html(notebook_name),\n",
    "                copy_file(\n",
    "                    Path(\"./src\") / f\"{notebook_name}.ipynb\",\n",
    "                    OUTPUT_DIR / f\"{notebook_name}.ipynb\",\n",
    "                    mkdir=True,\n",
    "                ),\n",
    "                copy_file(\n",
    "                    Path(\"./src\") / f\"{notebook_name}.ipynb\",\n",
    "                    Path(\"./_docs/notebooks/\") / f\"{notebook_name}.ipynb\",\n",
    "                    mkdir=True,\n",
    "                ),\n",
    "                jupyter_clear_output(notebook_name),\n",
    "                # jupyter_to_python(notebook_name, build_dir),\n",
    "                \"\"\"python -c \"import sys; from datetime import datetime; print(f'End \"\"\" + notebook + \"\"\": {datetime.now()}', file=sys.stderr)\" \"\"\",\n",
    "            ],\n",
    "            \"file_dep\": [\n",
    "                OUTPUT_DIR / f\"_{notebook_name}.py\",\n",
    "                *notebook_tasks[notebook][\"file_dep\"],\n",
    "            ],\n",
    "            \"targets\": [\n",
    "                OUTPUT_DIR / f\"{notebook_name}.html\",\n",
    "                OUTPUT_DIR / f\"{notebook_name}.ipynb\",\n",
    "                *notebook_tasks[notebook][\"targets\"],\n",
    "            ],\n",
    "            \"clean\": True,\n",
    "        }\n",
    "```\n",
    "\n",
    "## CIP \n",
    "```python\n",
    "        \n",
    "def task_run_notebooks():\n",
    "    \"\"\"Execute Jupyter notebooks and convert them to HTML & LaTeX.\"\"\"\n",
    "    notebook = \"main_cip\"\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            jupyter_execute_notebook(notebook),\n",
    "            jupyter_to_html(notebook),\n",
    "            jupyter_to_latex(notebook),\n",
    "            # convert_html_to_png  # Move PNGs and process files\n",
    "        ],\n",
    "        \"file_dep\": [\"./src/main_cip.ipynb\", str(MANUAL_DATA_DIR / \"CIP_2025.xlsx\")],\n",
    "        \"targets\": [\n",
    "            str(OUTPUT_DIR /\"html_files\"/ \"main_cip.html\"),\n",
    "            str(OUTPUT_DIR / \"main_cip.tex\"),\n",
    "        ],\n",
    "        \"task_dep\": [\"download_cip_data\"],\n",
    "        \"clean\": True,\n",
    "    }\n",
    "```\n",
    "\n",
    "## Market Expectations\n",
    "```python\n",
    "\n",
    "# fmt: off\n",
    "def task_run_notebooks():\n",
    "    \"\"\"Preps the notebooks for presentation format.\n",
    "    Execute notebooks if the script version of it has been changed.\n",
    "    \"\"\"\n",
    "    for notebook in notebook_tasks.keys():\n",
    "        notebook_name = notebook.split(\".\")[0]\n",
    "        yield {\n",
    "            \"name\": notebook,\n",
    "            \"actions\": [\n",
    "                \"\"\"python -c \"import sys; from datetime import datetime; print(f'Start \"\"\" + notebook + \"\"\": {datetime.now()}', file=sys.stderr)\" \"\"\",\n",
    "                jupyter_execute_notebook(notebook_name),\n",
    "                jupyter_to_html(notebook_name),\n",
    "                copy_file(\n",
    "                    Path(\"./src\") / f\"{notebook_name}.ipynb\",\n",
    "                    OUTPUT_DIR / f\"{notebook_name}.ipynb\",\n",
    "                    mkdir=True,\n",
    "                ),\n",
    "                jupyter_clear_output(notebook_name),\n",
    "                # jupyter_to_python(notebook_name, build_dir),\n",
    "                \"\"\"python -c \"import sys; from datetime import datetime; print(f'End \"\"\" + notebook + \"\"\": {datetime.now()}', file=sys.stderr)\" \"\"\",\n",
    "            ],\n",
    "            \"file_dep\": [\n",
    "                OUTPUT_DIR / f\"_{notebook_name}.py\",\n",
    "                *notebook_tasks[notebook][\"file_dep\"],\n",
    "            ],\n",
    "            \"targets\": [\n",
    "                OUTPUT_DIR / f\"{notebook_name}.html\",\n",
    "                OUTPUT_DIR / f\"{notebook_name}.ipynb\",\n",
    "                *notebook_tasks[notebook][\"targets\"],\n",
    "            ],\n",
    "            \"clean\": True,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6719ec",
   "metadata": {},
   "source": [
    "## Treasury Spot\n",
    "```python\n",
    "\n",
    "\n",
    "def task_process_treasury_data_notebook():\n",
    "    \"\"\"Optionally run `process_treasury_data.ipynb` if needed.\"\"\"\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            jupyter_execute_notebook(\"process_treasury_data\")\n",
    "        ],\n",
    "        \"file_dep\": [DATA_DIR / \"treasury_sf_output.csv\"],\n",
    "        \"clean\": True,\n",
    "    }\n",
    "\n",
    "def task_plot_exploration():\n",
    "    \"\"\"Run `01_explore_basis_trade_data_new.ipynb` for visualization.\"\"\"\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            jupyter_execute_notebook(\"01_explore_basis_trade_data_new\")\n",
    "        ],\n",
    "        \"file_dep\": [DATA_DIR / \"treasury_sf_output.csv\"],\n",
    "        \"clean\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61716189",
   "metadata": {},
   "source": [
    "# LaTeX compiliation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f556d",
   "metadata": {},
   "source": [
    "\n",
    "## TIPS Treasury Arbitrage:\n",
    "\n",
    "```python\n",
    "def task_compile_latex_docs():\n",
    "    \"\"\"Compile the LaTeX documents to PDFs\"\"\"\n",
    "    file_dep = [\n",
    "        \"./reports/report.tex\",\n",
    "        \"./reports/my_article_header.sty\",      # style \n",
    "        #\"./reports/slides_example.tex\",\n",
    "        #`\"./reports/my_beamer_header.sty\",       # style\n",
    "        \"./reports/my_common_header.sty\",       # style\n",
    "        # \"./reports/report_simple_example.tex\",\n",
    "        # \"./reports/slides_simple_example.tex\",\n",
    "        \"./src/generate_figures.py\",\n",
    "        \"./src/generate_latex_table.py\",\n",
    "    ]\n",
    "    targets = [\n",
    "        \"./reports/report.pdf\",\n",
    "        #\"./reports/slides_example.pdf\",\n",
    "        # \"./reports/report_simple_example.pdf\",\n",
    "        # \"./reports/slides_simple_example.pdf\",\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            # My custom LaTeX templates\n",
    "            \"latexmk -xelatex -halt-on-error -cd ./reports/report.tex\",  # Compile\n",
    "            \"latexmk -xelatex -halt-on-error -c -cd ./reports/report.tex\",  # Clean\n",
    "      ],\n",
    "        \"targets\": targets,\n",
    "        \"file_dep\": file_dep,\n",
    "        \"clean\": True,\n",
    "    }\n",
    "```\n",
    "\n",
    "\n",
    "## Equity Spot Futures\n",
    "```python\n",
    "def task_compile_latex_docs():\n",
    "    \"\"\"Compile the LaTeX documents to PDFs\"\"\"\n",
    "    file_dep = [\n",
    "        \"./reports/report.tex\"\n",
    "    ]\n",
    "    targets = [\n",
    "        \"./reports/report.pdf\"\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            \"latexmk -xelatex -halt-on-error -cd ./reports/report.tex\",  # Compile\n",
    "            \"latexmk -xelatex -halt-on-error -c -cd ./reports/report.tex\"  # Clean\n",
    "        ],\n",
    "        \"targets\": targets,\n",
    "        \"file_dep\": file_dep,\n",
    "        \"clean\": True,\n",
    "    }\n",
    "```\n",
    "## Market Expectations\n",
    "```python\n",
    "def task_compile_latex_docs():\n",
    "    \"\"\"Compile the LaTeX documents to PDFs\"\"\"\n",
    "    file_dep = [\n",
    "        \"./reports/project.tex\",\n",
    "    ]\n",
    "    targets = [\n",
    "        \"./reports/project.pdf\",\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            \"latexmk -xelatex -halt-on-error -cd ./reports/project.tex\",  # Compile\n",
    "            \"latexmk -xelatex -halt-on-error -c -cd ./reports/project.tex\",  # Clean\n",
    "        ],\n",
    "        \"targets\": targets,\n",
    "        \"file_dep\": file_dep,\n",
    "        \"clean\": True,\n",
    "    }\n",
    "```\n",
    "## Treasury spot\n",
    "```python\n",
    "def task_latex_to_document():\n",
    "    return {\n",
    "        'actions': ['python src/latex_to_document.py reports/Final_Report.tex'],\n",
    "        'file_dep': ['src/latex_to_document.py', 'reports/Final_Report.tex'],\n",
    "        'targets': ['reports/Final_Report.pdf'],\n",
    "        'clean': True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bdad8a",
   "metadata": {},
   "source": [
    "# Sphinx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1774b6",
   "metadata": {},
   "source": [
    "\n",
    "## Equity Spot Futures\n",
    "\n",
    "```python\n",
    "\n",
    "notebook_sphinx_pages = [\n",
    "    \"./_docs/_build/html/notebooks/\" + notebook.split(\".\")[0] + \".html\"\n",
    "    for notebook in notebook_tasks.keys()\n",
    "]\n",
    "sphinx_targets = [\n",
    "    \"./_docs/_build/html/index.html\",\n",
    "    *notebook_sphinx_pages\n",
    "]\n",
    "\n",
    "\n",
    "def copy_docs_src_to_docs():\n",
    "    \"\"\"\n",
    "    Copy all files and subdirectories from the docs_src directory to the _docs directory.\n",
    "    This function loops through all files in docs_src and copies them individually to _docs,\n",
    "    preserving the directory structure. It does not delete the contents of _docs beforehand.\n",
    "    \"\"\"\n",
    "    src = Path(\"docs_src\")\n",
    "    dst = Path(\"_docs\")\n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    dst.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Loop through all files and directories in docs_src\n",
    "    for item in src.rglob(\"*\"):\n",
    "        relative_path = item.relative_to(src)\n",
    "        target = dst / relative_path\n",
    "        if item.is_dir():\n",
    "            target.mkdir(parents=True, exist_ok=True)\n",
    "        else:\n",
    "            shutil.copy2(item, target)\n",
    "\n",
    "\n",
    "def copy_docs_build_to_docs():\n",
    "    \"\"\"\n",
    "    Copy all files and subdirectories from _docs/_build/html to docs.\n",
    "    This function copies each file individually while preserving the directory structure.\n",
    "    It does not delete any existing contents in docs.\n",
    "    After copying, it creates an empty .nojekyll file in the docs directory.\n",
    "    \"\"\"\n",
    "    src = Path(\"_docs/_build/html\")\n",
    "    dst = Path(\"docs\")\n",
    "    dst.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Loop through all files and directories in src\n",
    "    for item in src.rglob(\"*\"):\n",
    "        relative_path = item.relative_to(src)\n",
    "        target = dst / relative_path\n",
    "        if item.is_dir():\n",
    "            target.mkdir(parents=True, exist_ok=True)\n",
    "        else:\n",
    "            target.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(item, target)\n",
    "\n",
    "    # Touch an empty .nojekyll file in the docs directory.\n",
    "    (dst / \".nojekyll\").touch()\n",
    "\n",
    "\n",
    "def task_compile_sphinx_docs():\n",
    "    \"\"\"Compile Sphinx Docs\"\"\"\n",
    "    notebook_scripts = [\n",
    "        OUTPUT_DIR / (\"_\" + notebook.split(\".\")[0] + \".py\")\n",
    "        for notebook in notebook_tasks.keys()\n",
    "    ]\n",
    "    file_dep = [\n",
    "        \"./docs_src/conf.py\",\n",
    "        \"./docs_src/index.md\",\n",
    "        *notebook_scripts,\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            copy_docs_src_to_docs,\n",
    "            \"sphinx-build -M html ./_docs/ ./_docs/_build\",\n",
    "            copy_docs_build_to_docs,\n",
    "        ],\n",
    "        \"targets\": sphinx_targets,\n",
    "        \"file_dep\": file_dep,\n",
    "        \"task_dep\": [\"run_notebooks\"],\n",
    "        \"clean\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "## CIP \n",
    "\n",
    "```python\n",
    "\n",
    "def task_generate_paper():\n",
    "    \"\"\"Generate a LaTeX paper from the copied Jupyter Notebook.\"\"\"\n",
    "    paper_notebook = PUBLISH_DIR / \"paper.ipynb\"\n",
    "    paper_tex = PUBLISH_DIR / \"paper.tex\"\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"actions\": [\n",
    "            copy_notebook,  # Copy first\n",
    "            f\"jupyter nbconvert --execute --to notebook --inplace --ExecutePreprocessor.allow_errors=True \\\"{paper_notebook}\\\"\",\n",
    "            f\"jupyter nbconvert --to latex --output-dir=\\\"{PUBLISH_DIR}\\\" \\\"{paper_notebook}\\\"\",\n",
    "            f\"pdflatex -output-directory=\\\"{PUBLISH_DIR}\\\" \\\"{paper_tex}\\\"\",\n",
    "            # f\"bibtex \\\"{paper_tex.with_suffix('')}\\\"\",  # Keep commented if no bibliography\n",
    "            f\"pdflatex -output-directory=\\\"{PUBLISH_DIR}\\\" \\\"{paper_tex}\\\"\",\n",
    "            f\"pdflatex -output-directory=\\\"{PUBLISH_DIR}\\\" \\\"{paper_tex}\\\"\"\n",
    "        ],\n",
    "        \"file_dep\": [],\n",
    "        \"targets\": [str(paper_tex)],\n",
    "        \"task_dep\": [],\n",
    "        \"clean\": True,\n",
    "    }\n",
    "\n",
    "def task_clean_reports():\n",
    "    \"\"\"Remove unnecessary output files.\"\"\"\n",
    "    files_to_remove = [\n",
    "        PUBLISH_DIR / \"paper.aux\",\n",
    "        PUBLISH_DIR / \"paper.log\",\n",
    "        PUBLISH_DIR / \"paper.out\",\n",
    "        PUBLISH_DIR / \"spread_plot_rep.pdf\"\n",
    "    ]\n",
    "\n",
    "    def remove_files():\n",
    "        for file in files_to_remove:\n",
    "            if file.is_file():\n",
    "                file.unlink()\n",
    "                print(f\"Removed {file}\")\n",
    "            else:\n",
    "                print(f\"File not found: {file}\")\n",
    "\n",
    "    return {\n",
    "        \"actions\": [remove_files],\n",
    "        \"clean\": True,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd292a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
