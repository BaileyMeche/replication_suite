{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d99e5e94",
   "metadata": {},
   "source": [
    "# The Notebook that will be converted to task runner Doit\n",
    "\n",
    "\n",
    "1. Map over your “scrape-slices” → invoke Lambda or Batch.\n",
    "2. Wait (Check S3) → ensure all raw shards land.\n",
    "3. EMR Step → run your Spark/Dask ETL.\n",
    "4. JDBC Load → write processed tables into Aurora.\n",
    "5. Visualize via Dask Cloud Resources\n",
    "6. Push to Flask/frontend resource\n",
    "7. Notify via SNS or email."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd11034",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99538452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the 'src' directory to sys.path\n",
    "sys.path.append(str(Path.cwd() / \"src\"))\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "from settings import config\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "REGION       = config(\"REGION\")\n",
    "BUCKET       = config(\"BUCKET\")\n",
    "LOCAL_MANUAL_DATA_DIR    = config(\"LOCAL_MANUAL_DATA_DIR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "024520e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finalproject-macs30123-baileymeche'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7d351e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S3] Creating bucket 'finalproject-macs30123-baileymeche'...\n",
      "[S3] Bucket 'finalproject-macs30123-baileymeche' created.\n",
      "[S3] Uploading C:/Users/baile/Box Sync/sp25/MACS 30123/final-project-baileymeche/data_manual\\data_README.md → s3://finalproject-macs30123-baileymeche/MANUAL_DATA/data_README.md\n",
      "[S3] Uploading C:/Users/baile/Box Sync/sp25/MACS 30123/final-project-baileymeche/data_manual\\treasury_inflation_swaps.csv → s3://finalproject-macs30123-baileymeche/MANUAL_DATA/treasury_inflation_swaps.csv\n",
      "[S3] All files from 'C:/Users/baile/Box Sync/sp25/MACS 30123/final-project-baileymeche/data_manual' uploaded under 'MANUAL_DATA/'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ensure_s3_bucket(bucket_name: str, region: str = REGION):\n",
    "    \"\"\"Create the S3 bucket if it doesn't exist, otherwise do nothing.\"\"\"\n",
    "    s3 = boto3.client(\"s3\", region_name=region)\n",
    "    try:\n",
    "        s3.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"[S3] Bucket '{bucket_name}' already exists.\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        code = int(e.response[\"Error\"][\"Code\"])\n",
    "        if code == 404:\n",
    "            print(f\"[S3] Creating bucket '{bucket_name}'...\")\n",
    "            kwargs = {}\n",
    "            if region != \"us-east-1\":\n",
    "                kwargs[\"CreateBucketConfiguration\"] = {\"LocationConstraint\": region}\n",
    "            s3.create_bucket(Bucket=bucket_name, **kwargs)\n",
    "            print(f\"[S3] Bucket '{bucket_name}' created.\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def upload_directory(local_dir: str, bucket: str, prefix: str, region: str):\n",
    "    \"\"\"Recursively upload all files under local_dir to s3://bucket/prefix/...\"\"\"\n",
    "    s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "    if not os.path.isdir(local_dir):\n",
    "        raise RuntimeError(f\"Local directory '{local_dir}' not found.\")\n",
    "\n",
    "    # Ensure prefix ends with \"/\" if not empty\n",
    "    if prefix and not prefix.endswith(\"/\"):\n",
    "        prefix += \"/\"\n",
    "\n",
    "    for root, _, files in os.walk(local_dir):\n",
    "        for fname in files:\n",
    "            local_path = os.path.join(root, fname)\n",
    "            rel_path = os.path.relpath(local_path, local_dir)\n",
    "            s3_key = prefix + rel_path.replace(os.path.sep, \"/\")\n",
    "            print(f\"[S3] Uploading {local_path} → s3://{bucket}/{s3_key}\")\n",
    "            try:\n",
    "                s3.upload_file(local_path, bucket, s3_key)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to upload {local_path}: {e}\")\n",
    "\n",
    "    print(f\"[S3] All files from '{local_dir}' uploaded under '{prefix}'.\")\n",
    "\n",
    "# CREATE RAW BUCKET directory \n",
    "ensure_s3_bucket(BUCKET)\n",
    "upload_directory(LOCAL_MANUAL_DATA_DIR, BUCKET, S3_PREFIX, REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc27cc3",
   "metadata": {},
   "source": [
    "# Ingest & Scrape (Serverless + Batch)\n",
    "- Fan‐out with SQS → Lambda\n",
    "\n",
    "    - Push one message per “slice” (e.g. Fed curve for Jan – Mar 2025, TIPS for Apr – Jun, swaps for tenor = 2 yr, etc.).\n",
    "\n",
    "    - A small Lambda (512 MB, 1–2 vCPU) picks up each message, runs your pull_* scraper for that slice, and writes a Parquet shard to s3://…/raw/<module>/<slice>.parquet.\n",
    "\n",
    "- Edge Case: Longer pulls\n",
    "    - If you ever hit the 15 min/3 GB Lambda limit (e.g. a multi-year WRDS query), switch that particular task to an AWS Batch array job with the same code in a Docker container. You still drive it via SQS messages, but now each slice runs on a spot-backed EC2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea95949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw/100_Portfolios_10x10_sheet0.parquet',\n",
       " 'raw/100_Portfolios_10x10_sheet1.parquet',\n",
       " 'raw/100_Portfolios_10x10_sheet2.parquet',\n",
       " 'raw/100_Portfolios_10x10_sheet3.parquet',\n",
       " 'raw/100_Portfolios_10x10_sheet4.parquet',\n",
       " 'raw/100_Portfolios_10x10_sheet5.parquet',\n",
       " 'raw/100_Portfolios_10x10_sheet6.parquet',\n",
       " 'raw/100_Portfolios_10x10_sheet7.parquet',\n",
       " 'raw/100_Portfolios_10x10_sheet8.parquet',\n",
       " 'raw/100_Portfolios_10x10_sheet9.parquet',\n",
       " 'raw/25_Portfolios_5x5_sheet0.parquet',\n",
       " 'raw/25_Portfolios_5x5_sheet1.parquet',\n",
       " 'raw/25_Portfolios_5x5_sheet2.parquet',\n",
       " 'raw/25_Portfolios_5x5_sheet3.parquet',\n",
       " 'raw/25_Portfolios_5x5_sheet4.parquet',\n",
       " 'raw/25_Portfolios_5x5_sheet5.parquet',\n",
       " 'raw/25_Portfolios_5x5_sheet6.parquet',\n",
       " 'raw/25_Portfolios_5x5_sheet7.parquet',\n",
       " 'raw/25_Portfolios_5x5_sheet8.parquet',\n",
       " 'raw/25_Portfolios_5x5_sheet9.parquet',\n",
       " 'raw/6_Portfolios_2x3_sheet0.parquet',\n",
       " 'raw/6_Portfolios_2x3_sheet1.parquet',\n",
       " 'raw/6_Portfolios_2x3_sheet2.parquet',\n",
       " 'raw/6_Portfolios_2x3_sheet3.parquet',\n",
       " 'raw/6_Portfolios_2x3_sheet4.parquet',\n",
       " 'raw/6_Portfolios_2x3_sheet5.parquet',\n",
       " 'raw/6_Portfolios_2x3_sheet6.parquet',\n",
       " 'raw/6_Portfolios_2x3_sheet7.parquet',\n",
       " 'raw/6_Portfolios_2x3_sheet8.parquet',\n",
       " 'raw/6_Portfolios_2x3_sheet9.parquet',\n",
       " 'raw/fed_tips_yield_curve.parquet',\n",
       " 'raw/fed_yield_curve.parquet',\n",
       " 'scripts/requirements.txt',\n",
       " 'scripts/scraper_1_fed_yield_curve.py',\n",
       " 'scripts/scraper_2_fed_tips_yield_curve.py',\n",
       " 'scripts/scraper_3_expectations.py',\n",
       " 'scripts/settings.py']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check bucket contents\n",
    "bucket = BUCKET\n",
    "bucket_resource = s3_resource.Bucket(bucket)\n",
    "[obj.key for obj in bucket_resource.objects.all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45712c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import time\n",
    "from io import BytesIO\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def s3_parquet_preview(bucket, key, rows=10):\n",
    "    '''\n",
    "    Downloads Parquet file from S3 using boto3 and reads preview into a DataFrame.\n",
    "    '''\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    file_stream = BytesIO(response['Body'].read())\n",
    "    df = pd.read_parquet(file_stream, engine='fastparquet')  # or 'fastparquet'\n",
    "    return df.head(rows)\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "df = s3_parquet_preview(bucket=BUCKET,\n",
    "                        key='raw/fed_yield_curve.parquet')\n",
    "print(time.time() - t0, 'seconds')\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aff44b",
   "metadata": {},
   "source": [
    " lambda_src/serial_scraper.py\n",
    " lambda_src/scraper_lambda.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc55fb",
   "metadata": {},
   "source": [
    "# RDS MySQL: Cleaned Results \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136ed69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a07447b",
   "metadata": {},
   "source": [
    "# ETL & Analysis (EMR + Dask) for Heavy‐Lifting\n",
    "\n",
    "- EMR PySpark Step\n",
    "    - Spin up a small EMR cluster (1 master + 3–5 m5.xlarge cores, spot if you like).\n",
    "    - One Spark job reads raw.* tables via the S3 connector, does your merges + compute_tips_treasury() logic at scale, and writes out s3://…/processed/tips_treasury.parquet (and the equivalent for your 5 modules).\n",
    "\n",
    "- Dask for custom kernels\n",
    "\n",
    "    - If you have bespoke Python loops (e.g. bootstraps, simulations), launch a Dask cluster on the same EMR (or on a separate set of spot EC2s) with dask-ec2 or EMR’s Dask support.\n",
    "    - Use Numba/CuDF if you need GPU acceleration (p3 instances) for your heaviest routines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73309d32",
   "metadata": {},
   "source": [
    "## Data directory for functions \n",
    "### TIPS Treasury \n",
    "* `S3_MANUAL_DATA` / \"treasury_inflation_swaps.csv\"   -> `import_inflation_swap_data()`\n",
    "* `S3_DATA` / \"fed_yield_curve.parquet\"               ->  `import_treasury_yields()`\n",
    "* `S3_DATA` / \"fed_tips_yield_curve.parquet\"          -> `import_tips_yields()` \n",
    "* `import_inflation_swap_data(), import_treasury_yields(), import_tips_yields()`-> `compute_tips_treasury()` -> [OUTPUT] / \"tips_treasury_implied_rf.parquet\"\n",
    "### Equity Spot Futures\n",
    "```python\n",
    "targets = [\n",
    "        PROCESSED_DIR / \"all_indices_calendar_spreads.csv\",\n",
    "        PROCESSED_DIR / \"INDU_calendar_spread.csv\",\n",
    "        PROCESSED_DIR / \"SPX_calendar_spread.csv\",\n",
    "        PROCESSED_DIR / \"NDX_calendar_spread.csv\",\n",
    "    ]\n",
    "```\n",
    "```scss\n",
    "futures_data_processing.py\n",
    " ├─► `S3_MANUAL_DATA` / \"bloomberg_historical_data.parquet\"\n",
    " ├─► process_index_futures(raw_data, futures_codes) \n",
    " │     ├─► parse_contract_month_year(cs)                [utility]\n",
    " │     └─► get_third_friday(year, month)                [utility]\n",
    " └─► merge_calendar_spreads(all_futures)    \n",
    "       ├─► pd.merge(term1, term2) \n",
    "       └─► to_csv(… → OUTPUT)                           : targets\n",
    "```\n",
    "```python\n",
    "targets = [\n",
    "        PROCESSED_DIR / \"cleaned_ois_rates.csv\"\n",
    "    ]\n",
    "```\n",
    "```scss\n",
    "OIS_data_processing.py\n",
    " ├─► check os.path.exists(INPUT_FILE) : S3_MANUAL_DATA / \"bloomberg_historical_data.parquet\"\n",
    " ├─► process_ois_data(filepath=INPUT_FILE)\n",
    "      ├─► pd.read_parquet(filepath)                \n",
    "      └─► ois_df.to_csv(output_path, index=True)   : targets\n",
    "```\n",
    "```python\n",
    "targets = [\n",
    "        PROCESSED_DIR / \"SPX_Forward_Rates.csv\",\n",
    "        PROCESSED_DIR / \"NDX_Forward_Rates.csv\",\n",
    "        PROCESSED_DIR / \"INDU_Forward_Rates.csv\",\n",
    "        OUTPUT_DIR / \"all_indices_spread_to_2020.png\",\n",
    "        OUTPUT_DIR / \"all_indices_spread_to_present.png\"\n",
    "    ]\n",
    "```\n",
    "```scss\n",
    "Spread_calculations.py\n",
    " ├─► process_index_forward_rates(index_code)\n",
    " │     ├─► pd.read_csv(fut_file)                    PROCESSED_DIR / \"<INDEX>_Calendar_spread.csv\"\n",
    " │     ├─► pd.read_csv(ois_file)                    PROCESSED_DIR / \"cleaned_ois_rates.csv\"\n",
    " │     ├─► build_daily_dividends(index_code)\n",
    " │     │      └─► pd.read_parquet(input_file)       \"bloomberg_historical_data.parquet\"\n",
    " │     ├─► barndorff_nielsen_filter(…)              [pure transform]\n",
    " │     └─► merged_df.to_csv(out_file)               PROCESSED_DIR / \"<INDEX>_Forward_Rates.csv\"\n",
    " └─► plot_all_indices(results)\n",
    "        └─► _plot(…, \"to_2020\")                     : targets\n",
    "        └─► _plot(…, \"to_present\")                  : targets\n",
    "```\n",
    "### CIP \n",
    "```scss\n",
    "$project‐map: (\n",
    "  files: (\n",
    "    // Notebooks & scripts\n",
    "    main_cip_notebook:   \"src/main_cip.ipynb\",\n",
    "    cip_analysis_script: \"src/cip_analysis.py\",\n",
    "    pull_cip_module:     \"src/pull_bloomberg_cip_data.py\",\n",
    "    settings_module:     \"src/settings.py\",\n",
    "    dir_funcs_module:    \"src/directory_functions.py\",\n",
    "    manual_data_excel:   \"data_manual/CIP_2025.xlsx\"\n",
    "  ),\n",
    "\n",
    "$module‐dependency‐map: (\n",
    "  data_fetch: ( ),\n",
    "  plots: ( data_fetch ),\n",
    "  summary‐stats code (compute_cip, compute_cip_statistics, save_cip_statistics_as_html)\n",
    "  stats: ( data_fetch, plots ),\n",
    "  notebooks: ( data_fetch, plots, stats )\n",
    "\n",
    "clean_data.py\n",
    " ├─► pull_bloomberg_cip_data.load_raw('2025-03-01')   MANUAL_DATA_DIR / \"CIP_2025.xlsx\"\n",
    " ├─► df.to_csv(output_file=\"tidy_data.csv\")           OUTPUT_DIR / \"tidy_data.csv\"\n",
    "\n",
    " module pull_bloomberg_cip_data.py\n",
    " ├─► settings.BLOOMBERG (config)        \n",
    " \n",
    "download()\n",
    " ├─► requests.get(url)\n",
    " ├─► open(\"./data_manual/CIP_2025.xlsx\",\"wb\")   [I/O write: \"./data_manual/CIP_2025.xlsx\"]\n",
    " └─► pd.read_excel(target_file)                 [I/O read: \"./data_manual/CIP_2025.xlsx\"]\n",
    "\n",
    "fetch_bloomberg_historical_data(start, end)\n",
    " └─► blp.bdh(...) (no file I/O) → DataFrames\n",
    "\n",
    "plot_cip(end)\n",
    " ├─ if BLOOMBERG=False:\n",
    " │     ├─► pd.read_excel(\"./data_manual/CIP_2025.xlsx\")  \n",
    " │     └─► plt.savefig(f\"spread_plot_{yr}.pdf/png\")  [I/O write: \"spread_plot_{yr}.pdf\", \"spread_plot_{yr}.png\"]\n",
    " └─ else:\n",
    "       └─► fetch_bloomberg_historical_data(...)\n",
    "\n",
    "load_raw(end, plot)\n",
    " ├─ if BLOOMBERG=False:\n",
    " │     ├─► pd.read_excel(...) or download()    [I/O read/write: Excel paths or download]\n",
    " │     └─► returns df_merged in memory\n",
    " └─ else:\n",
    "       └─► fetch_bloomberg_historical_data(...)\n",
    "\n",
    "compute_cip(end)\n",
    " └─► load_raw(end) → in-memory DataFrame (no file I/O)\n",
    "\n",
    "load_raw_pieces(end, excel, plot)\n",
    " ├─ if BLOOMBERG=False:\n",
    " │     └─► pd.read_excel(...)              [I/O read: Excel paths]\n",
    " └─ else:\n",
    "       └─► fetch_bloomberg_historical_data(...)\n",
    "```\n",
    "```python\n",
    " \"targets\": [\n",
    "            str(OUTPUT_DIR / \"cip_summary_overall.html\"),\n",
    "            str(OUTPUT_DIR / \"cip_correlation_matrix.html\"),\n",
    "            str(OUTPUT_DIR / \"cip_annual_statistics.html\"),\n",
    "        ]\n",
    "```\n",
    "```scss\n",
    "cip_analysis.py\n",
    " ├─ save_cip_statistics_as_html(stats_dict)\n",
    " ├─ compute_cip(end='2020-01-01')\n",
    " │    └─ load_raw(end)             # from pull_bloomberg_cip_data\n",
    " └─ compute_cip_statistics(cip_data)\n",
    " {\n",
    "  \"overall_statistics\": DataFrame,\n",
    "  \"correlation_matrix\": DataFrame,\n",
    "  \"annual_statistics\": DataFrame\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "### Treasury Spot \n",
    "  \n",
    "\n",
    "* **Reads**\n",
    "\n",
    "  * `${DATA_DIR}/treasury_df.csv`\n",
    "  * `${DATA_DIR}/ois_df.csv`\n",
    "  * `${DATA_DIR}/last_day_df.csv`\n",
    "* **Writes**\n",
    "\n",
    "  * `${OUTPUT_DIR}/arbitrage_spread_2.pdf`\n",
    "  * `${OUTPUT_DIR}/arbitrage_spread_5.pdf`\n",
    "  * `${OUTPUT_DIR}/arbitrage_spread_10.pdf`\n",
    "  * `${OUTPUT_DIR}/arbitrage_spread_20.pdf`\n",
    "  * `${OUTPUT_DIR}/arbitrage_spread_30.pdf`\n",
    "  * `${DATA_DIR}/treasury_sf_output.csv`\n",
    "* **In-memory**\n",
    "\n",
    "  * `df_long`, intermediate columns, and final `df_out`, `df_wide`\n",
    "\n",
    "### Market Expectations\n",
    "\n",
    "```scss\n",
    "./src/pull_ken_french_data.py  ←──  task_pull_ken_french_data ──> Excel portfolios\n",
    "       (data fetch & cleaning)\n",
    "\n",
    "./src/pull_CRSP_index.py      ←──  task_pull_CRSP_index ──>  crsp_value_weighted_index.csv\n",
    "       (data fetch & cleaning)\n",
    "\n",
    "Notebooks:\n",
    "  01_Market_...ipynb      \\\n",
    "  run_regressions.ipynb   ├─> task_convert_notebooks_to_scripts ──> `_*.py`\n",
    "                          └─> task_run_notebooks  \n",
    "                                ├─> **execute notebook code** (analysis/regressions)\n",
    "                                ├─> HTML export\n",
    "                                └─> copy & clear outputs\n",
    "\n",
    "reports/project.tex  ←──  task_compile_latex_docs  ──> project.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7041ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fd42d80",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aa5e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(bucket_name):\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "    for item in bucket.objects.all():\n",
    "        item.delete()\n",
    "        \n",
    "cleanup(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce125d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[S3] Emptying bucket 'finalproject-macs30123-baileymeche'...\n",
      "[S3] Deleting bucket 'finalproject-macs30123-baileymeche'...\n",
      "[S3] Bucket 'finalproject-macs30123-baileymeche' deleted.\n",
      "\n",
      "=== TEARDOWN COMPLETE ===\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rds = boto3.client(\"rds\", region_name=REGION)\n",
    "ec2 = boto3.client(\"ec2\", region_name=REGION)\n",
    "sm  = boto3.client(\"secretsmanager\", region_name=REGION)\n",
    "s3  = boto3.resource(\"s3\", region_name=REGION)\n",
    "\n",
    "def delete_db_instance(instance_id):\n",
    "    try:\n",
    "        print(f\"[RDS] Deleting instance '{instance_id}'...\")\n",
    "        rds.delete_db_instance(\n",
    "            DBInstanceIdentifier=instance_id,\n",
    "            SkipFinalSnapshot=True,\n",
    "            DeleteAutomatedBackups=True\n",
    "        )\n",
    "        waiter = rds.get_waiter(\"db_instance_deleted\")\n",
    "        waiter.wait(DBInstanceIdentifier=instance_id)\n",
    "        print(f\"[RDS] Instance '{instance_id}' deleted.\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"DBInstanceNotFound\":\n",
    "            print(f\"[RDS] Instance '{instance_id}' not found; skipping.\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def delete_db_cluster(cluster_id):\n",
    "    try:\n",
    "        print(f\"[RDS] Deleting cluster '{cluster_id}'...\")\n",
    "        rds.delete_db_cluster(\n",
    "            DBClusterIdentifier=cluster_id,\n",
    "            SkipFinalSnapshot=True\n",
    "        )\n",
    "        waiter = rds.get_waiter(\"db_cluster_deleted\")\n",
    "        waiter.wait(DBClusterIdentifier=cluster_id)\n",
    "        print(f\"[RDS] Cluster '{cluster_id}' deleted.\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"DBClusterNotFoundFault\":\n",
    "            print(f\"[RDS] Cluster '{cluster_id}' not found; skipping.\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def delete_subnet_group(group_name):\n",
    "    try:\n",
    "        print(f\"[RDS] Deleting subnet group '{group_name}'...\")\n",
    "        rds.delete_db_subnet_group(DBSubnetGroupName=group_name)\n",
    "        print(f\"[RDS] Subnet group '{group_name}' deleted.\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"DBSubnetGroupNotFoundFault\":\n",
    "            print(f\"[RDS] Subnet group '{group_name}' not found; skipping.\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def delete_secret(secret_name):\n",
    "    try:\n",
    "        print(f\"[Secrets] Deleting secret '{secret_name}'...\")\n",
    "        sm.delete_secret(SecretId=secret_name, ForceDeleteWithoutRecovery=True)\n",
    "        print(f\"[Secrets] Secret '{secret_name}' deleted.\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\n",
    "            print(f\"[Secrets] Secret '{secret_name}' not found; skipping.\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def delete_security_group(name, vpc_id):\n",
    "    # find by name+vpc\n",
    "    resp = ec2.describe_security_groups(Filters=[\n",
    "        {\"Name\":\"group-name\",\"Values\":[name]},\n",
    "        {\"Name\":\"vpc-id\",\"Values\":[vpc_id]}\n",
    "    ])\n",
    "    groups = resp[\"SecurityGroups\"]\n",
    "    if not groups:\n",
    "        print(f\"[EC2] SG '{name}' not found; skipping.\")\n",
    "        return\n",
    "    sg_id = groups[0][\"GroupId\"]\n",
    "    # revoke any ingress rules\n",
    "    perms = groups[0].get(\"IpPermissions\", [])\n",
    "    if perms:\n",
    "        print(f\"[EC2] Revoking ingress on {sg_id}...\")\n",
    "        ec2.revoke_security_group_ingress(GroupId=sg_id, IpPermissions=perms)\n",
    "    # delete\n",
    "    try:\n",
    "        print(f\"[EC2] Deleting SG '{sg_id}'...\")\n",
    "        ec2.delete_security_group(GroupId=sg_id)\n",
    "        print(f\"[EC2] SG '{sg_id}' deleted.\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        print(f\"[EC2] Could not delete SG '{sg_id}': {e}\")\n",
    "\n",
    "def empty_and_delete_bucket(bucket_name):\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    # delete all objects\n",
    "    print(f\"[S3] Emptying bucket '{bucket_name}'...\")\n",
    "    bucket.objects.all().delete()\n",
    "    # delete all versions (if versioning enabled)\n",
    "    try:\n",
    "        bucket.object_versions.delete()\n",
    "    except Exception:\n",
    "        pass\n",
    "    # delete bucket itself\n",
    "    try:\n",
    "        print(f\"[S3] Deleting bucket '{bucket_name}'...\")\n",
    "        bucket.delete()\n",
    "        print(f\"[S3] Bucket '{bucket_name}' deleted.\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        print(f\"[S3] Could not delete bucket '{bucket_name}': {e}\")\n",
    "\n",
    "# 1) Tear down RDS\n",
    "# delete_db_instance(INSTANCE_ID)\n",
    "# delete_db_cluster(CLUSTER_ID)\n",
    "# delete_subnet_group(SUBNET_GROUP)\n",
    "\n",
    "# # 2) Delete Secrets Manager entry\n",
    "# delete_secret(SECRET_NAME)\n",
    "\n",
    "# # 3) Delete Security Group\n",
    "# delete_security_group(SG_NAME, VPC_ID)\n",
    "\n",
    "# 4) Delete S3 bucket\n",
    "empty_and_delete_bucket(BUCKET)\n",
    "\n",
    "print(\"\\n=== TEARDOWN COMPLETE ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1a30f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Deleting Lambda function 'ScraperWorker'…\n",
      "   Error deleting Lambda: Function not found: arn:aws:lambda:us-east-1:325826323478:function:ScraperWorker\n",
      "→ Emptying bucket 'finalproject-macs30123-baileymeche'…\n",
      "   Error cleaning/deleting bucket: The specified bucket does not exist\n",
      "→ Deleting RDS instance 'my-scrape-db'…\n",
      "   Delete initiated. Waiting for deletion to complete…\n",
      "   RDS instance deleted.\n"
     ]
    }
   ],
   "source": [
    "# teardown.ipynb cell\n",
    "\n",
    "import boto3\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# === Configuration ===\n",
    "REGION      = \"us-east-1\"\n",
    "LAMBDA_NAME = \"ScraperWorker\"\n",
    "BUCKET_NAME = \"finalproject-macs30123-baileymeche\"\n",
    "RDS_ID      = \"my-scrape-db\"\n",
    "\n",
    "# === Clients ===\n",
    "lam = boto3.client(\"lambda\", region_name=REGION)\n",
    "s3  = boto3.resource(\"s3\", region_name=REGION)\n",
    "rds = boto3.client(\"rds\", region_name=REGION)\n",
    "\n",
    "# 1️⃣ Delete Lambda function\n",
    "print(f\"→ Deleting Lambda function '{LAMBDA_NAME}'…\")\n",
    "try:\n",
    "    lam.delete_function(FunctionName=LAMBDA_NAME)\n",
    "    print(\"   Lambda deleted.\")\n",
    "except ClientError as e:\n",
    "    print(\"   Error deleting Lambda:\", e.response[\"Error\"][\"Message\"])\n",
    "\n",
    "# 2️⃣ Empty and (optionally) delete S3 bucket\n",
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "print(f\"→ Emptying bucket '{BUCKET_NAME}'…\")\n",
    "# delete all objects (and versions, if versioned)\n",
    "try:\n",
    "    # If versioning enabled, you need to delete all versions\n",
    "    bucket.object_versions.delete()\n",
    "    print(\"   All objects deleted.\")\n",
    "    # Now delete bucket itself\n",
    "    bucket.delete()\n",
    "    print(f\"   Bucket '{BUCKET_NAME}' deleted.\")\n",
    "except ClientError as e:\n",
    "    print(\"   Error cleaning/deleting bucket:\", e.response[\"Error\"][\"Message\"])\n",
    "\n",
    "# 3️⃣ Delete RDS instance\n",
    "print(f\"→ Deleting RDS instance '{RDS_ID}'…\")\n",
    "try:\n",
    "    rds.delete_db_instance(\n",
    "        DBInstanceIdentifier=RDS_ID,\n",
    "        SkipFinalSnapshot=True,   # change to False if you want to keep a final snapshot\n",
    "        DeleteAutomatedBackups=True\n",
    "    )\n",
    "    print(\"   Delete initiated. Waiting for deletion to complete…\")\n",
    "    waiter = rds.get_waiter(\"db_instance_deleted\")\n",
    "    waiter.wait(DBInstanceIdentifier=RDS_ID)\n",
    "    print(\"   RDS instance deleted.\")\n",
    "except ClientError as e:\n",
    "    print(\"   Error deleting RDS:\", e.response[\"Error\"][\"Message\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repro_suite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
